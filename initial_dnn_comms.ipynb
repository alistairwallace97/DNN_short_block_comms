{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from keras import backend as K\n",
    "from keras.layers import GaussianNoise, advanced_activations\n",
    "from keras.engine.topology import Layer\n",
    "from keras.legacy import interfaces\n",
    "from keras.initializers import Zeros as kZeros\n",
    "from keras.utils import multi_gpu_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from time import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Likely Symbol Layer\n",
    "I have intially done this with a normal function, however this hasn't been working with Keras. What I'm probably going to do next is make a custom layer for this as well and then add this. <br>\n",
    "Not sure whether to use this in training though as using softwax may be a good way of training the model to give decisive decisions and it's also pretty useful for seeing the probabilities, and therefore confidence in it's decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_likely_symbol(posterior_probs):\n",
    "    row_indices = np.arange(posterior_probs.shape[0])\n",
    "    col_indices = np.argmax(posterior_probs, axis=1)\n",
    "    ret_arr = np.zeros(posterior_probs.shape)\n",
    "    ret_arr[row_indices,col_indices] = 1\n",
    "    return ret_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras backend (tensorflow) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_likely(posterior_probs):\n",
    "    max_vals = K.max(posterior_probs, axis=1, keepdims=True) \n",
    "    max_vals = K.cast(max_vals, 'float32')\n",
    "    geT = K.greater_equal(posterior_probs, max_vals)\n",
    "    return K.cast(geT, 'float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom most likely symbol layer\n",
    "This layer need to, for all in-phase and quaternary outputs, pick the most likely symbol. <br>\n",
    "I need to take in a softmax probability distribution for all the possible symbols (One hot encoded) and pick the one with the maximum likelyhood (value). Say $M$=2, then $S$ = {01,10} so $\\hat{S}$ = {01,10}. This layer converts something that looks something like [0.63,0.27] to [1,0] <br>\n",
    "**Note:** This layer should only be used in testing because gradients do not propogate through round functions. So this is a layer that should only be used at test time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MostLikelySymbol(Layer):\n",
    "    \"\"\"Return the most likely symbol from a softmax input in the\n",
    "    one hot encoded form.\n",
    "\n",
    "    This layer is only active at test time as otherwise it would\n",
    "    stop gradient propogation during training. Also it is useful\n",
    "    to train with a softmax output to encourage a decisive \n",
    "    decision and because it means you can assess confidence.\n",
    "\n",
    "    # Arguments\n",
    "        None\n",
    "\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    \"\"\"\n",
    "\n",
    "    @interfaces.legacy_gaussiannoise_support\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MostLikelySymbol, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        def most_likely():\n",
    "            max_vals = K.max(inputs, axis=1, keepdims=True) \n",
    "            max_vals = K.cast(max_vals, 'float32')\n",
    "            geT = K.greater_equal(inputs, max_vals)\n",
    "            return K.cast(geT, 'float32')            \n",
    "        return K.in_train_phase(inputs, most_likely, training=training)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        base_config = super(MostLikelySymbol, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom GaussianNoise layer\n",
    "Ok, so the problem with the built in GaussianNoise layer is that it only adds noise at training time and not at testing time. So I've had a look in the source code and found how this is enacted. <br>\n",
    "I'm now going to define my own custom layer which does exactly the same thing, but just adds the noise at both training time and testing time. <br>\n",
    "I'm going to copy the source code from keras then just fiddle it so it does what I want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise2(Layer):\n",
    "    \"\"\"Apply additive zero-centered Gaussian noise at both traning\n",
    "    and test time.\n",
    "\n",
    "    This is useful to mitigate overfitting\n",
    "    (you could see it as a form of random data augmentation).\n",
    "    Gaussian Noise (GS) is a natural choice as corruption process\n",
    "    for real valued inputs.\n",
    "\n",
    "    Unlike the built in GaussianNoise regularisation layer it is \n",
    "    active at both training and test time. \n",
    "\n",
    "    # Arguments\n",
    "        stddev: float, standard deviation of the noise distribution.\n",
    "\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    \"\"\"\n",
    "\n",
    "    @interfaces.legacy_gaussiannoise_support\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super(GaussianNoise2, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        def noised():\n",
    "            return inputs + K.random_normal(shape=K.shape(inputs),\n",
    "                                            mean=0.,\n",
    "                                            stddev=self.stddev)\n",
    "        return K.in_train_phase(noised, noised, training=training)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'stddev': self.stddev}\n",
    "        base_config = super(GaussianNoise2, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(M, R, sigma, activation_func):\n",
    "    ### Initialising Parameters\n",
    "    k = np.log2(M) # Number of bits needed to represent M \n",
    "                   # messages\n",
    "    Nc = int(round(k/R)) # Number of bit being used to represent\n",
    "                        # channel symbols being used \n",
    "                        # Number of complex channel uses\n",
    "    Nr = Nc*2 # Number of real channel uses\n",
    "    \n",
    "    ### Defining Layers\n",
    "    ## TRANSMITTER\n",
    "    # This is my input placeholder\n",
    "    input_message = Input(shape=(M,), name=\"input\")\n",
    "    # Encoded representation of the input\n",
    "    # Relu layer to capture non-linearity\n",
    "    tx1 = Dense(Nr,activation=activation_func, name=\"tx1\")\\\n",
    "                (input_message)\n",
    "    # Linear layer to give channel symbols not\n",
    "    # clustered around 0 and 1.\n",
    "    tx2 = Dense(Nr,activation=activation_func, name=\"tx2\")(tx1)\n",
    "    # Reshape it to complex channel symbols\n",
    "    tx_complex = Lambda(lambda x : K.reshape(x, (-1,Nc,2)),\n",
    "                       output_shape=(Nc,2), \\\n",
    "                        name=\"tx_reshape\")(tx2)\n",
    "\n",
    "    # Normalisation Layer\n",
    "    tx_norm = Lambda(lambda x : K.l2_normalize(x,axis=2),\n",
    "                     output_shape=(Nc,2), name=\"tx_norm\")\\\n",
    "                        (tx_complex)\n",
    "#     tx_norm = BatchNormalization(axis=2)(tx_complex)\n",
    "\n",
    "    # Add Noise \n",
    "    noise = GaussianNoise2(sigma)(tx_norm)\n",
    "\n",
    "    ## RECIEVER\n",
    "    # Flatten the input\n",
    "    noise_flat = Lambda(lambda x : K.reshape(x, (-1,Nr)),\n",
    "                       output_shape=(Nr,),\\\n",
    "                        name=\"noise_flat\")(noise)\n",
    "    # Multiple Dense Layers\n",
    "    # Dense relu layer to capture non linearity\n",
    "    rx1 = Dense(M,activation=activation_func, name=\"rx1\")\\\n",
    "                (noise_flat)\n",
    "    # Dense layer with softmax activation\n",
    "    rx_softmax = Dense(M,activation='softmax', \\\n",
    "                       name=\"rx_softmax\")(rx1)\n",
    "    # Select the symbols with the maximum probabilities\n",
    "    ml_symbs = MostLikelySymbol()(rx_softmax)\n",
    "    \n",
    "    ###Defining the models\n",
    "    autoencoder = Model(input_message, rx_softmax)\n",
    "    ## Model the Tx and Rx seperately as well\n",
    "    # Model the Tx\n",
    "    transmitter = Model(input_message, tx_norm)\n",
    "    # Model the Tx plus the noise\n",
    "    channel_sym_with_noise = Model(input_message, noise)\n",
    "    channel_symbol = Input(shape=(Nr,))\n",
    "    # Take the last layer of the autoencoder model\n",
    "    reciever_layers = autoencoder.layers[-2](channel_symbol)\n",
    "    reciever_layers = autoencoder.layers[-1](reciever_layers)\n",
    "\n",
    "    # Create a model of the reciever\n",
    "    reciever = Model(channel_symbol, reciever_layers)\n",
    "    autoencoder_symbs = Model(input_message,ml_symbs) \n",
    "    \n",
    "    # Compile the model\n",
    "    autoencoder.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")\n",
    "    return autoencoder, transmitter, reciever,\\\n",
    "            autoencoder_symbs, k, Nc, Nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_non_complex_channel_model(k, Nr, sigma, activation_func):\n",
    "    ### Initialising Parameters\n",
    "    assert(round(k) == k)\n",
    "    M = 2**k\n",
    "\n",
    "    ### Defining Layers\n",
    "    ## TRANSMITTER\n",
    "    # This is my input placeholder\n",
    "    input_message = Input(shape=(M,), name=\"input\")\n",
    "    # Encoded representation of the input\n",
    "    # Relu layer to capture non-linearity\n",
    "    tx1 = Dense(Nr,activation=activation_func, name=\"tx1\")\\\n",
    "                (input_message)\n",
    "    # Linear layer to give channel symbols not\n",
    "    # clustered around 0 and 1.\n",
    "    tx2 = Dense(Nr,activation=activation_func, name=\"tx2\")(tx1)\n",
    "\n",
    "    # Normalisation Layer\n",
    "    tx_norm = Lambda(lambda x : (Nr/2)*K.l2_normalize(x,axis=1),\n",
    "                     output_shape=(Nr,), name=\"tx_norm\")\\\n",
    "                        (tx2)\n",
    "\n",
    "    # Add Noise \n",
    "    noise = GaussianNoise2(sigma)(tx_norm)\n",
    "\n",
    "    ## RECIEVER\n",
    "    # Multiple Dense Layers\n",
    "    # Dense relu layer to capture non linearity\n",
    "    rx1 = Dense(Nr,activation=activation_func, name=\"rx1\")\\\n",
    "                (noise)\n",
    "    # Dense layer with softmax activation\n",
    "    rx_softmax = Dense(M,activation='softmax', \\\n",
    "                       name=\"rx_softmax\")(rx1)\n",
    "    # Select the symbols with the maximum probabilities\n",
    "    ml_symbs = MostLikelySymbol()(rx_softmax)\n",
    "\n",
    "    ###Defining the models\n",
    "    autoencoder = Model(input_message, rx_softmax)\n",
    "    ## Model the Tx and Rx seperately as well\n",
    "    # Model the Tx\n",
    "    transmitter = Model(input_message, tx_norm)\n",
    "    # Model the Tx plus the noise\n",
    "    channel_sym_with_noise = Model(input_message, noise)\n",
    "    channel_symbol = Input(shape=(Nr,))\n",
    "    # Take the last layer of the autoencoder model\n",
    "    reciever_layers = autoencoder.layers[-2](channel_symbol)\n",
    "    reciever_layers = autoencoder.layers[-1](reciever_layers)\n",
    "\n",
    "    # Create a model of the reciever\n",
    "    reciever = Model(channel_symbol, reciever_layers)\n",
    "    autoencoder_symbs = Model(input_message,ml_symbs) \n",
    "\n",
    "    # Compile the model\n",
    "    autoencoder.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")\n",
    "    return autoencoder, transmitter, channel_sym_with_noise, \\\n",
    "            reciever, autoencoder_symbs, k, Nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_shapes(start, end, num_steps):\n",
    "    shapes = [start]\n",
    "    diff = (end-start)/(num_steps-1)\n",
    "    # Always start with a full dense layer\n",
    "    for i in range(1,num_steps):\n",
    "        shapes.append(int(start + i*diff))\n",
    "    return shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_complex_n_layer_lr_tanh_tapering_model(M, R, sigma, \\\n",
    "                                                hl_activation_func, \\\n",
    "                                                ol_activation_func, \\\n",
    "                                                num_layers):\n",
    "    ### Initialising Parameters\n",
    "    k = np.log2(M) # Number of bits needed to represent M \n",
    "                   # messages\n",
    "    Nc = int(round(k/R)) # Number of bit being used to represent\n",
    "                        # channel symbols being used \n",
    "                        # Number of complex channel uses\n",
    "    Nr = Nc*2 # Number of real channel uses\n",
    "    print(\"M = \", M)\n",
    "    print(\"k = \", k)\n",
    "    print(\"Nc = \", Nc)\n",
    "    print(\"Nr = \", Nr)\n",
    "\n",
    "    ### Defining Layers\n",
    "    ## TRANSMITTER\n",
    "    tx_shapes = get_layer_shapes(M, Nr, num_layers)\n",
    "    input_message = Input(shape=(M,), name=\"input\")\n",
    "    # Hidden Tx layers\n",
    "    tx1 = Dense(tx_shapes[0],activation=hl_activation_func, \\\n",
    "                name=\"tx1\")(input_message)\n",
    "    for i in range(1,num_layers-1):\n",
    "        tx1 = Dense(tx_shapes[i],activation=hl_activation_func, \\\n",
    "                    name=(\"tx\"+str(i+1)))(tx1)\n",
    "    # Final layer with a different activation function to capture non\n",
    "    # linearity\n",
    "    tx_n = Dense(tx_shapes[-1],activation=ol_activation_func, \\\n",
    "                 name=(\"tx\"+str(num_layers)))(tx1)\n",
    "    # Reshape it to complex channel symbols\n",
    "    tx_complex = Lambda(lambda x : K.reshape(x, (-1,Nc,2)),\n",
    "                       output_shape=(Nc,2), \\\n",
    "                        name=\"tx_reshape\")(tx_n)\n",
    "\n",
    "    # Normalisation Layer\n",
    "    tx_norm = Lambda(lambda x : K.l2_normalize(x,axis=2),\n",
    "                     output_shape=(Nc,2), name=\"tx_norm\")\\\n",
    "                        (tx_complex)\n",
    "\n",
    "    # Add Noise \n",
    "    noise = GaussianNoise2(sigma)(tx_norm)\n",
    "\n",
    "    ## RECIEVER\n",
    "    # Flatten the input\n",
    "    noise_flat = Lambda(lambda x : K.reshape(x, (-1,Nr)),\n",
    "                       output_shape=(Nr,),\\\n",
    "                        name=\"noise_flat\")(noise)\n",
    "    # First layer with the different activation function\n",
    "    # to capture non-linearity and for symmetry with the \n",
    "    # transmitter.\n",
    "    rx1 = Dense(tx_shapes[-2],activation=ol_activation_func, name=\"rx1\")\\\n",
    "                (noise_flat)\n",
    "    # Hidden Rx Layers\n",
    "    if(num_layers >= 3):\n",
    "        layer_ind = -3\n",
    "    else:\n",
    "        layer_ind = -2\n",
    "    rx_i = Dense(tx_shapes[layer_ind],activation=hl_activation_func, \\\n",
    "                name=\"rx2\")(rx1)\n",
    "    for i in range(2,num_layers):\n",
    "        ind = max(0,num_layers - 2 - i)\n",
    "        rx_i = Dense(tx_shapes[ind],activation=hl_activation_func, \\\n",
    "                    name=(\"rx\"+str(i+1)))(rx_i)\n",
    "    # Dense layer with softmax activation\n",
    "    rx_softmax = Dense(tx_shapes[0],activation='softmax', \\\n",
    "                       name=\"rx_softmax\")(rx_i)\n",
    "    \n",
    "    # Select the symbols with the maximum probabilities\n",
    "    ml_symbs = MostLikelySymbol()(rx_softmax)\n",
    "    \n",
    "    ###Defining the models\n",
    "    autoencoder = Model(input_message, rx_softmax)\n",
    "    ## Model the Tx and Rx seperately as well\n",
    "    # Model the Tx\n",
    "    transmitter = Model(input_message, tx_norm)\n",
    "    # Model the Tx plus the noise\n",
    "    channel_sym_with_noise = Model(input_message, noise)\n",
    "    channel_symbol = Input(shape=(Nr,))\n",
    "    # Take the last layer of the autoencoder model\n",
    "    reciever_layers = autoencoder.layers[-(num_layers+1)](channel_symbol)\n",
    "    for i in range(num_layers):\n",
    "        reciever_layers = autoencoder.layers[-(num_layers-i)](reciever_layers)\n",
    "\n",
    "    # Create a model of the reciever\n",
    "    reciever = Model(channel_symbol, reciever_layers)\n",
    "    autoencoder_symbs = Model(input_message,ml_symbs) \n",
    "    \n",
    "    # Compile the model\n",
    "    autoencoder.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")\n",
    "    return autoencoder, transmitter, reciever,\\\n",
    "            autoencoder_symbs, k, Nc, Nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_constellation_diagram(valid_set, transmitter, name):\n",
    "    channel_symbols = transmitter.predict(valid_set)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(channel_symbols[:,:,0], channel_symbols[:,:,1],\\\n",
    "            'k.')\n",
    "#     ax.set_title(\"CS Constellation Diagram: \"+name)\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    plt.xticks([-2,-1,0,1,2])\n",
    "    plt.yticks([-2,-1,0,1,2])\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_block_error_rate(test_data, pred_symbs):\n",
    "    errors = (test_data != pred_symbs)\n",
    "    block_errors = errors.any(axis=1)\n",
    "    return block_errors.sum()/block_errors.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_sigma(Eb_N0_db, R):\n",
    "    Eb_N0 = 10.**(Eb_N0_db/10.)\n",
    "    return np.sqrt(1./(2.*R*Eb_N0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_set(M, total_size):\n",
    "    t0 = time()\n",
    "    all_one_hot_messages = np.diag(np.ones(M))\n",
    "    perc_train = 0.75\n",
    "    perc_valid = 0.1\n",
    "\n",
    "    ## Making Data Set\n",
    "    multiple = total_size//M\n",
    "    diff = total_size - (multiple * M)\n",
    "\n",
    "    ## Get quotient \n",
    "    ## Converted the array into a list because it is significantly\n",
    "    ## faster\n",
    "    l = []\n",
    "    all_one_hot_messages_lst = all_one_hot_messages.tolist()\n",
    "    for mult in range(multiple):\n",
    "        for i in range(M):\n",
    "            l.append([all_one_hot_messages_lst[i]])\n",
    "    data = np.concatenate(l)\n",
    "\n",
    "    # Add remainder\n",
    "    random_inds = np.random.choice(np.arange(M),size=diff, replace=False)\n",
    "    extra_rows = all_one_hot_messages[random_inds,:]\n",
    "    data = np.concatenate((data, extra_rows), axis=0)\n",
    "    np.random.shuffle(data)\n",
    "    file_path = \"./data/data\"+str(M)+\".npy\"\n",
    "    np.save(file_path, data)\n",
    "    print(f\"Took {time() - t0}s\")\n",
    "    return data, file_path, all_one_hot_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BKSP functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpsk_encode(int_bit):\n",
    "    if(int_bit == 0):\n",
    "        return -1\n",
    "    elif(int_bit == 1):\n",
    "        return 1\n",
    "    else:\n",
    "        assert(False)\n",
    "bpsk_encode_vec = np.vectorize(bpsk_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpsk_decode(float_bit):\n",
    "    if(float_bit <= 0.):\n",
    "        return 0\n",
    "    elif(float_bit >0.):\n",
    "        return 1\n",
    "    else:\n",
    "        assert(False)\n",
    "bpsk_decode_vec = np.vectorize(bpsk_decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hamming 7,4 functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential matrices for Hamming encoding\n",
    "G = np.array(\\\n",
    "             [[1,1,0,1],\n",
    "             [1,0,1,1],\n",
    "             [1,0,0,0],\n",
    "             [0,1,1,1],\n",
    "             [0,1,0,0],\n",
    "             [0,0,1,0],\n",
    "             [0,0,0,1]])\n",
    "H = np.array(\\\n",
    "            [[1,0,1,0,1,0,1],\n",
    "            [0,1,1,0,0,1,1],\n",
    "            [0,0,0,1,1,1,1]])\n",
    "p = np.array([[1],\n",
    "             [0],\n",
    "             [1],\n",
    "             [1]])\n",
    "R_ham = np.array([[0,0,1,0,0,0,0],\n",
    "                 [0,0,0,0,1,0,0],\n",
    "                 [0,0,0,0,0,1,0],\n",
    "                 [0,0,0,0,0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lsb(dec_num):\n",
    "    return float(int(bin(int(dec_num))[-1]))\n",
    "# vectorising the function made it about 40% faster,\n",
    "# not incredible but makes a small difference\n",
    "make_lsb_vec = np.vectorize(make_lsb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_ind(arr_col):\n",
    "    return 4*arr_col[0] + 2*arr_col[1] + arr_col[2] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_error_correction(data, error_inds):\n",
    "    for i in range(data.shape[1]):\n",
    "        if(error_inds[i] >= 0):\n",
    "            data[int(error_inds[i]),i] = float(np.logical_not(data[int(error_inds[i]),i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_7_4_encode(test_data, G):\n",
    "    test_data_enc = np.matmul(G,test_data.T)\n",
    "    return make_lsb_vec(test_data_enc)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_7_4_decode_and_correct(received, H, R):\n",
    "    # Round values to [0,1]\n",
    "    r_round = np.where(received > 0.5, 1, 0)\n",
    "    # Do parity check\n",
    "    parity_check = np.matmul(H,r_round)\n",
    "    parity_check = make_lsb_vec(parity_check)   \n",
    "    # Get error bit indices\n",
    "    error_inds = np.apply_along_axis(get_error_ind, 0,\\\n",
    "                                     parity_check)\n",
    "    # Do error correction\n",
    "    do_error_correction(r_round, error_inds)\n",
    "    # Decode corrected message\n",
    "    return np.matmul(R,r_round).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tSNE_channel_symbols(channel_symbols, dot_size = None):\n",
    "    tSNE_channel_symbols = TSNE(n_components=2).fit_transform(channel_symbols)\n",
    "    # Normalise the dimensionally reduced channel symbols\n",
    "    no_mean = tSNE_channel_symbols - np.mean(tSNE_channel_symbols, axis=0)\n",
    "    avg_power = np.mean(np.sqrt(np.sum(np.square(no_mean),axis=1)))\n",
    "    normalised = no_mean/avg_power\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    if(dot_size == None):\n",
    "        ax.plot(normalised[:,0], normalised[:,1],\\\n",
    "                'k.')\n",
    "    else:\n",
    "        ax.plot(normalised[:,0], normalised[:,1],\\\n",
    "                'k.', markersize=1)\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    plt.xticks([-2,-1,0,1,2])\n",
    "    plt.yticks([-2,-1,0,1,2])\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tSNE_constellation_diagram(valid_set, transmitter, dot_size = None):\n",
    "    channel_symbols = transmitter.predict(valid_set)\n",
    "    plot_tSNE_channel_symbols(channel_symbols, dot_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_noisy_tSNE_constellation_diagram(valid_set, transmitter, num_iters, dot_size = None):\n",
    "    all_symbs = np.empty([0,7])\n",
    "    for i in range(num_iters):\n",
    "        channel_symbols = channel_sym_with_noise7_4.predict(all_one_hot_messages16)\n",
    "        all_symbs = np.vstack([channel_symbols,all_symbs])    \n",
    "    plot_tSNE_channel_symbols(all_symbs, dot_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_bler(M, R, Eb_N0, weights_file_path, test_data, act_f):\n",
    "    ## Get noise std_dev \n",
    "    noise_std = get_noise_sigma(Eb_N0, R)   \n",
    "    ## Make new model with loaded weights\n",
    "    autoencoder_tmp, transmitter_tmp, \\\n",
    "        reciever_tmp, autoencoder_symbs_tmp, \\\n",
    "        k_l, Nc_l, Nr_l \\\n",
    "        = make_model(M, R, noise_std, act_f)\n",
    "    autoencoder_tmp.load_weights(weights_file_path, by_name=True)    \n",
    "    ## Check Accuracy on test set\n",
    "    pred_symbs = autoencoder_symbs_tmp.predict(test_data)\n",
    "    return get_block_error_rate(test_data, pred_symbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_complex_noise_bler(k, Nr, Eb_N0, weights_file_path, test_data, act_f):\n",
    "    ## Get noise std_dev \n",
    "    noise_std = get_noise_sigma(Eb_N0, R)   \n",
    "    ## Make new model with loaded weights\n",
    "    autoencoder_tmp, transmitter_tmp, channel_sym_with_noise_tmp, \\\n",
    "        reciever_tmp, autoencoder_symbs_tmp, k_tmp, Nr_tmp \\\n",
    "        = make_non_complex_channel_model(k, Nr, noise_std, act_f)\n",
    "    autoencoder_tmp.load_weights(weights_file_path, by_name=True)    \n",
    "    ## Check Accuracy on test set\n",
    "    pred_symbs = autoencoder_symbs_tmp.predict(test_data)\n",
    "    return get_block_error_rate(test_data, pred_symbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpsk_get_bler(std, test_data):\n",
    "    bpsk_encoded = bpsk_encode_vec(test_data)\n",
    "    # Add AWGN noise\n",
    "    noise = std * np.random.randn(bpsk_encoded.shape[0],\\\n",
    "                                  bpsk_encoded.shape[1])\n",
    "    received = bpsk_encoded + noise\n",
    "    bpsk_decoded = bpsk_decode_vec(received)\n",
    "\n",
    "    # Get Block error rate\n",
    "    return get_block_error_rate(test_data, bpsk_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do list\n",
    "- DONE/Work out how to add noise even at test time\n",
    "- DONE/Make most likely symbol layer\n",
    "- DONE/Turn all the inputs and outputs into complex numbers \n",
    "- DONE/Fix the normalisation layer, currently for M=2 and N=1 I'm just getting 0 and 1, whereas you'd expect -c and +c. Note this needs to take into account the IQ pairs.\n",
    "- DONE/Make BPSK encoding\n",
    "- DONE/Make Hamming encoding \n",
    "- DONE/Get a graph comparing Hamming encoding, BPSK and an autoencoder\n",
    "- Sweep across different activation functions to see what error and constellation diagrams they get.\n",
    "- Sweep across different batch sizes to see what gives a better model\n",
    "- Sweep across different training noise_stds to find the best for training\n",
    "- Graph didn't look great to try out a (7,4) autoencoder\n",
    "- Implement t-SNE so I can get constellation diagrams for the cases with n>2.\n",
    "\n",
    "- try out different activation functions \n",
    "- try out different numbers of layers\n",
    "- Try out using different activation functions for two halfs of a given layer. This is called Inception networks. \n",
    "- first consider without dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work Log\n",
    "06/02/2019\n",
    "- Changed the normalisation to use the BatchNormalization function <br>\n",
    "\n",
    "07/02/2019\n",
    "- Changed the batch size, increased the data size and changed the axis along which BatchRegularization was done\n",
    "- The results were very inconsistent, reading suggested this might be to do with the ReLu activation function giving dependence on initialisation values due to neuron death.\n",
    "- Consequently tried a leaky-relu activation function\n",
    "- Found a bug that one layer was being bypassed\n",
    "- Switched back to original regularisation layer but over a different axis.\n",
    "- Decided it makes sense to use an l2 regulariser as that reflects a transmitter with a constant power budget\n",
    "- Named all layers so that weights could be loaded from one layer into another\n",
    "- Plotted BLER graph for (2,2) case\n",
    "- Started implementing hamming encoding\n",
    "\n",
    "08/02/2019\n",
    "- Changed the constellation diagram plot to match the O'Shea paper's format.\n",
    "\n",
    "13/02/2019\n",
    "- Added hard decision Hamming encoding and decoding\n",
    "- Added BPSK encoding and decoding\n",
    "- Compared Autoencoder (2,2), BPSK (4,4) and Hamming (7,4) hard decision one one graph\n",
    "\n",
    "14/02/2019\n",
    "- Compared four activation functions to see which one had the best verification error.\n",
    "\n",
    "6-12/05/2019\n",
    "- Compared the leaky-relu activation function against the tanh activation function and found it marginally outperformed it, therefore proceeded from then on with the tanh activation function.\n",
    "- Made a (7,4) autoencoder model and compared it's performance against the others.\n",
    "- Used t-SNE to get constellation diagrams for the (7,4) model, both with and without noise. \n",
    "- Found the t-SNE method of dimensionality reduction introduces some randomness and so the constellation diagram is slightly different each time.\n",
    "- Had to normalise the outputs of the t-SNE dimensionality reduction to zero-mean and unit power, despite it's input being stationary satisfying the afore-mentioned conditions.\n",
    "- Developed (2,2) and (8,8) BPSK, compared these against each other and against the other encoding methods.\n",
    "- Produced Figure's 3a and 3b from the O'Shea paper, however the ordering of the capabilities of the encoding methods is not in the right order. The (8,8) and (7,4) autoencoders underperform, and BPSK over performs.\n",
    "    - The ordering is odd, I would expect it to be from best to worst: (7,4), (2,2), (8,8). But it is actually currently: (2,2), (7,4), (8,8).\n",
    "    - I have yet to find out what Hamming Maximum Likelihood Decision is.\n",
    "- Decided that the reason the (8,8) and (7,4) models might be underperforming may be because of a lack of layers, so tried increasing the number of layers and making the final layer of each block have a tanh activation funciton to try and capture non-linearity.\n",
    "\n",
    "13/05/2019\n",
    "- I realised that my 8_8 was actually and 8_4, so I've adjusted that and I'm retraining the model to see what kind of validation_loss it gets. The best 8_4 accuracy found was 1.06, an initial re-training with no tapering gave a val_loss of 0.0750.\n",
    "\n",
    "14/05/2019\n",
    "- Trained models with 2,3,4 and 5 layers and tapered layer widths.\n",
    "    - Yet to test their performances, they're still training.\n",
    "- Realised that the (7,4) model was being unfairly normalised. The (8,8) and (2,2) models are normalised so that they have unit power for each complex channel symbol. Whereas the (7,4) model is being normalised so that it has unit power for all 7 bits. So I need to multiply all the values post normalisation by sqrt(3.5) to make it even.\n",
    "    - I need to check if this applies to the BPSK stuff as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IQ complex number general notes\n",
    "The symbols at each end are not in IQ form, but the channel symbols need to be in IQ form. <br>\n",
    "Potentially I could do this purely by increasing the number of nodes in the middle. But I'm not sure how I specify the pair relationships.<br>\n",
    "Currently reading about this in EE3-03 Communication System, Lecutre 2.d \"Constellation Diagram & Line Codes...\", slide 15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat with M.Varasteh\n",
    "- first consider without dropout\n",
    "- try out different activation functions \n",
    "- try out different numbers of layers\n",
    "- Try out using different activation functions for two halfs of a given layer. This is called Inception networks. \n",
    "- You can't do this with standard code out of tf and keras, will have to write custom code.\n",
    "- Tanh is usually good because it is linear for small inputs, relu is better for capturing non-linearity \n",
    "- Inception learning allows the optimisation to pick the activation function\n",
    "- Can have a custom activation function which picks is a weighted sum of say relu and tanh.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters I should look at\n",
    "- Activation functions:\n",
    "    - Tanh - ~linear for small inputs \n",
    "    - Relu - captures non linearity well\n",
    "    - Sigmoid, softmax, linear\n",
    "    - Inception learning\n",
    "- Loss functions\n",
    "    - MSE, categorical cross entropy\n",
    "- Regularisation\n",
    "    - Dropout\n",
    "    - Normalisation constraints:\n",
    "        - $\\lVert x\\rVert _2^2 \\leq n$, $\\lvert x_i \\rvert \\leq 1$, $E[\\lvert x_i\\rvert ^2] \\lt 1$\n",
    "- Structure\n",
    "    - Try making it 2d in the first layer vs in the last layer\n",
    "    - Number of layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Ideas\n",
    "- In O'Shea et al they used AWGN with variance = (2*R*E_b/N_0)^-1\n",
    "Later I'd like to try\n",
    "    - Slow and Fast Rayleigh fading \n",
    "    - Try all the types of stuff found in the textbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def Noise Layer\n",
    "# Add noise, probably start with AWGN\n",
    "# Later try\n",
    "# - Slow and Fast Rayleigh fading \n",
    "# - Try all the types of stuff found in the textbook\n",
    "# x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def Reciever\n",
    "# Multiple dense layers\n",
    "# Dense layer with softmax activation\n",
    "# This gives an array of posterior probabilities, so then\n",
    "# pick the maximum of these posterior probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "- Start with M = 2 (k = 1), n = 1 (so R = 1), try and learn BPSK\n",
    "- Let's learn it, have a look at the constellation diagram, then try some bigger M's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the two most_likely_symbol functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17, 0.54, 0.29],\n",
       "       [0.15, 0.01, 0.84],\n",
       "       [0.28, 0.15, 0.57],\n",
       "       [0.52, 0.05, 0.42],\n",
       "       [0.62, 0.03, 0.35]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pps2 = np.random.rand(5,3)\n",
    "totals = np.expand_dims(np.sum(pps2, axis=1), axis=1)\n",
    "pps2 = np.divide(pps2,totals)\n",
    "pps2 = np.round(pps2,2)\n",
    "ppsT = K.constant(pps2)\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    ppsT.eval(session=sess)\n",
    "ppsT.eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_likely(ppsT).eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_likely_symbol(pps2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a model\n",
    "I'm thinking that to express every input as a complex number (in-phase and quaterature) parts, I could run two NN's in parallel, but then it wouldn't be able to learn any links between ther two. <br>\n",
    "Alternatively make it twice as wide, so each input goes in in pairs, and select the most likely symbols from even indices and odd indices seperately, output of that is my complex output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The O'Shea paper trained their models $E_b/N_o = 7db$, so the training variance is calculated below using the formula from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma =  0.22334179607548157\n",
      "WARNING:tensorflow:From /home/apsw/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    }
   ],
   "source": [
    "M = 4 # Number of one hot encoded messages\n",
    "R = 2 # R = k/n\n",
    "\n",
    "sigma = get_noise_sigma(7, R)\n",
    "print(\"sigma = \",sigma)\n",
    "act_f = keras.layers.advanced_activations.LeakyReLU()\n",
    "act_f.__name__ = 'leakyrelu'\n",
    "autoencoder2_2, transmitter2_2, reciever2_2, autoencoder_symbs2_2,\\\n",
    "    k2_2, Nc2_2, Nr2_2 = make_model(M, R, sigma, act_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma =  0.15792649852735607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    }
   ],
   "source": [
    "M = 16 # Number of one hot encoded messages\n",
    "R = 4 # R = k/n_r\n",
    "\n",
    "sigma = get_noise_sigma(7, R)\n",
    "print(\"sigma = \",sigma)\n",
    "autoencoder2_4, transmitter2_4, reciever2_4, autoencoder_symbs2_4,\\\n",
    "    k2_4, Nc2_4, Nr2_4 = make_model(M, R, sigma, act_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma =  0.22334179607548157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    }
   ],
   "source": [
    "M = 2**8 # Number of one hot encoded messages\n",
    "R = 2 # R = k/n_r\n",
    "\n",
    "sigma = get_noise_sigma(7, R)\n",
    "print(\"sigma = \",sigma)\n",
    "autoencoder8_8, transmitter8_8, reciever8_8, autoencoder_symbs8_8,\\\n",
    "    k8_8, Nc8_8, Nr8_8 = make_model(M, R, sigma, act_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma =  0.22334179607548157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    }
   ],
   "source": [
    "k = 4 # Number of one hot encoded messages\n",
    "Nr = 7 # R = k/n_r\n",
    "\n",
    "sigma = get_noise_sigma(7, R)\n",
    "print(\"sigma = \",sigma)\n",
    "autoencoder7_4, transmitter7_4, channel_sym_with_noise7_4, reciever7_4, \\\n",
    "    autoencoder_symbs7_4, k7_4, Nr7_4 \\\n",
    "    = make_non_complex_channel_model(k, Nr, sigma, act_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (8,8)\n",
    "# M = 2**8 # Number of one hot encoded messages\n",
    "# R = 2 # R = k/n_r\n",
    "# sigma = get_noise_sigma(7, R)\n",
    "# hl_activation_func = keras.layers.advanced_activations.LeakyReLU()\n",
    "# hl_activation_func.__name__ = 'leakyrelu'\n",
    "# ol_activation_func = \"tanh\"\n",
    "# num_layers = 3\n",
    "\n",
    "# autoencoder8_8_tap, transmitter8_8_tap, reciever8_8_tap, \\\n",
    "#     autoencoder_symbs8_8_tap, k8_8_tap, Nc8_8_tap, Nr8_8_tap \\\n",
    "#     = make_complex_n_layer_lr_tanh_tapering_model(M, R, sigma, \\\n",
    "#                                                   hl_activation_func, \\\n",
    "#                                                   ol_activation_func, \\\n",
    "#                                                   num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M =  256\n",
      "k =  8.0\n",
      "Nc =  4\n",
      "Nr =  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    }
   ],
   "source": [
    "# (8,8)\n",
    "M = 2**8 # Number of one hot encoded messages\n",
    "R = 2 # R = k/n_r\n",
    "sigma = get_noise_sigma(7, R)\n",
    "hl_activation_func = keras.layers.advanced_activations.LeakyReLU()\n",
    "hl_activation_func.__name__ = 'leakyrelu'\n",
    "ol_activation_func = \"tanh\"\n",
    "num_layers = 2\n",
    "\n",
    "autoencoder8_8_tap_2l, transmitter8_8_tap_2l, reciever8_8_tap_2l, \\\n",
    "    autoencoder_symbs8_8_tap_2l, k8_8_tap_2l, Nc8_8_tap_2l, Nr8_8_tap_2l \\\n",
    "    = make_complex_n_layer_lr_tanh_tapering_model(M, R, sigma, \\\n",
    "                                                  hl_activation_func, \\\n",
    "                                                  ol_activation_func, \\\n",
    "                                                  num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M =  256\n",
      "k =  8.0\n",
      "Nc =  4\n",
      "Nr =  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    }
   ],
   "source": [
    "# (8,8)\n",
    "M = 2**8 # Number of one hot encoded messages\n",
    "R = 2 # R = k/n_r\n",
    "sigma = get_noise_sigma(7, R)\n",
    "hl_activation_func = keras.layers.advanced_activations.LeakyReLU()\n",
    "hl_activation_func.__name__ = 'leakyrelu'\n",
    "ol_activation_func = \"tanh\"\n",
    "num_layers = 3\n",
    "\n",
    "autoencoder8_8_tap_3l, transmitter8_8_tap_3l, reciever8_8_tap_3l, \\\n",
    "    autoencoder_symbs8_8_tap_3l, k8_8_tap_3l, Nc8_8_tap_3l, Nr8_8_tap_3l \\\n",
    "    = make_complex_n_layer_lr_tanh_tapering_model(M, R, sigma, \\\n",
    "                                                  hl_activation_func, \\\n",
    "                                                  ol_activation_func, \\\n",
    "                                                  num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M =  256\n",
      "k =  8.0\n",
      "Nc =  4\n",
      "Nr =  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    }
   ],
   "source": [
    "# (8,8)\n",
    "M = 2**8 # Number of one hot encoded messages\n",
    "R = 2 # R = k/n_r\n",
    "sigma = get_noise_sigma(7, R)\n",
    "hl_activation_func = keras.layers.advanced_activations.LeakyReLU()\n",
    "hl_activation_func.__name__ = 'leakyrelu'\n",
    "ol_activation_func = \"tanh\"\n",
    "num_layers = 4\n",
    "\n",
    "autoencoder8_8_tap_4l, transmitter8_8_tap_4l, reciever8_8_tap_4l, \\\n",
    "    autoencoder_symbs8_8_tap_4l, k8_8_tap_4l, Nc8_8_tap_4l, Nr8_8_tap_4l \\\n",
    "    = make_complex_n_layer_lr_tanh_tapering_model(M, R, sigma, \\\n",
    "                                                  hl_activation_func, \\\n",
    "                                                  ol_activation_func, \\\n",
    "                                                  num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M =  256\n",
      "k =  8.0\n",
      "Nc =  4\n",
      "Nr =  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    }
   ],
   "source": [
    "# (8,8)\n",
    "M = 2**8 # Number of one hot encoded messages\n",
    "R = 2 # R = k/n_r\n",
    "sigma = get_noise_sigma(7, R)\n",
    "hl_activation_func = keras.layers.advanced_activations.LeakyReLU()\n",
    "hl_activation_func.__name__ = 'leakyrelu'\n",
    "ol_activation_func = \"tanh\"\n",
    "num_layers = 5\n",
    "\n",
    "autoencoder8_8_tap_5l, transmitter8_8_tap_5l, reciever8_8_tap_5l, \\\n",
    "    autoencoder_symbs8_8_tap_5l, k8_8_tap_5l, Nc8_8_tap_5l, Nr8_8_tap_5l \\\n",
    "    = make_complex_n_layer_lr_tanh_tapering_model(M, R, sigma, \\\n",
    "                                                  hl_activation_func, \\\n",
    "                                                  ol_activation_func, \\\n",
    "                                                  num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keras multiple GPU models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replicates `model` on 8 GPUs.\n",
    "# # This assumes that your machine has 8 available GPUs.\n",
    "# parallel_model = multi_gpu_model(model, gpus=8)\n",
    "# parallel_model.compile(loss='categorical_crossentropy',\n",
    "#                        optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a toy data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a set of data for a particular M\n",
    "# all_one_hot_messages = np.diag(np.ones(M))\n",
    "# print(\"all_one_hot_messages.shape = \",all_one_hot_messages.shape)\n",
    "# #total_size = 10,000,000 (10M)\n",
    "# total_size = 10000000\n",
    "\n",
    "# # Automatically saves the data for m to a filepath of\n",
    "# # './data/data${M}.npy'\n",
    "# func_data4, file_path4, all_one_hot_messages4 = get_data_set(4, total_size)\n",
    "# func_data16, file_path16, all_one_hot_messages16 = get_data_set(16, total_size)\n",
    "# func_data256, file_path256, all_one_hot_messages256 = get_data_set(256, total_size)\n",
    "\n",
    "# # Don't use this function unless it's for a new M, just\n",
    "# # load the data you have calculated other times.\n",
    "# # This makes results more comparible and saves time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data calculated from previsou runs\n",
    "data256 = np.load('./data/data256.npy')\n",
    "data16 = np.load('./data/data16.npy')\n",
    "data4 = np.load('./data/data4.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data4.shape = (7200000, 4)\n",
      "valid_data4.shape = (800000, 4)\n",
      "test_data4.shape = (2000000, 4)\n"
     ]
    }
   ],
   "source": [
    "# Splitting into training, testing and validation sets\n",
    "train_data4, test_data4 = train_test_split(data4, \\\n",
    "                                         train_size=0.8)\n",
    "train_data4, valid_data4 = train_test_split(train_data4, \\\n",
    "                                         train_size=0.9)\n",
    "print(f\"train_data4.shape = {train_data4.shape}\")\n",
    "print(f\"valid_data4.shape = {valid_data4.shape}\")\n",
    "print(f\"test_data4.shape = {test_data4.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data16.shape = (7200000, 16)\n",
      "valid_data16.shape = (800000, 16)\n",
      "test_data16.shape = (2000000, 16)\n"
     ]
    }
   ],
   "source": [
    "# Splitting into training, testing and validation sets\n",
    "train_data16, test_data16 = train_test_split(data16, \\\n",
    "                                         train_size=0.8)\n",
    "train_data16, valid_data16 = train_test_split(train_data16, \\\n",
    "                                         train_size=0.9)\n",
    "print(f\"train_data16.shape = {train_data16.shape}\")\n",
    "print(f\"valid_data16.shape = {valid_data16.shape}\")\n",
    "print(f\"test_data16.shape = {test_data16.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data256.shape = (72000, 256)\n",
      "valid_data256.shape = (8000, 256)\n",
      "test_data256.shape = (20000, 256)\n"
     ]
    }
   ],
   "source": [
    "# Splitting into training, testing and validation sets\n",
    "train_data256, test_data256 = train_test_split(data256, \\\n",
    "                                         train_size=0.8)\n",
    "train_data256, valid_data256 = train_test_split(train_data256, \\\n",
    "                                         train_size=0.9)\n",
    "print(f\"train_data256.shape = {train_data256.shape}\")\n",
    "print(f\"valid_data256.shape = {valid_data256.shape}\")\n",
    "print(f\"test_data256.shape = {test_data256.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0074 - val_loss: 0.0075\n",
      "Epoch 2/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0074 - val_loss: 0.0072\n",
      "Epoch 3/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0073 - val_loss: 0.0074\n",
      "Epoch 4/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 5/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0072 - val_loss: 0.0070\n",
      "Epoch 6/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 7/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 8/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 9/100\n",
      "7200000/7200000 [==============================] - 10s 1us/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 10/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 11/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 12/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 13/100\n",
      "7200000/7200000 [==============================] - 10s 1us/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 14/100\n",
      "7200000/7200000 [==============================] - 10s 1us/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 15/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 16/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 17/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 18/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 19/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0066 - val_loss: 0.0063\n",
      "Epoch 20/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0065 - val_loss: 0.0064\n",
      "Epoch 21/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0065 - val_loss: 0.0064\n",
      "Epoch 22/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0064 - val_loss: 0.0066\n",
      "Epoch 23/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0065 - val_loss: 0.0063\n",
      "Epoch 24/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0065 - val_loss: 0.0063\n",
      "Epoch 25/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0064 - val_loss: 0.0063\n",
      "Epoch 26/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0063 - val_loss: 0.0062\n",
      "Epoch 27/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0063 - val_loss: 0.0065\n",
      "Epoch 28/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 29/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0063 - val_loss: 0.0062\n",
      "Epoch 30/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0062 - val_loss: 0.0062\n",
      "Epoch 31/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0061 - val_loss: 0.0063\n",
      "Epoch 32/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 33/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0061 - val_loss: 0.0062\n",
      "Epoch 34/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 35/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 36/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 37/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 38/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 39/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0059 - val_loss: 0.0060\n",
      "Epoch 40/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 41/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 42/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 43/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 44/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 45/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 46/100\n",
      "7200000/7200000 [==============================] - 10s 1us/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 47/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 48/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 49/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0057 - val_loss: 0.0056\n",
      "Epoch 50/100\n",
      "7200000/7200000 [==============================] - 8s 1us/step - loss: 0.0057 - val_loss: 0.0056\n",
      "Epoch 51/100\n",
      "7200000/7200000 [==============================] - 10s 1us/step - loss: 0.0056 - val_loss: 0.0057\n",
      "Epoch 52/100\n",
      "7200000/7200000 [==============================] - 10s 1us/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 53/100\n",
      "7200000/7200000 [==============================] - 10s 1us/step - loss: 0.0056 - val_loss: 0.0057\n",
      "Epoch 54/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0055 - val_loss: 0.0056\n",
      "Epoch 55/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 56/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0056 - val_loss: 0.0055\n",
      "Epoch 57/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0055 - val_loss: 0.0056\n",
      "Epoch 58/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 59/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 60/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 61/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0055 - val_loss: 0.0055\n",
      "Epoch 62/100\n",
      "7200000/7200000 [==============================] - 10s 1us/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 63/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0054 - val_loss: 0.0055\n",
      "Epoch 64/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0053 - val_loss: 0.0054\n",
      "Epoch 65/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0053 - val_loss: 0.0054\n",
      "Epoch 66/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0054 - val_loss: 0.0052\n",
      "Epoch 67/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 68/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 69/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 70/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0052 - val_loss: 0.0054\n",
      "Epoch 71/100\n",
      "7200000/7200000 [==============================] - 10s 1us/step - loss: 0.0053 - val_loss: 0.0054\n",
      "Epoch 72/100\n",
      "7200000/7200000 [==============================] - 10s 1us/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 73/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0052 - val_loss: 0.0050\n",
      "Epoch 74/100\n",
      "7200000/7200000 [==============================] - 10s 1us/step - loss: 0.0053 - val_loss: 0.0051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100\n",
      "7200000/7200000 [==============================] - 10s 1us/step - loss: 0.0052 - val_loss: 0.0053\n",
      "Epoch 76/100\n",
      "7200000/7200000 [==============================] - 11s 2us/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 77/100\n",
      "7200000/7200000 [==============================] - 10s 1us/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 78/100\n",
      "7200000/7200000 [==============================] - 11s 1us/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 79/100\n",
      "7200000/7200000 [==============================] - 12s 2us/step - loss: 0.0051 - val_loss: 0.0049\n",
      "Epoch 80/100\n",
      "7200000/7200000 [==============================] - 10s 1us/step - loss: 0.0051 - val_loss: 0.0053\n",
      "Epoch 81/100\n",
      "7200000/7200000 [==============================] - 10s 1us/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 82/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 83/100\n",
      "7200000/7200000 [==============================] - 11s 2us/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 84/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 85/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0050 - val_loss: 0.0051\n",
      "Epoch 86/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0050 - val_loss: 0.0049\n",
      "Epoch 87/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0050 - val_loss: 0.0051\n",
      "Epoch 88/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0050 - val_loss: 0.0048\n",
      "Epoch 89/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0049 - val_loss: 0.0048\n",
      "Epoch 90/100\n",
      "7200000/7200000 [==============================] - 11s 2us/step - loss: 0.0050 - val_loss: 0.0049\n",
      "Epoch 91/100\n",
      "7200000/7200000 [==============================] - 10s 1us/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 92/100\n",
      "7200000/7200000 [==============================] - 10s 1us/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 93/100\n",
      "7200000/7200000 [==============================] - 10s 1us/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 94/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0049 - val_loss: 0.0050\n",
      "Epoch 95/100\n",
      "7200000/7200000 [==============================] - 10s 1us/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 96/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 97/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0050 - val_loss: 0.0048\n",
      "Epoch 98/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 99/100\n",
      "7200000/7200000 [==============================] - 9s 1us/step - loss: 0.0049 - val_loss: 0.0047\n",
      "Epoch 100/100\n",
      "7200000/7200000 [==============================] - 10s 1us/step - loss: 0.0049 - val_loss: 0.0049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f194c503e48>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder2_2.fit(train_data4, train_data4,\n",
    "                epochs=100,\n",
    "                batch_size=1000*M,\n",
    "                shuffle=True,\n",
    "                validation_data=(valid_data4,\n",
    "                                 valid_data4),\n",
    "                callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/100\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.5193 - val_loss: 0.5183\n",
      "Epoch 2/100\n",
      "7200000/7200000 [==============================] - 12s 2us/step - loss: 0.5176 - val_loss: 0.5181\n",
      "Epoch 3/100\n",
      "7200000/7200000 [==============================] - 15s 2us/step - loss: 0.5174 - val_loss: 0.5168\n",
      "Epoch 4/100\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.5174 - val_loss: 0.5178\n",
      "Epoch 5/100\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.5178 - val_loss: 0.5183\n",
      "Epoch 6/100\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.5177 - val_loss: 0.5185\n",
      "Epoch 7/100\n",
      "7200000/7200000 [==============================] - 17s 2us/step - loss: 0.5179 - val_loss: 0.5178\n",
      "Epoch 8/100\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.5174 - val_loss: 0.5186\n",
      "Epoch 9/100\n",
      "7200000/7200000 [==============================] - 13s 2us/step - loss: 0.5175 - val_loss: 0.5176\n",
      "Epoch 10/100\n",
      "7200000/7200000 [==============================] - 13s 2us/step - loss: 0.5172 - val_loss: 0.5180\n",
      "Epoch 11/100\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.5174 - val_loss: 0.5172\n",
      "Epoch 12/100\n",
      "7200000/7200000 [==============================] - 13s 2us/step - loss: 0.5171 - val_loss: 0.5162\n",
      "Epoch 13/100\n",
      "7200000/7200000 [==============================] - 15s 2us/step - loss: 0.5176 - val_loss: 0.5177\n",
      "Epoch 14/100\n",
      "7200000/7200000 [==============================] - 13s 2us/step - loss: 0.5171 - val_loss: 0.5187\n",
      "Epoch 15/100\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.5181 - val_loss: 0.5177\n",
      "Epoch 16/100\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.5178 - val_loss: 0.5180\n",
      "Epoch 17/100\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.5177 - val_loss: 0.5186\n",
      "Epoch 18/100\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.5177 - val_loss: 0.5183\n",
      "Epoch 19/100\n",
      "7200000/7200000 [==============================] - 15s 2us/step - loss: 0.5176 - val_loss: 0.5172\n",
      "Epoch 20/100\n",
      "7200000/7200000 [==============================] - 15s 2us/step - loss: 0.5172 - val_loss: 0.5179\n",
      "Epoch 21/100\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.5172 - val_loss: 0.5182\n",
      "Epoch 22/100\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.5178 - val_loss: 0.5166\n",
      "Epoch 23/100\n",
      "7200000/7200000 [==============================] - 15s 2us/step - loss: 0.5176 - val_loss: 0.5170\n",
      "Epoch 24/100\n",
      "7200000/7200000 [==============================] - 15s 2us/step - loss: 0.5181 - val_loss: 0.5187\n",
      "Epoch 25/100\n",
      "7200000/7200000 [==============================] - 15s 2us/step - loss: 0.5176 - val_loss: 0.5183\n",
      "Epoch 26/100\n",
      "7200000/7200000 [==============================] - 15s 2us/step - loss: 0.5176 - val_loss: 0.5175\n",
      "Epoch 27/100\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.5172 - val_loss: 0.5175\n",
      "Epoch 28/100\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.5175 - val_loss: 0.5177\n",
      "Epoch 29/100\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.5171 - val_loss: 0.5173\n",
      "Epoch 30/100\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.5175 - val_loss: 0.5164\n",
      "Epoch 31/100\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.5173 - val_loss: 0.5178\n",
      "Epoch 32/100\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.5177 - val_loss: 0.5170\n",
      "Epoch 00032: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f385830a5f8>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder2_4.fit(train_data16, train_data16,\n",
    "                epochs=100,\n",
    "                batch_size=1000*M,\n",
    "                shuffle=True,\n",
    "                validation_data=(valid_data16,\n",
    "                                 valid_data16),\n",
    "                callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 8000 samples\n",
      "Epoch 1/1000\n",
      "72000/72000 [==============================] - 2s 28us/step - loss: 0.0145 - val_loss: 0.0367\n",
      "Epoch 2/1000\n",
      "72000/72000 [==============================] - 2s 22us/step - loss: 0.0361 - val_loss: 0.0340\n",
      "Epoch 3/1000\n",
      "72000/72000 [==============================] - 2s 22us/step - loss: 0.0308 - val_loss: 0.0207\n",
      "Epoch 4/1000\n",
      "72000/72000 [==============================] - 2s 22us/step - loss: 0.0212 - val_loss: 0.0223\n",
      "Epoch 5/1000\n",
      "72000/72000 [==============================] - 1s 21us/step - loss: 0.0202 - val_loss: 0.0204\n",
      "Epoch 6/1000\n",
      "72000/72000 [==============================] - 2s 26us/step - loss: 0.0215 - val_loss: 0.0207\n",
      "Epoch 7/1000\n",
      "72000/72000 [==============================] - 2s 22us/step - loss: 0.0223 - val_loss: 0.0245\n",
      "Epoch 8/1000\n",
      "72000/72000 [==============================] - 2s 23us/step - loss: 0.0213 - val_loss: 0.0230\n",
      "Epoch 9/1000\n",
      "72000/72000 [==============================] - 2s 23us/step - loss: 0.0216 - val_loss: 0.0201\n",
      "Epoch 10/1000\n",
      "72000/72000 [==============================] - 2s 22us/step - loss: 0.0206 - val_loss: 0.0188\n",
      "Epoch 11/1000\n",
      "72000/72000 [==============================] - 2s 21us/step - loss: 0.0190 - val_loss: 0.0194\n",
      "Epoch 12/1000\n",
      "72000/72000 [==============================] - 2s 25us/step - loss: 0.0184 - val_loss: 0.0210\n",
      "Epoch 13/1000\n",
      "72000/72000 [==============================] - 2s 25us/step - loss: 0.0182 - val_loss: 0.0171\n",
      "Epoch 14/1000\n",
      "72000/72000 [==============================] - 2s 23us/step - loss: 0.0185 - val_loss: 0.0181\n",
      "Epoch 15/1000\n",
      "72000/72000 [==============================] - 2s 28us/step - loss: 0.0191 - val_loss: 0.0199\n",
      "Epoch 16/1000\n",
      "72000/72000 [==============================] - 2s 31us/step - loss: 0.0176 - val_loss: 0.0176\n",
      "Epoch 17/1000\n",
      "72000/72000 [==============================] - 2s 30us/step - loss: 0.0171 - val_loss: 0.0178\n",
      "Epoch 18/1000\n",
      "72000/72000 [==============================] - 2s 30us/step - loss: 0.0175 - val_loss: 0.0192\n",
      "Epoch 19/1000\n",
      "72000/72000 [==============================] - 2s 25us/step - loss: 0.0169 - val_loss: 0.0173\n",
      "Epoch 20/1000\n",
      "72000/72000 [==============================] - 2s 22us/step - loss: 0.0163 - val_loss: 0.0169\n",
      "Epoch 21/1000\n",
      "72000/72000 [==============================] - 2s 30us/step - loss: 0.0168 - val_loss: 0.0172\n",
      "Epoch 22/1000\n",
      "72000/72000 [==============================] - 2s 26us/step - loss: 0.0164 - val_loss: 0.0142\n",
      "Epoch 23/1000\n",
      "72000/72000 [==============================] - 2s 26us/step - loss: 0.0159 - val_loss: 0.0165\n",
      "Epoch 24/1000\n",
      "72000/72000 [==============================] - 2s 26us/step - loss: 0.0158 - val_loss: 0.0137\n",
      "Epoch 25/1000\n",
      "72000/72000 [==============================] - 2s 29us/step - loss: 0.0152 - val_loss: 0.0162\n",
      "Epoch 26/1000\n",
      "72000/72000 [==============================] - 2s 30us/step - loss: 0.0150 - val_loss: 0.0153\n",
      "Epoch 27/1000\n",
      "72000/72000 [==============================] - 2s 31us/step - loss: 0.0156 - val_loss: 0.0183\n",
      "Epoch 28/1000\n",
      "72000/72000 [==============================] - 2s 29us/step - loss: 0.0144 - val_loss: 0.0143\n",
      "Epoch 29/1000\n",
      "72000/72000 [==============================] - 2s 28us/step - loss: 0.0147 - val_loss: 0.0140\n",
      "Epoch 30/1000\n",
      "72000/72000 [==============================] - 2s 29us/step - loss: 0.0149 - val_loss: 0.0166\n",
      "Epoch 31/1000\n",
      "72000/72000 [==============================] - 2s 30us/step - loss: 0.0146 - val_loss: 0.0152\n",
      "Epoch 32/1000\n",
      "72000/72000 [==============================] - 2s 28us/step - loss: 0.0144 - val_loss: 0.0162\n",
      "Epoch 33/1000\n",
      "72000/72000 [==============================] - 2s 22us/step - loss: 0.0140 - val_loss: 0.0140\n",
      "Epoch 34/1000\n",
      "72000/72000 [==============================] - 2s 21us/step - loss: 0.0142 - val_loss: 0.0136\n",
      "Epoch 35/1000\n",
      "72000/72000 [==============================] - 2s 24us/step - loss: 0.0143 - val_loss: 0.0159\n",
      "Epoch 36/1000\n",
      "72000/72000 [==============================] - 2s 28us/step - loss: 0.0140 - val_loss: 0.0130\n",
      "Epoch 37/1000\n",
      "72000/72000 [==============================] - 2s 28us/step - loss: 0.0143 - val_loss: 0.0119\n",
      "Epoch 38/1000\n",
      "72000/72000 [==============================] - 2s 24us/step - loss: 0.0133 - val_loss: 0.0145\n",
      "Epoch 39/1000\n",
      "72000/72000 [==============================] - 2s 21us/step - loss: 0.0139 - val_loss: 0.0149\n",
      "Epoch 40/1000\n",
      "72000/72000 [==============================] - 2s 21us/step - loss: 0.0131 - val_loss: 0.0130\n",
      "Epoch 41/1000\n",
      "72000/72000 [==============================] - 2s 21us/step - loss: 0.0139 - val_loss: 0.0126\n",
      "Epoch 42/1000\n",
      "72000/72000 [==============================] - 2s 21us/step - loss: 0.0140 - val_loss: 0.0129\n",
      "Epoch 43/1000\n",
      "72000/72000 [==============================] - 2s 22us/step - loss: 0.0140 - val_loss: 0.0123\n",
      "Epoch 44/1000\n",
      "72000/72000 [==============================] - 2s 22us/step - loss: 0.0134 - val_loss: 0.0141\n",
      "Epoch 45/1000\n",
      "72000/72000 [==============================] - 2s 21us/step - loss: 0.0138 - val_loss: 0.0132\n",
      "Epoch 46/1000\n",
      "72000/72000 [==============================] - 2s 22us/step - loss: 0.0138 - val_loss: 0.0141\n",
      "Epoch 47/1000\n",
      "72000/72000 [==============================] - 2s 22us/step - loss: 0.0137 - val_loss: 0.0172\n",
      "Epoch 48/1000\n",
      "72000/72000 [==============================] - 2s 21us/step - loss: 0.0132 - val_loss: 0.0138\n",
      "Epoch 49/1000\n",
      "72000/72000 [==============================] - 2s 21us/step - loss: 0.0131 - val_loss: 0.0131\n",
      "Epoch 50/1000\n",
      "72000/72000 [==============================] - 2s 21us/step - loss: 0.0133 - val_loss: 0.0122\n",
      "Epoch 51/1000\n",
      "72000/72000 [==============================] - 2s 24us/step - loss: 0.0132 - val_loss: 0.0135\n",
      "Epoch 52/1000\n",
      "72000/72000 [==============================] - 2s 23us/step - loss: 0.0131 - val_loss: 0.0135\n",
      "Epoch 53/1000\n",
      "72000/72000 [==============================] - 2s 26us/step - loss: 0.0138 - val_loss: 0.0145\n",
      "Epoch 54/1000\n",
      "72000/72000 [==============================] - 2s 24us/step - loss: 0.0130 - val_loss: 0.0123\n",
      "Epoch 55/1000\n",
      "72000/72000 [==============================] - 2s 23us/step - loss: 0.0129 - val_loss: 0.0145\n",
      "Epoch 56/1000\n",
      "72000/72000 [==============================] - 2s 25us/step - loss: 0.0122 - val_loss: 0.0114\n",
      "Epoch 57/1000\n",
      "72000/72000 [==============================] - 2s 23us/step - loss: 0.0135 - val_loss: 0.0113\n",
      "Epoch 58/1000\n",
      "72000/72000 [==============================] - 2s 23us/step - loss: 0.0130 - val_loss: 0.0111\n",
      "Epoch 59/1000\n",
      "72000/72000 [==============================] - 2s 22us/step - loss: 0.0132 - val_loss: 0.0127\n",
      "Epoch 60/1000\n",
      "72000/72000 [==============================] - 2s 22us/step - loss: 0.0133 - val_loss: 0.0138\n",
      "Epoch 61/1000\n",
      "72000/72000 [==============================] - 2s 22us/step - loss: 0.0126 - val_loss: 0.0121\n",
      "Epoch 62/1000\n",
      "72000/72000 [==============================] - 2s 22us/step - loss: 0.0128 - val_loss: 0.0127\n",
      "Epoch 63/1000\n",
      "72000/72000 [==============================] - 2s 23us/step - loss: 0.0128 - val_loss: 0.0131\n",
      "Epoch 64/1000\n",
      "72000/72000 [==============================] - 2s 24us/step - loss: 0.0128 - val_loss: 0.0107\n",
      "Epoch 65/1000\n",
      "72000/72000 [==============================] - 2s 23us/step - loss: 0.0133 - val_loss: 0.0116\n",
      "Epoch 66/1000\n",
      "72000/72000 [==============================] - 2s 22us/step - loss: 0.0127 - val_loss: 0.0116\n",
      "Epoch 67/1000\n",
      "72000/72000 [==============================] - 2s 22us/step - loss: 0.0125 - val_loss: 0.0123\n",
      "Epoch 68/1000\n",
      "72000/72000 [==============================] - 2s 25us/step - loss: 0.0129 - val_loss: 0.0143\n",
      "Epoch 69/1000\n",
      "72000/72000 [==============================] - 2s 25us/step - loss: 0.0122 - val_loss: 0.0121\n",
      "Epoch 70/1000\n",
      "72000/72000 [==============================] - 2s 24us/step - loss: 0.0123 - val_loss: 0.0138\n",
      "Epoch 71/1000\n",
      "72000/72000 [==============================] - 2s 26us/step - loss: 0.0130 - val_loss: 0.0144\n",
      "Epoch 72/1000\n",
      "72000/72000 [==============================] - 2s 23us/step - loss: 0.0125 - val_loss: 0.0133\n",
      "Epoch 73/1000\n",
      "72000/72000 [==============================] - 2s 22us/step - loss: 0.0125 - val_loss: 0.0144\n",
      "Epoch 74/1000\n",
      "72000/72000 [==============================] - 2s 21us/step - loss: 0.0126 - val_loss: 0.0136\n",
      "Epoch 75/1000\n",
      "72000/72000 [==============================] - 2s 23us/step - loss: 0.0129 - val_loss: 0.0122\n",
      "Epoch 76/1000\n",
      "72000/72000 [==============================] - 2s 24us/step - loss: 0.0127 - val_loss: 0.0122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/1000\n",
      "72000/72000 [==============================] - 2s 22us/step - loss: 0.0129 - val_loss: 0.0120\n",
      "Epoch 78/1000\n",
      "72000/72000 [==============================] - 2s 21us/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 79/1000\n",
      "72000/72000 [==============================] - 2s 25us/step - loss: 0.0124 - val_loss: 0.0132\n",
      "Epoch 80/1000\n",
      "72000/72000 [==============================] - 2s 25us/step - loss: 0.0125 - val_loss: 0.0138\n",
      "Epoch 81/1000\n",
      "72000/72000 [==============================] - 2s 22us/step - loss: 0.0121 - val_loss: 0.0127\n",
      "Epoch 82/1000\n",
      "72000/72000 [==============================] - 2s 25us/step - loss: 0.0127 - val_loss: 0.0117\n",
      "Epoch 83/1000\n",
      "72000/72000 [==============================] - 2s 24us/step - loss: 0.0120 - val_loss: 0.0123\n",
      "Epoch 84/1000\n",
      "72000/72000 [==============================] - 2s 22us/step - loss: 0.0125 - val_loss: 0.0123\n",
      "Epoch 00084: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f38e0170f98>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder8_8.fit(train_data256, train_data256,\n",
    "                epochs=1000,\n",
    "                batch_size=1000*M,\n",
    "                shuffle=True,\n",
    "                validation_data=(valid_data256,\n",
    "                                 valid_data256),\n",
    "                callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# autoencoder8_8_tap.fit(train_data256, train_data256,\n",
    "#                        epochs=1000,\n",
    "#                        batch_size=1000*M,\n",
    "#                        shuffle=True,\n",
    "#                        validation_data=(valid_data256,\n",
    "#                                         valid_data256),\n",
    "#                        callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 8000 samples\n",
      "Epoch 1/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 5.5420 - val_loss: 5.4696\n",
      "Epoch 2/1000\n",
      "72000/72000 [==============================] - 2s 35us/step - loss: 5.4703 - val_loss: 5.4045\n",
      "Epoch 3/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 5.4042 - val_loss: 5.3422\n",
      "Epoch 4/1000\n",
      "72000/72000 [==============================] - 2s 34us/step - loss: 5.3422 - val_loss: 5.2817\n",
      "Epoch 5/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 5.2817 - val_loss: 5.2237\n",
      "Epoch 6/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 5.2234 - val_loss: 5.1602\n",
      "Epoch 7/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 5.1617 - val_loss: 5.0987\n",
      "Epoch 8/1000\n",
      "72000/72000 [==============================] - 2s 33us/step - loss: 5.0992 - val_loss: 5.0322\n",
      "Epoch 9/1000\n",
      "72000/72000 [==============================] - 2s 34us/step - loss: 5.0334 - val_loss: 4.9612\n",
      "Epoch 10/1000\n",
      "72000/72000 [==============================] - 2s 33us/step - loss: 4.9631 - val_loss: 4.8908\n",
      "Epoch 11/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 4.8913 - val_loss: 4.8139\n",
      "Epoch 12/1000\n",
      "72000/72000 [==============================] - 2s 34us/step - loss: 4.8139 - val_loss: 4.7322\n",
      "Epoch 13/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 4.7340 - val_loss: 4.6468\n",
      "Epoch 14/1000\n",
      "72000/72000 [==============================] - 2s 35us/step - loss: 4.6488 - val_loss: 4.5586\n",
      "Epoch 15/1000\n",
      "72000/72000 [==============================] - 3s 36us/step - loss: 4.5591 - val_loss: 4.4660\n",
      "Epoch 16/1000\n",
      "72000/72000 [==============================] - 3s 36us/step - loss: 4.4663 - val_loss: 4.3705\n",
      "Epoch 17/1000\n",
      "72000/72000 [==============================] - 3s 43us/step - loss: 4.3696 - val_loss: 4.2678\n",
      "Epoch 18/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 4.2684 - val_loss: 4.1620\n",
      "Epoch 19/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 4.1650 - val_loss: 4.0558\n",
      "Epoch 20/1000\n",
      "72000/72000 [==============================] - 3s 49us/step - loss: 4.0572 - val_loss: 3.9462\n",
      "Epoch 21/1000\n",
      "72000/72000 [==============================] - 3s 44us/step - loss: 3.9473 - val_loss: 3.8334\n",
      "Epoch 22/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 3.8351 - val_loss: 3.7151\n",
      "Epoch 23/1000\n",
      "72000/72000 [==============================] - 3s 42us/step - loss: 3.7178 - val_loss: 3.5974\n",
      "Epoch 24/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 3.5998 - val_loss: 3.4764\n",
      "Epoch 25/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 3.4801 - val_loss: 3.3546\n",
      "Epoch 26/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 3.3579 - val_loss: 3.2298\n",
      "Epoch 27/1000\n",
      "72000/72000 [==============================] - 3s 43us/step - loss: 3.2339 - val_loss: 3.1106\n",
      "Epoch 28/1000\n",
      "72000/72000 [==============================] - 3s 44us/step - loss: 3.1109 - val_loss: 2.9821\n",
      "Epoch 29/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 2.9841 - val_loss: 2.8574\n",
      "Epoch 30/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 2.8587 - val_loss: 2.7380\n",
      "Epoch 31/1000\n",
      "72000/72000 [==============================] - 3s 42us/step - loss: 2.7349 - val_loss: 2.6121\n",
      "Epoch 32/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 2.6120 - val_loss: 2.4926\n",
      "Epoch 33/1000\n",
      "72000/72000 [==============================] - 3s 43us/step - loss: 2.4869 - val_loss: 2.3695\n",
      "Epoch 34/1000\n",
      "72000/72000 [==============================] - 3s 44us/step - loss: 2.3662 - val_loss: 2.2420\n",
      "Epoch 35/1000\n",
      "72000/72000 [==============================] - 3s 42us/step - loss: 2.2464 - val_loss: 2.1336\n",
      "Epoch 36/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 2.1303 - val_loss: 2.0182\n",
      "Epoch 37/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 2.0146 - val_loss: 1.9014\n",
      "Epoch 38/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 1.9031 - val_loss: 1.7987\n",
      "Epoch 39/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 1.7971 - val_loss: 1.6935\n",
      "Epoch 40/1000\n",
      "72000/72000 [==============================] - 3s 42us/step - loss: 1.6884 - val_loss: 1.5908\n",
      "Epoch 41/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 1.5897 - val_loss: 1.4951\n",
      "Epoch 42/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 1.4942 - val_loss: 1.3980\n",
      "Epoch 43/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 1.3999 - val_loss: 1.3116\n",
      "Epoch 44/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 1.3126 - val_loss: 1.2330\n",
      "Epoch 45/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 1.2270 - val_loss: 1.1476\n",
      "Epoch 46/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 1.1491 - val_loss: 1.0772\n",
      "Epoch 47/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 1.0736 - val_loss: 1.0033\n",
      "Epoch 48/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 1.0026 - val_loss: 0.9388\n",
      "Epoch 49/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 0.9355 - val_loss: 0.8691\n",
      "Epoch 50/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.8727 - val_loss: 0.8179\n",
      "Epoch 51/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.8152 - val_loss: 0.7603\n",
      "Epoch 52/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.7584 - val_loss: 0.7112\n",
      "Epoch 53/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.7097 - val_loss: 0.6600\n",
      "Epoch 54/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.6610 - val_loss: 0.6141\n",
      "Epoch 55/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 0.6150 - val_loss: 0.5739\n",
      "Epoch 56/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.5743 - val_loss: 0.5365\n",
      "Epoch 57/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.5353 - val_loss: 0.4950\n",
      "Epoch 58/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.4985 - val_loss: 0.4635\n",
      "Epoch 59/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 0.4674 - val_loss: 0.4398\n",
      "Epoch 60/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.4346 - val_loss: 0.4093\n",
      "Epoch 61/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.4074 - val_loss: 0.3812\n",
      "Epoch 62/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.3810 - val_loss: 0.3552\n",
      "Epoch 63/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 0.3558 - val_loss: 0.3355\n",
      "Epoch 64/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 0.3345 - val_loss: 0.3118\n",
      "Epoch 65/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 0.3140 - val_loss: 0.2950\n",
      "Epoch 66/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.2958 - val_loss: 0.2814\n",
      "Epoch 67/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 0.2779 - val_loss: 0.2659\n",
      "Epoch 68/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.2628 - val_loss: 0.2485\n",
      "Epoch 69/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.2481 - val_loss: 0.2351\n",
      "Epoch 70/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.2343 - val_loss: 0.2238\n",
      "Epoch 71/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.2222 - val_loss: 0.2111\n",
      "Epoch 72/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.2112 - val_loss: 0.1994\n",
      "Epoch 73/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 0.2022 - val_loss: 0.1879\n",
      "Epoch 74/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.1920 - val_loss: 0.1858\n",
      "Epoch 75/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.1835 - val_loss: 0.1746\n",
      "Epoch 76/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.1766 - val_loss: 0.1693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 0.1675 - val_loss: 0.1589\n",
      "Epoch 78/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.1598 - val_loss: 0.1534\n",
      "Epoch 79/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.1556 - val_loss: 0.1448\n",
      "Epoch 80/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.1479 - val_loss: 0.1438\n",
      "Epoch 81/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.1433 - val_loss: 0.1422\n",
      "Epoch 82/1000\n",
      "72000/72000 [==============================] - 3s 43us/step - loss: 0.1376 - val_loss: 0.1322\n",
      "Epoch 83/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.1332 - val_loss: 0.1279\n",
      "Epoch 84/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.1283 - val_loss: 0.1252\n",
      "Epoch 85/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 0.1245 - val_loss: 0.1202\n",
      "Epoch 86/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.1198 - val_loss: 0.1163\n",
      "Epoch 87/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.1165 - val_loss: 0.1132\n",
      "Epoch 88/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.1137 - val_loss: 0.1104\n",
      "Epoch 89/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.1091 - val_loss: 0.1082\n",
      "Epoch 90/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.1073 - val_loss: 0.1070\n",
      "Epoch 91/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.1041 - val_loss: 0.1024\n",
      "Epoch 92/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.1022 - val_loss: 0.0988\n",
      "Epoch 93/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0995 - val_loss: 0.0987\n",
      "Epoch 94/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0963 - val_loss: 0.0935\n",
      "Epoch 95/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0952 - val_loss: 0.0936\n",
      "Epoch 96/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0923 - val_loss: 0.0884\n",
      "Epoch 97/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0905 - val_loss: 0.0871\n",
      "Epoch 98/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0882 - val_loss: 0.0842\n",
      "Epoch 99/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0868 - val_loss: 0.0826\n",
      "Epoch 100/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0851 - val_loss: 0.0816\n",
      "Epoch 101/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0832 - val_loss: 0.0796\n",
      "Epoch 102/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0818 - val_loss: 0.0797\n",
      "Epoch 103/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0799 - val_loss: 0.0825\n",
      "Epoch 104/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0784 - val_loss: 0.0747\n",
      "Epoch 105/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 0.0768 - val_loss: 0.0729\n",
      "Epoch 106/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0750 - val_loss: 0.0734\n",
      "Epoch 107/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0740 - val_loss: 0.0750\n",
      "Epoch 108/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0729 - val_loss: 0.0740\n",
      "Epoch 109/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 110/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0706 - val_loss: 0.0700\n",
      "Epoch 111/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 0.0692 - val_loss: 0.0693\n",
      "Epoch 112/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0685 - val_loss: 0.0654\n",
      "Epoch 113/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0671 - val_loss: 0.0687\n",
      "Epoch 114/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 0.0662 - val_loss: 0.0654\n",
      "Epoch 115/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 0.0657 - val_loss: 0.0664\n",
      "Epoch 116/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0650 - val_loss: 0.0663\n",
      "Epoch 117/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0629 - val_loss: 0.0632\n",
      "Epoch 118/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0624 - val_loss: 0.0627\n",
      "Epoch 119/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0621 - val_loss: 0.0591\n",
      "Epoch 120/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0610 - val_loss: 0.0625\n",
      "Epoch 121/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0597 - val_loss: 0.0605\n",
      "Epoch 122/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0588 - val_loss: 0.0578\n",
      "Epoch 123/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0585 - val_loss: 0.0574\n",
      "Epoch 124/1000\n",
      "72000/72000 [==============================] - 3s 43us/step - loss: 0.0580 - val_loss: 0.0567\n",
      "Epoch 125/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0575 - val_loss: 0.0576\n",
      "Epoch 126/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0565 - val_loss: 0.0567\n",
      "Epoch 127/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 0.0559 - val_loss: 0.0563\n",
      "Epoch 128/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0553 - val_loss: 0.0576\n",
      "Epoch 129/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0540 - val_loss: 0.0517\n",
      "Epoch 130/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0539 - val_loss: 0.0523\n",
      "Epoch 131/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0530 - val_loss: 0.0537\n",
      "Epoch 132/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0532 - val_loss: 0.0533\n",
      "Epoch 133/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0522 - val_loss: 0.0511\n",
      "Epoch 134/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0519 - val_loss: 0.0505\n",
      "Epoch 135/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0507 - val_loss: 0.0521\n",
      "Epoch 136/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0506 - val_loss: 0.0496\n",
      "Epoch 137/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0499 - val_loss: 0.0482\n",
      "Epoch 138/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0497 - val_loss: 0.0483\n",
      "Epoch 139/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0493 - val_loss: 0.0481\n",
      "Epoch 140/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0495 - val_loss: 0.0461\n",
      "Epoch 141/1000\n",
      "72000/72000 [==============================] - 3s 42us/step - loss: 0.0473 - val_loss: 0.0458\n",
      "Epoch 142/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0476 - val_loss: 0.0464\n",
      "Epoch 143/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0467 - val_loss: 0.0485\n",
      "Epoch 144/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0468 - val_loss: 0.0470\n",
      "Epoch 145/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0456 - val_loss: 0.0481\n",
      "Epoch 146/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0447 - val_loss: 0.0441\n",
      "Epoch 147/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0452 - val_loss: 0.0441\n",
      "Epoch 148/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 0.0452 - val_loss: 0.0438\n",
      "Epoch 149/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 0.0434 - val_loss: 0.0433\n",
      "Epoch 150/1000\n",
      "72000/72000 [==============================] - 2s 34us/step - loss: 0.0441 - val_loss: 0.0431\n",
      "Epoch 151/1000\n",
      "72000/72000 [==============================] - 2s 34us/step - loss: 0.0431 - val_loss: 0.0414\n",
      "Epoch 152/1000\n",
      "72000/72000 [==============================] - 2s 35us/step - loss: 0.0428 - val_loss: 0.0425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0436 - val_loss: 0.0419\n",
      "Epoch 154/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 0.0430 - val_loss: 0.0397\n",
      "Epoch 155/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 0.0421 - val_loss: 0.0420\n",
      "Epoch 156/1000\n",
      "72000/72000 [==============================] - 2s 34us/step - loss: 0.0416 - val_loss: 0.0403\n",
      "Epoch 157/1000\n",
      "72000/72000 [==============================] - 2s 33us/step - loss: 0.0410 - val_loss: 0.0420\n",
      "Epoch 158/1000\n",
      "72000/72000 [==============================] - 2s 35us/step - loss: 0.0408 - val_loss: 0.0391\n",
      "Epoch 159/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 0.0403 - val_loss: 0.0407\n",
      "Epoch 160/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 0.0401 - val_loss: 0.0401\n",
      "Epoch 161/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 0.0404 - val_loss: 0.0402\n",
      "Epoch 162/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 0.0397 - val_loss: 0.0412\n",
      "Epoch 163/1000\n",
      "72000/72000 [==============================] - 2s 34us/step - loss: 0.0392 - val_loss: 0.0394\n",
      "Epoch 164/1000\n",
      "72000/72000 [==============================] - 2s 34us/step - loss: 0.0392 - val_loss: 0.0386\n",
      "Epoch 165/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 0.0377 - val_loss: 0.0386\n",
      "Epoch 166/1000\n",
      "72000/72000 [==============================] - 2s 35us/step - loss: 0.0383 - val_loss: 0.0371\n",
      "Epoch 167/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 0.0382 - val_loss: 0.0385\n",
      "Epoch 168/1000\n",
      "72000/72000 [==============================] - 3s 36us/step - loss: 0.0381 - val_loss: 0.0371\n",
      "Epoch 169/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 0.0376 - val_loss: 0.0367\n",
      "Epoch 170/1000\n",
      "72000/72000 [==============================] - 2s 34us/step - loss: 0.0367 - val_loss: 0.0373\n",
      "Epoch 171/1000\n",
      "72000/72000 [==============================] - 3s 36us/step - loss: 0.0362 - val_loss: 0.0350\n",
      "Epoch 172/1000\n",
      "72000/72000 [==============================] - 3s 36us/step - loss: 0.0363 - val_loss: 0.0374\n",
      "Epoch 173/1000\n",
      "72000/72000 [==============================] - 3s 36us/step - loss: 0.0356 - val_loss: 0.0355\n",
      "Epoch 174/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 0.0354 - val_loss: 0.0349\n",
      "Epoch 175/1000\n",
      "72000/72000 [==============================] - 3s 36us/step - loss: 0.0358 - val_loss: 0.0341\n",
      "Epoch 176/1000\n",
      "72000/72000 [==============================] - 3s 36us/step - loss: 0.0354 - val_loss: 0.0367\n",
      "Epoch 177/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 0.0358 - val_loss: 0.0350\n",
      "Epoch 178/1000\n",
      "72000/72000 [==============================] - 2s 34us/step - loss: 0.0349 - val_loss: 0.0368\n",
      "Epoch 179/1000\n",
      "72000/72000 [==============================] - 2s 35us/step - loss: 0.0346 - val_loss: 0.0356\n",
      "Epoch 180/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0351 - val_loss: 0.0338\n",
      "Epoch 181/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0341 - val_loss: 0.0354\n",
      "Epoch 182/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 0.0336 - val_loss: 0.0337\n",
      "Epoch 183/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0336 - val_loss: 0.0329\n",
      "Epoch 184/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 0.0340 - val_loss: 0.0331\n",
      "Epoch 185/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0329 - val_loss: 0.0326\n",
      "Epoch 186/1000\n",
      "72000/72000 [==============================] - 3s 35us/step - loss: 0.0334 - val_loss: 0.0338\n",
      "Epoch 187/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0331 - val_loss: 0.0332\n",
      "Epoch 188/1000\n",
      "72000/72000 [==============================] - 3s 36us/step - loss: 0.0329 - val_loss: 0.0315\n",
      "Epoch 189/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0323 - val_loss: 0.0317\n",
      "Epoch 190/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0315 - val_loss: 0.0306\n",
      "Epoch 191/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0312 - val_loss: 0.0312\n",
      "Epoch 192/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0318 - val_loss: 0.0320\n",
      "Epoch 193/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0322 - val_loss: 0.0314\n",
      "Epoch 194/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0318 - val_loss: 0.0307\n",
      "Epoch 195/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0314 - val_loss: 0.0303\n",
      "Epoch 196/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0309 - val_loss: 0.0324\n",
      "Epoch 197/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0309 - val_loss: 0.0304\n",
      "Epoch 198/1000\n",
      "72000/72000 [==============================] - 3s 36us/step - loss: 0.0308 - val_loss: 0.0295\n",
      "Epoch 199/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0306 - val_loss: 0.0314\n",
      "Epoch 200/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0305 - val_loss: 0.0290\n",
      "Epoch 201/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0298 - val_loss: 0.0299\n",
      "Epoch 202/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0295 - val_loss: 0.0296\n",
      "Epoch 203/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0296 - val_loss: 0.0325\n",
      "Epoch 204/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0293 - val_loss: 0.0302\n",
      "Epoch 205/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0301 - val_loss: 0.0278\n",
      "Epoch 206/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0291 - val_loss: 0.0290\n",
      "Epoch 207/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0285 - val_loss: 0.0298\n",
      "Epoch 208/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0283 - val_loss: 0.0292\n",
      "Epoch 209/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0290 - val_loss: 0.0289\n",
      "Epoch 210/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0286 - val_loss: 0.0281\n",
      "Epoch 211/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0282 - val_loss: 0.0290\n",
      "Epoch 212/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0283 - val_loss: 0.0274\n",
      "Epoch 213/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 0.0276 - val_loss: 0.0282\n",
      "Epoch 214/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0278 - val_loss: 0.0275\n",
      "Epoch 215/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0281 - val_loss: 0.0273\n",
      "Epoch 216/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0277 - val_loss: 0.0276\n",
      "Epoch 217/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0276 - val_loss: 0.0265\n",
      "Epoch 218/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0273 - val_loss: 0.0268\n",
      "Epoch 219/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0273 - val_loss: 0.0260\n",
      "Epoch 220/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0273 - val_loss: 0.0270\n",
      "Epoch 221/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0270 - val_loss: 0.0280\n",
      "Epoch 222/1000\n",
      "72000/72000 [==============================] - 3s 36us/step - loss: 0.0263 - val_loss: 0.0262\n",
      "Epoch 223/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0267 - val_loss: 0.0260\n",
      "Epoch 224/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0266 - val_loss: 0.0262\n",
      "Epoch 225/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0263 - val_loss: 0.0258\n",
      "Epoch 226/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0261 - val_loss: 0.0250\n",
      "Epoch 227/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0262 - val_loss: 0.0249\n",
      "Epoch 228/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0262 - val_loss: 0.0269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0257 - val_loss: 0.0261\n",
      "Epoch 230/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0255 - val_loss: 0.0237\n",
      "Epoch 231/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0260 - val_loss: 0.0268\n",
      "Epoch 232/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0258 - val_loss: 0.0241\n",
      "Epoch 233/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0247 - val_loss: 0.0245\n",
      "Epoch 234/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0251 - val_loss: 0.0241\n",
      "Epoch 235/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0254 - val_loss: 0.0246\n",
      "Epoch 236/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0251 - val_loss: 0.0252\n",
      "Epoch 237/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0251 - val_loss: 0.0254\n",
      "Epoch 238/1000\n",
      "72000/72000 [==============================] - 3s 36us/step - loss: 0.0244 - val_loss: 0.0238\n",
      "Epoch 239/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0248 - val_loss: 0.0249\n",
      "Epoch 240/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0245 - val_loss: 0.0256\n",
      "Epoch 241/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0249 - val_loss: 0.0232\n",
      "Epoch 242/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0243 - val_loss: 0.0237\n",
      "Epoch 243/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0242 - val_loss: 0.0242\n",
      "Epoch 244/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0238 - val_loss: 0.0233\n",
      "Epoch 245/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0240 - val_loss: 0.0237\n",
      "Epoch 246/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0238 - val_loss: 0.0246\n",
      "Epoch 247/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0238 - val_loss: 0.0253\n",
      "Epoch 248/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0226 - val_loss: 0.0235\n",
      "Epoch 249/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0237 - val_loss: 0.0246\n",
      "Epoch 250/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0237 - val_loss: 0.0212\n",
      "Epoch 251/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0239 - val_loss: 0.0224\n",
      "Epoch 252/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0236 - val_loss: 0.0224\n",
      "Epoch 253/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0226 - val_loss: 0.0219\n",
      "Epoch 254/1000\n",
      "72000/72000 [==============================] - 3s 36us/step - loss: 0.0231 - val_loss: 0.0230\n",
      "Epoch 255/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0230 - val_loss: 0.0224\n",
      "Epoch 256/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0223 - val_loss: 0.0224\n",
      "Epoch 257/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0233 - val_loss: 0.0228\n",
      "Epoch 258/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0229 - val_loss: 0.0212\n",
      "Epoch 259/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0226 - val_loss: 0.0223\n",
      "Epoch 260/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0222 - val_loss: 0.0224\n",
      "Epoch 261/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0219 - val_loss: 0.0224\n",
      "Epoch 262/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0219 - val_loss: 0.0208\n",
      "Epoch 263/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0223 - val_loss: 0.0219\n",
      "Epoch 264/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0222 - val_loss: 0.0244\n",
      "Epoch 265/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0219 - val_loss: 0.0225\n",
      "Epoch 266/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0220 - val_loss: 0.0211\n",
      "Epoch 267/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0213 - val_loss: 0.0213\n",
      "Epoch 268/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0218 - val_loss: 0.0227\n",
      "Epoch 269/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0211 - val_loss: 0.0220\n",
      "Epoch 270/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0213 - val_loss: 0.0207\n",
      "Epoch 271/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0213 - val_loss: 0.0186\n",
      "Epoch 272/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0219 - val_loss: 0.0229\n",
      "Epoch 273/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0212 - val_loss: 0.0230\n",
      "Epoch 274/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0215 - val_loss: 0.0208\n",
      "Epoch 275/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0213 - val_loss: 0.0216\n",
      "Epoch 276/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0205 - val_loss: 0.0202\n",
      "Epoch 277/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0206 - val_loss: 0.0230\n",
      "Epoch 278/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0209 - val_loss: 0.0211\n",
      "Epoch 279/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0213 - val_loss: 0.0204\n",
      "Epoch 280/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0208 - val_loss: 0.0226\n",
      "Epoch 281/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0205 - val_loss: 0.0207\n",
      "Epoch 282/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0206 - val_loss: 0.0221\n",
      "Epoch 283/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0212 - val_loss: 0.0208\n",
      "Epoch 284/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0202 - val_loss: 0.0209\n",
      "Epoch 285/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0206 - val_loss: 0.0186\n",
      "Epoch 286/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0204 - val_loss: 0.0197\n",
      "Epoch 287/1000\n",
      "72000/72000 [==============================] - 3s 36us/step - loss: 0.0203 - val_loss: 0.0191\n",
      "Epoch 288/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0201 - val_loss: 0.0220\n",
      "Epoch 289/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0197 - val_loss: 0.0195\n",
      "Epoch 290/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0200 - val_loss: 0.0202\n",
      "Epoch 291/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0196 - val_loss: 0.0208\n",
      "Epoch 292/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0194 - val_loss: 0.0201\n",
      "Epoch 293/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0199 - val_loss: 0.0196\n",
      "Epoch 294/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0200 - val_loss: 0.0184\n",
      "Epoch 295/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0197 - val_loss: 0.0190\n",
      "Epoch 296/1000\n",
      "72000/72000 [==============================] - 3s 36us/step - loss: 0.0192 - val_loss: 0.0193\n",
      "Epoch 297/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0193 - val_loss: 0.0195\n",
      "Epoch 298/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0190 - val_loss: 0.0194\n",
      "Epoch 299/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0192 - val_loss: 0.0194\n",
      "Epoch 300/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0189 - val_loss: 0.0197\n",
      "Epoch 301/1000\n",
      "72000/72000 [==============================] - 3s 36us/step - loss: 0.0195 - val_loss: 0.0196\n",
      "Epoch 302/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.0191 - val_loss: 0.0201\n",
      "Epoch 303/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0192 - val_loss: 0.0193\n",
      "Epoch 304/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0194 - val_loss: 0.0172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 305/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0186 - val_loss: 0.0197\n",
      "Epoch 306/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0192 - val_loss: 0.0189\n",
      "Epoch 307/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0190 - val_loss: 0.0178\n",
      "Epoch 308/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0185 - val_loss: 0.0197\n",
      "Epoch 309/1000\n",
      "72000/72000 [==============================] - 3s 36us/step - loss: 0.0188 - val_loss: 0.0195\n",
      "Epoch 310/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0183 - val_loss: 0.0205\n",
      "Epoch 311/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0186 - val_loss: 0.0194\n",
      "Epoch 312/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0189 - val_loss: 0.0189\n",
      "Epoch 313/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0180 - val_loss: 0.0199\n",
      "Epoch 314/1000\n",
      "72000/72000 [==============================] - 3s 36us/step - loss: 0.0190 - val_loss: 0.0190\n",
      "Epoch 315/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0189 - val_loss: 0.0176\n",
      "Epoch 316/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0180 - val_loss: 0.0196\n",
      "Epoch 317/1000\n",
      "72000/72000 [==============================] - 3s 39us/step - loss: 0.0181 - val_loss: 0.0203\n",
      "Epoch 318/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0184 - val_loss: 0.0159\n",
      "Epoch 319/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0180 - val_loss: 0.0179\n",
      "Epoch 320/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0186 - val_loss: 0.0168\n",
      "Epoch 321/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0185 - val_loss: 0.0177\n",
      "Epoch 322/1000\n",
      "72000/72000 [==============================] - 3s 36us/step - loss: 0.0184 - val_loss: 0.0164\n",
      "Epoch 323/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0176 - val_loss: 0.0174\n",
      "Epoch 324/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0177 - val_loss: 0.0177\n",
      "Epoch 325/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0175 - val_loss: 0.0185\n",
      "Epoch 326/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0179 - val_loss: 0.0164\n",
      "Epoch 327/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0172 - val_loss: 0.0171\n",
      "Epoch 328/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0169 - val_loss: 0.0192\n",
      "Epoch 329/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0182 - val_loss: 0.0179\n",
      "Epoch 330/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0175 - val_loss: 0.0171\n",
      "Epoch 331/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0176 - val_loss: 0.0171\n",
      "Epoch 332/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0179 - val_loss: 0.0162\n",
      "Epoch 333/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0178 - val_loss: 0.0183\n",
      "Epoch 334/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0173 - val_loss: 0.0173\n",
      "Epoch 335/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0167 - val_loss: 0.0161\n",
      "Epoch 336/1000\n",
      "72000/72000 [==============================] - 3s 37us/step - loss: 0.0177 - val_loss: 0.0184\n",
      "Epoch 337/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0163 - val_loss: 0.0184\n",
      "Epoch 338/1000\n",
      "72000/72000 [==============================] - 3s 38us/step - loss: 0.0174 - val_loss: 0.0166\n",
      "Epoch 00338: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f384d020cc0>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder8_8_tap_2l.fit(train_data256, train_data256,\n",
    "                       epochs=1000,\n",
    "                       batch_size=1000*M,\n",
    "                       shuffle=True,\n",
    "                       validation_data=(valid_data256,\n",
    "                                        valid_data256),\n",
    "                       callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 8000 samples\n",
      "Epoch 1/1000\n",
      "72000/72000 [==============================] - 9s 119us/step - loss: 5.5453 - val_loss: 5.4861\n",
      "Epoch 2/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 5.4865 - val_loss: 5.4349\n",
      "Epoch 3/1000\n",
      "72000/72000 [==============================] - 3s 42us/step - loss: 5.4346 - val_loss: 5.3781\n",
      "Epoch 4/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 5.3767 - val_loss: 5.3211\n",
      "Epoch 5/1000\n",
      "72000/72000 [==============================] - 3s 43us/step - loss: 5.3196 - val_loss: 5.2632\n",
      "Epoch 6/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 5.2615 - val_loss: 5.1938\n",
      "Epoch 7/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 5.1919 - val_loss: 5.1167\n",
      "Epoch 8/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 5.1153 - val_loss: 5.0354\n",
      "Epoch 9/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 5.0321 - val_loss: 4.9454\n",
      "Epoch 10/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 4.9421 - val_loss: 4.8464\n",
      "Epoch 11/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 4.8431 - val_loss: 4.7410\n",
      "Epoch 12/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 4.7367 - val_loss: 4.6264\n",
      "Epoch 13/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 4.6221 - val_loss: 4.5054\n",
      "Epoch 14/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 4.4992 - val_loss: 4.3766\n",
      "Epoch 15/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 4.3690 - val_loss: 4.2391\n",
      "Epoch 16/1000\n",
      "72000/72000 [==============================] - 3s 47us/step - loss: 4.2322 - val_loss: 4.0989\n",
      "Epoch 17/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 4.0905 - val_loss: 3.9489\n",
      "Epoch 18/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 3.9411 - val_loss: 3.7940\n",
      "Epoch 19/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 3.7883 - val_loss: 3.6394\n",
      "Epoch 20/1000\n",
      "72000/72000 [==============================] - 4s 51us/step - loss: 3.6315 - val_loss: 3.4844\n",
      "Epoch 21/1000\n",
      "72000/72000 [==============================] - 3s 48us/step - loss: 3.4719 - val_loss: 3.3217\n",
      "Epoch 22/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 3.3096 - val_loss: 3.1583\n",
      "Epoch 23/1000\n",
      "72000/72000 [==============================] - 3s 48us/step - loss: 3.1500 - val_loss: 2.9999\n",
      "Epoch 24/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 2.9871 - val_loss: 2.8276\n",
      "Epoch 25/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 2.8166 - val_loss: 2.6645\n",
      "Epoch 26/1000\n",
      "72000/72000 [==============================] - 3s 44us/step - loss: 2.6528 - val_loss: 2.4910\n",
      "Epoch 27/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 2.4817 - val_loss: 2.3320\n",
      "Epoch 28/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 2.3231 - val_loss: 2.1700\n",
      "Epoch 29/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 2.1623 - val_loss: 2.0116\n",
      "Epoch 30/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 2.0056 - val_loss: 1.8617\n",
      "Epoch 31/1000\n",
      "72000/72000 [==============================] - 3s 42us/step - loss: 1.8566 - val_loss: 1.7230\n",
      "Epoch 32/1000\n",
      "72000/72000 [==============================] - 3s 47us/step - loss: 1.7097 - val_loss: 1.5821\n",
      "Epoch 33/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 1.5744 - val_loss: 1.4448\n",
      "Epoch 34/1000\n",
      "72000/72000 [==============================] - 3s 43us/step - loss: 1.4387 - val_loss: 1.3201\n",
      "Epoch 35/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 1.3131 - val_loss: 1.2062\n",
      "Epoch 36/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 1.1966 - val_loss: 1.0875\n",
      "Epoch 37/1000\n",
      "72000/72000 [==============================] - 4s 56us/step - loss: 1.0869 - val_loss: 0.9894\n",
      "Epoch 38/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.9841 - val_loss: 0.8950\n",
      "Epoch 39/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.8881 - val_loss: 0.8060\n",
      "Epoch 40/1000\n",
      "72000/72000 [==============================] - 4s 51us/step - loss: 0.8043 - val_loss: 0.7239\n",
      "Epoch 41/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.7208 - val_loss: 0.6566\n",
      "Epoch 42/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.6463 - val_loss: 0.5854\n",
      "Epoch 43/1000\n",
      "72000/72000 [==============================] - 3s 43us/step - loss: 0.5822 - val_loss: 0.5221\n",
      "Epoch 44/1000\n",
      "72000/72000 [==============================] - 3s 44us/step - loss: 0.5218 - val_loss: 0.4641\n",
      "Epoch 45/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.4676 - val_loss: 0.4196\n",
      "Epoch 46/1000\n",
      "72000/72000 [==============================] - 3s 44us/step - loss: 0.4157 - val_loss: 0.3719\n",
      "Epoch 47/1000\n",
      "72000/72000 [==============================] - 3s 40us/step - loss: 0.3750 - val_loss: 0.3364\n",
      "Epoch 48/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 0.3354 - val_loss: 0.3079\n",
      "Epoch 49/1000\n",
      "72000/72000 [==============================] - 3s 48us/step - loss: 0.3045 - val_loss: 0.2730\n",
      "Epoch 50/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.2716 - val_loss: 0.2442\n",
      "Epoch 51/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.2437 - val_loss: 0.2181\n",
      "Epoch 52/1000\n",
      "72000/72000 [==============================] - 3s 47us/step - loss: 0.2206 - val_loss: 0.2033\n",
      "Epoch 53/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.2023 - val_loss: 0.1905\n",
      "Epoch 54/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.1827 - val_loss: 0.1693\n",
      "Epoch 55/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.1699 - val_loss: 0.1561\n",
      "Epoch 56/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.1552 - val_loss: 0.1470\n",
      "Epoch 57/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.1450 - val_loss: 0.1330\n",
      "Epoch 58/1000\n",
      "72000/72000 [==============================] - 3s 48us/step - loss: 0.1338 - val_loss: 0.1261\n",
      "Epoch 59/1000\n",
      "72000/72000 [==============================] - 4s 49us/step - loss: 0.1227 - val_loss: 0.1177\n",
      "Epoch 60/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.1159 - val_loss: 0.1052\n",
      "Epoch 61/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.1097 - val_loss: 0.1010\n",
      "Epoch 62/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.1025 - val_loss: 0.0945\n",
      "Epoch 63/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0973 - val_loss: 0.0888\n",
      "Epoch 64/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0916 - val_loss: 0.0872\n",
      "Epoch 65/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0866 - val_loss: 0.0832\n",
      "Epoch 66/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0835 - val_loss: 0.0771\n",
      "Epoch 67/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0779 - val_loss: 0.0755\n",
      "Epoch 68/1000\n",
      "72000/72000 [==============================] - 4s 49us/step - loss: 0.0755 - val_loss: 0.0713\n",
      "Epoch 69/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 0.0731 - val_loss: 0.0668\n",
      "Epoch 70/1000\n",
      "72000/72000 [==============================] - 3s 42us/step - loss: 0.0685 - val_loss: 0.0689\n",
      "Epoch 71/1000\n",
      "72000/72000 [==============================] - 3s 42us/step - loss: 0.0665 - val_loss: 0.0664\n",
      "Epoch 72/1000\n",
      "72000/72000 [==============================] - 3s 42us/step - loss: 0.0652 - val_loss: 0.0661\n",
      "Epoch 73/1000\n",
      "72000/72000 [==============================] - 3s 43us/step - loss: 0.0618 - val_loss: 0.0582\n",
      "Epoch 74/1000\n",
      "72000/72000 [==============================] - 3s 41us/step - loss: 0.0585 - val_loss: 0.0609\n",
      "Epoch 75/1000\n",
      "72000/72000 [==============================] - 3s 47us/step - loss: 0.0576 - val_loss: 0.0551\n",
      "Epoch 76/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0568 - val_loss: 0.0560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/1000\n",
      "72000/72000 [==============================] - 3s 47us/step - loss: 0.0543 - val_loss: 0.0545\n",
      "Epoch 78/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0536 - val_loss: 0.0537\n",
      "Epoch 79/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0521 - val_loss: 0.0497\n",
      "Epoch 80/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0503 - val_loss: 0.0516\n",
      "Epoch 81/1000\n",
      "72000/72000 [==============================] - 3s 48us/step - loss: 0.0490 - val_loss: 0.0465\n",
      "Epoch 82/1000\n",
      "72000/72000 [==============================] - 3s 48us/step - loss: 0.0481 - val_loss: 0.0475\n",
      "Epoch 83/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0473 - val_loss: 0.0481\n",
      "Epoch 84/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0461 - val_loss: 0.0461\n",
      "Epoch 85/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0451 - val_loss: 0.0475\n",
      "Epoch 86/1000\n",
      "72000/72000 [==============================] - 4s 51us/step - loss: 0.0444 - val_loss: 0.0442\n",
      "Epoch 87/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0431 - val_loss: 0.0456\n",
      "Epoch 88/1000\n",
      "72000/72000 [==============================] - 3s 49us/step - loss: 0.0435 - val_loss: 0.0387\n",
      "Epoch 89/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0417 - val_loss: 0.0419\n",
      "Epoch 90/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0415 - val_loss: 0.0430\n",
      "Epoch 91/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0414 - val_loss: 0.0386\n",
      "Epoch 92/1000\n",
      "72000/72000 [==============================] - 3s 47us/step - loss: 0.0390 - val_loss: 0.0406\n",
      "Epoch 93/1000\n",
      "72000/72000 [==============================] - 3s 47us/step - loss: 0.0386 - val_loss: 0.0411\n",
      "Epoch 94/1000\n",
      "72000/72000 [==============================] - 4s 51us/step - loss: 0.0387 - val_loss: 0.0351\n",
      "Epoch 95/1000\n",
      "72000/72000 [==============================] - 3s 48us/step - loss: 0.0384 - val_loss: 0.0357\n",
      "Epoch 96/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0372 - val_loss: 0.0363\n",
      "Epoch 97/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0369 - val_loss: 0.0357\n",
      "Epoch 98/1000\n",
      "72000/72000 [==============================] - 4s 51us/step - loss: 0.0368 - val_loss: 0.0383\n",
      "Epoch 99/1000\n",
      "72000/72000 [==============================] - 4s 49us/step - loss: 0.0349 - val_loss: 0.0362\n",
      "Epoch 100/1000\n",
      "72000/72000 [==============================] - 3s 47us/step - loss: 0.0360 - val_loss: 0.0355\n",
      "Epoch 101/1000\n",
      "72000/72000 [==============================] - 3s 48us/step - loss: 0.0350 - val_loss: 0.0321\n",
      "Epoch 102/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0333 - val_loss: 0.0314\n",
      "Epoch 103/1000\n",
      "72000/72000 [==============================] - 4s 51us/step - loss: 0.0338 - val_loss: 0.0368\n",
      "Epoch 104/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0338 - val_loss: 0.0330\n",
      "Epoch 105/1000\n",
      "72000/72000 [==============================] - 3s 48us/step - loss: 0.0332 - val_loss: 0.0355\n",
      "Epoch 106/1000\n",
      "72000/72000 [==============================] - 3s 48us/step - loss: 0.0319 - val_loss: 0.0333\n",
      "Epoch 107/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0332 - val_loss: 0.0295\n",
      "Epoch 108/1000\n",
      "72000/72000 [==============================] - 4s 51us/step - loss: 0.0319 - val_loss: 0.0320\n",
      "Epoch 109/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0326 - val_loss: 0.0318\n",
      "Epoch 110/1000\n",
      "72000/72000 [==============================] - 3s 44us/step - loss: 0.0311 - val_loss: 0.0306\n",
      "Epoch 111/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0315 - val_loss: 0.0315\n",
      "Epoch 112/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0294 - val_loss: 0.0314\n",
      "Epoch 113/1000\n",
      "72000/72000 [==============================] - 3s 48us/step - loss: 0.0304 - val_loss: 0.0301\n",
      "Epoch 114/1000\n",
      "72000/72000 [==============================] - 4s 51us/step - loss: 0.0292 - val_loss: 0.0299\n",
      "Epoch 115/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0293 - val_loss: 0.0290\n",
      "Epoch 116/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0290 - val_loss: 0.0291\n",
      "Epoch 117/1000\n",
      "72000/72000 [==============================] - 3s 47us/step - loss: 0.0293 - val_loss: 0.0294\n",
      "Epoch 118/1000\n",
      "72000/72000 [==============================] - 3s 43us/step - loss: 0.0293 - val_loss: 0.0308\n",
      "Epoch 119/1000\n",
      "72000/72000 [==============================] - 4s 49us/step - loss: 0.0281 - val_loss: 0.0281\n",
      "Epoch 120/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0283 - val_loss: 0.0273\n",
      "Epoch 121/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0287 - val_loss: 0.0257\n",
      "Epoch 122/1000\n",
      "72000/72000 [==============================] - 4s 56us/step - loss: 0.0288 - val_loss: 0.0291\n",
      "Epoch 123/1000\n",
      "72000/72000 [==============================] - 4s 58us/step - loss: 0.0276 - val_loss: 0.0267\n",
      "Epoch 124/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.0276 - val_loss: 0.0272\n",
      "Epoch 125/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0270 - val_loss: 0.0261\n",
      "Epoch 126/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0269 - val_loss: 0.0262\n",
      "Epoch 127/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0266 - val_loss: 0.0270\n",
      "Epoch 128/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0267 - val_loss: 0.0269\n",
      "Epoch 129/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0256 - val_loss: 0.0257\n",
      "Epoch 130/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0256 - val_loss: 0.0267\n",
      "Epoch 131/1000\n",
      "72000/72000 [==============================] - 4s 49us/step - loss: 0.0256 - val_loss: 0.0257\n",
      "Epoch 132/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0249 - val_loss: 0.0249\n",
      "Epoch 133/1000\n",
      "72000/72000 [==============================] - 3s 44us/step - loss: 0.0249 - val_loss: 0.0260\n",
      "Epoch 134/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0252 - val_loss: 0.0245\n",
      "Epoch 135/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0245 - val_loss: 0.0248\n",
      "Epoch 136/1000\n",
      "72000/72000 [==============================] - 3s 47us/step - loss: 0.0238 - val_loss: 0.0230\n",
      "Epoch 137/1000\n",
      "72000/72000 [==============================] - 3s 47us/step - loss: 0.0249 - val_loss: 0.0263\n",
      "Epoch 138/1000\n",
      "72000/72000 [==============================] - 4s 49us/step - loss: 0.0243 - val_loss: 0.0261\n",
      "Epoch 139/1000\n",
      "72000/72000 [==============================] - 4s 51us/step - loss: 0.0241 - val_loss: 0.0252\n",
      "Epoch 140/1000\n",
      "72000/72000 [==============================] - 3s 48us/step - loss: 0.0237 - val_loss: 0.0229\n",
      "Epoch 141/1000\n",
      "72000/72000 [==============================] - 4s 51us/step - loss: 0.0238 - val_loss: 0.0250\n",
      "Epoch 142/1000\n",
      "72000/72000 [==============================] - 3s 48us/step - loss: 0.0236 - val_loss: 0.0229\n",
      "Epoch 143/1000\n",
      "72000/72000 [==============================] - 3s 48us/step - loss: 0.0240 - val_loss: 0.0214\n",
      "Epoch 144/1000\n",
      "72000/72000 [==============================] - 4s 49us/step - loss: 0.0227 - val_loss: 0.0237\n",
      "Epoch 145/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0235 - val_loss: 0.0220\n",
      "Epoch 146/1000\n",
      "72000/72000 [==============================] - 4s 49us/step - loss: 0.0234 - val_loss: 0.0236\n",
      "Epoch 147/1000\n",
      "72000/72000 [==============================] - 4s 51us/step - loss: 0.0230 - val_loss: 0.0230\n",
      "Epoch 148/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0232 - val_loss: 0.0218\n",
      "Epoch 149/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0218 - val_loss: 0.0229\n",
      "Epoch 150/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0223 - val_loss: 0.0240\n",
      "Epoch 151/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0220 - val_loss: 0.0253\n",
      "Epoch 152/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0223 - val_loss: 0.0229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0225 - val_loss: 0.0208\n",
      "Epoch 154/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0216 - val_loss: 0.0215\n",
      "Epoch 155/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.0215 - val_loss: 0.0225\n",
      "Epoch 156/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.0222 - val_loss: 0.0196\n",
      "Epoch 157/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0219 - val_loss: 0.0202\n",
      "Epoch 158/1000\n",
      "72000/72000 [==============================] - 4s 56us/step - loss: 0.0217 - val_loss: 0.0222\n",
      "Epoch 159/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0209 - val_loss: 0.0207\n",
      "Epoch 160/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0205 - val_loss: 0.0231\n",
      "Epoch 161/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0210 - val_loss: 0.0223\n",
      "Epoch 162/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0207 - val_loss: 0.0198\n",
      "Epoch 163/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0200 - val_loss: 0.0215\n",
      "Epoch 164/1000\n",
      "72000/72000 [==============================] - 5s 68us/step - loss: 0.0202 - val_loss: 0.0204\n",
      "Epoch 165/1000\n",
      "72000/72000 [==============================] - 4s 58us/step - loss: 0.0196 - val_loss: 0.0207\n",
      "Epoch 166/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0200 - val_loss: 0.0184\n",
      "Epoch 167/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.0201 - val_loss: 0.0212\n",
      "Epoch 168/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0200 - val_loss: 0.0196\n",
      "Epoch 169/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0193 - val_loss: 0.0212\n",
      "Epoch 170/1000\n",
      "72000/72000 [==============================] - 3s 48us/step - loss: 0.0200 - val_loss: 0.0207\n",
      "Epoch 171/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0196 - val_loss: 0.0173\n",
      "Epoch 172/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0201 - val_loss: 0.0219\n",
      "Epoch 173/1000\n",
      "72000/72000 [==============================] - 4s 56us/step - loss: 0.0193 - val_loss: 0.0181\n",
      "Epoch 174/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0188 - val_loss: 0.0175\n",
      "Epoch 175/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0195 - val_loss: 0.0176\n",
      "Epoch 176/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0190 - val_loss: 0.0193\n",
      "Epoch 177/1000\n",
      "72000/72000 [==============================] - 4s 51us/step - loss: 0.0186 - val_loss: 0.0185\n",
      "Epoch 178/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0188 - val_loss: 0.0190\n",
      "Epoch 179/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0186 - val_loss: 0.0191\n",
      "Epoch 180/1000\n",
      "72000/72000 [==============================] - 4s 56us/step - loss: 0.0187 - val_loss: 0.0208\n",
      "Epoch 181/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0185 - val_loss: 0.0195\n",
      "Epoch 182/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0189 - val_loss: 0.0180\n",
      "Epoch 183/1000\n",
      "72000/72000 [==============================] - 4s 58us/step - loss: 0.0186 - val_loss: 0.0177\n",
      "Epoch 184/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0182 - val_loss: 0.0181\n",
      "Epoch 185/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.0186 - val_loss: 0.0172\n",
      "Epoch 186/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0181 - val_loss: 0.0209\n",
      "Epoch 187/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0183 - val_loss: 0.0177\n",
      "Epoch 188/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0180 - val_loss: 0.0161\n",
      "Epoch 189/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0185 - val_loss: 0.0173\n",
      "Epoch 190/1000\n",
      "72000/72000 [==============================] - 4s 51us/step - loss: 0.0175 - val_loss: 0.0179\n",
      "Epoch 191/1000\n",
      "72000/72000 [==============================] - 4s 49us/step - loss: 0.0177 - val_loss: 0.0186\n",
      "Epoch 192/1000\n",
      "72000/72000 [==============================] - 4s 51us/step - loss: 0.0179 - val_loss: 0.0180\n",
      "Epoch 193/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0176 - val_loss: 0.0175\n",
      "Epoch 194/1000\n",
      "72000/72000 [==============================] - 4s 49us/step - loss: 0.0178 - val_loss: 0.0171\n",
      "Epoch 195/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0172 - val_loss: 0.0159\n",
      "Epoch 196/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0171 - val_loss: 0.0171\n",
      "Epoch 197/1000\n",
      "72000/72000 [==============================] - 4s 49us/step - loss: 0.0171 - val_loss: 0.0179\n",
      "Epoch 198/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0164 - val_loss: 0.0149\n",
      "Epoch 199/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0179 - val_loss: 0.0196\n",
      "Epoch 200/1000\n",
      "72000/72000 [==============================] - 4s 49us/step - loss: 0.0162 - val_loss: 0.0181\n",
      "Epoch 201/1000\n",
      "72000/72000 [==============================] - 4s 49us/step - loss: 0.0163 - val_loss: 0.0192\n",
      "Epoch 202/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0171 - val_loss: 0.0173\n",
      "Epoch 203/1000\n",
      "72000/72000 [==============================] - 4s 56us/step - loss: 0.0158 - val_loss: 0.0164\n",
      "Epoch 204/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0163 - val_loss: 0.0171\n",
      "Epoch 205/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0166 - val_loss: 0.0171\n",
      "Epoch 206/1000\n",
      "72000/72000 [==============================] - 4s 58us/step - loss: 0.0166 - val_loss: 0.0151\n",
      "Epoch 207/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0169 - val_loss: 0.0159\n",
      "Epoch 208/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 209/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0163 - val_loss: 0.0177\n",
      "Epoch 210/1000\n",
      "72000/72000 [==============================] - 4s 51us/step - loss: 0.0168 - val_loss: 0.0193\n",
      "Epoch 211/1000\n",
      "72000/72000 [==============================] - 4s 56us/step - loss: 0.0156 - val_loss: 0.0185\n",
      "Epoch 212/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0170 - val_loss: 0.0180\n",
      "Epoch 213/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0159 - val_loss: 0.0150\n",
      "Epoch 214/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.0161 - val_loss: 0.0161\n",
      "Epoch 215/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0152 - val_loss: 0.0143\n",
      "Epoch 216/1000\n",
      "72000/72000 [==============================] - 3s 48us/step - loss: 0.0161 - val_loss: 0.0152\n",
      "Epoch 217/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0158 - val_loss: 0.0139\n",
      "Epoch 218/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0156 - val_loss: 0.0173\n",
      "Epoch 219/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0160 - val_loss: 0.0157\n",
      "Epoch 220/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0157 - val_loss: 0.0165\n",
      "Epoch 221/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0157 - val_loss: 0.0143\n",
      "Epoch 222/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0148 - val_loss: 0.0163\n",
      "Epoch 223/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0148 - val_loss: 0.0154\n",
      "Epoch 224/1000\n",
      "72000/72000 [==============================] - 4s 56us/step - loss: 0.0161 - val_loss: 0.0160\n",
      "Epoch 225/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.0156 - val_loss: 0.0163\n",
      "Epoch 226/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.0144 - val_loss: 0.0140\n",
      "Epoch 227/1000\n",
      "72000/72000 [==============================] - 5s 64us/step - loss: 0.0162 - val_loss: 0.0131\n",
      "Epoch 228/1000\n",
      "72000/72000 [==============================] - 5s 70us/step - loss: 0.0157 - val_loss: 0.0158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0152 - val_loss: 0.0137\n",
      "Epoch 230/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0150 - val_loss: 0.0147\n",
      "Epoch 231/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0145 - val_loss: 0.0163\n",
      "Epoch 232/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0144 - val_loss: 0.0135\n",
      "Epoch 233/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0153 - val_loss: 0.0142\n",
      "Epoch 234/1000\n",
      "72000/72000 [==============================] - 3s 49us/step - loss: 0.0139 - val_loss: 0.0158\n",
      "Epoch 235/1000\n",
      "72000/72000 [==============================] - 3s 47us/step - loss: 0.0148 - val_loss: 0.0148\n",
      "Epoch 236/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0146 - val_loss: 0.0154\n",
      "Epoch 237/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0154 - val_loss: 0.0161\n",
      "Epoch 238/1000\n",
      "72000/72000 [==============================] - 3s 47us/step - loss: 0.0150 - val_loss: 0.0141\n",
      "Epoch 239/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0149 - val_loss: 0.0140\n",
      "Epoch 240/1000\n",
      "72000/72000 [==============================] - 3s 47us/step - loss: 0.0146 - val_loss: 0.0146\n",
      "Epoch 241/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0143 - val_loss: 0.0133\n",
      "Epoch 242/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0151 - val_loss: 0.0163\n",
      "Epoch 243/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0145 - val_loss: 0.0128\n",
      "Epoch 244/1000\n",
      "72000/72000 [==============================] - 3s 47us/step - loss: 0.0140 - val_loss: 0.0151\n",
      "Epoch 245/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0143 - val_loss: 0.0148\n",
      "Epoch 246/1000\n",
      "72000/72000 [==============================] - 3s 47us/step - loss: 0.0144 - val_loss: 0.0143\n",
      "Epoch 247/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0140 - val_loss: 0.0140\n",
      "Epoch 248/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0138 - val_loss: 0.0139\n",
      "Epoch 249/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0140 - val_loss: 0.0139\n",
      "Epoch 250/1000\n",
      "72000/72000 [==============================] - 3s 47us/step - loss: 0.0140 - val_loss: 0.0128\n",
      "Epoch 251/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0132 - val_loss: 0.0131\n",
      "Epoch 252/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0145 - val_loss: 0.0147\n",
      "Epoch 253/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0145 - val_loss: 0.0142\n",
      "Epoch 254/1000\n",
      "72000/72000 [==============================] - 3s 47us/step - loss: 0.0145 - val_loss: 0.0147\n",
      "Epoch 255/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0136 - val_loss: 0.0142\n",
      "Epoch 256/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0146 - val_loss: 0.0135\n",
      "Epoch 257/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0146 - val_loss: 0.0129\n",
      "Epoch 258/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0137 - val_loss: 0.0132\n",
      "Epoch 259/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0140 - val_loss: 0.0129\n",
      "Epoch 260/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0135 - val_loss: 0.0140\n",
      "Epoch 261/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0138 - val_loss: 0.0127\n",
      "Epoch 262/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.0133 - val_loss: 0.0151\n",
      "Epoch 263/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0133 - val_loss: 0.0137\n",
      "Epoch 264/1000\n",
      "72000/72000 [==============================] - 4s 49us/step - loss: 0.0143 - val_loss: 0.0137\n",
      "Epoch 265/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0135 - val_loss: 0.0141\n",
      "Epoch 266/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0135 - val_loss: 0.0115\n",
      "Epoch 267/1000\n",
      "72000/72000 [==============================] - 5s 72us/step - loss: 0.0134 - val_loss: 0.0130\n",
      "Epoch 268/1000\n",
      "72000/72000 [==============================] - 4s 51us/step - loss: 0.0129 - val_loss: 0.0140\n",
      "Epoch 269/1000\n",
      "72000/72000 [==============================] - 4s 56us/step - loss: 0.0126 - val_loss: 0.0144\n",
      "Epoch 270/1000\n",
      "72000/72000 [==============================] - 6s 78us/step - loss: 0.0130 - val_loss: 0.0124\n",
      "Epoch 271/1000\n",
      "72000/72000 [==============================] - 5s 64us/step - loss: 0.0135 - val_loss: 0.0141\n",
      "Epoch 272/1000\n",
      "72000/72000 [==============================] - 4s 51us/step - loss: 0.0130 - val_loss: 0.0106\n",
      "Epoch 273/1000\n",
      "72000/72000 [==============================] - 3s 47us/step - loss: 0.0128 - val_loss: 0.0143\n",
      "Epoch 274/1000\n",
      "72000/72000 [==============================] - 3s 44us/step - loss: 0.0130 - val_loss: 0.0150\n",
      "Epoch 275/1000\n",
      "72000/72000 [==============================] - 4s 51us/step - loss: 0.0130 - val_loss: 0.0136\n",
      "Epoch 276/1000\n",
      "72000/72000 [==============================] - 3s 47us/step - loss: 0.0137 - val_loss: 0.0112\n",
      "Epoch 277/1000\n",
      "72000/72000 [==============================] - 3s 44us/step - loss: 0.0137 - val_loss: 0.0133\n",
      "Epoch 278/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0130 - val_loss: 0.0115\n",
      "Epoch 279/1000\n",
      "72000/72000 [==============================] - 5s 66us/step - loss: 0.0138 - val_loss: 0.0131\n",
      "Epoch 280/1000\n",
      "72000/72000 [==============================] - 3s 47us/step - loss: 0.0129 - val_loss: 0.0150\n",
      "Epoch 281/1000\n",
      "72000/72000 [==============================] - 3s 44us/step - loss: 0.0132 - val_loss: 0.0115\n",
      "Epoch 282/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0131 - val_loss: 0.0142\n",
      "Epoch 283/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0129 - val_loss: 0.0144\n",
      "Epoch 284/1000\n",
      "72000/72000 [==============================] - 3s 44us/step - loss: 0.0124 - val_loss: 0.0118\n",
      "Epoch 285/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0134 - val_loss: 0.0117\n",
      "Epoch 286/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0123 - val_loss: 0.0122\n",
      "Epoch 287/1000\n",
      "72000/72000 [==============================] - 3s 46us/step - loss: 0.0121 - val_loss: 0.0127\n",
      "Epoch 288/1000\n",
      "72000/72000 [==============================] - 4s 50us/step - loss: 0.0126 - val_loss: 0.0113\n",
      "Epoch 289/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0130 - val_loss: 0.0121\n",
      "Epoch 290/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0122 - val_loss: 0.0138\n",
      "Epoch 291/1000\n",
      "72000/72000 [==============================] - 3s 44us/step - loss: 0.0129 - val_loss: 0.0127\n",
      "Epoch 292/1000\n",
      "72000/72000 [==============================] - 3s 45us/step - loss: 0.0117 - val_loss: 0.0111\n",
      "Epoch 00292: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f384d006c50>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder8_8_tap_3l.fit(train_data256, train_data256,\n",
    "                       epochs=1000,\n",
    "                       batch_size=1000*M,\n",
    "                       shuffle=True,\n",
    "                       validation_data=(valid_data256,\n",
    "                                        valid_data256),\n",
    "                       callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 8000 samples\n",
      "Epoch 1/1000\n",
      "72000/72000 [==============================] - 8s 108us/step - loss: 5.5461 - val_loss: 5.5019\n",
      "Epoch 2/1000\n",
      "72000/72000 [==============================] - 5s 65us/step - loss: 5.5016 - val_loss: 5.4573\n",
      "Epoch 3/1000\n",
      "72000/72000 [==============================] - 5s 74us/step - loss: 5.4564 - val_loss: 5.4106\n",
      "Epoch 4/1000\n",
      "72000/72000 [==============================] - 5s 68us/step - loss: 5.4106 - val_loss: 5.3554\n",
      "Epoch 5/1000\n",
      "72000/72000 [==============================] - 6s 85us/step - loss: 5.3551 - val_loss: 5.2943\n",
      "Epoch 6/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 5.2936 - val_loss: 5.2225\n",
      "Epoch 7/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 5.2227 - val_loss: 5.1443\n",
      "Epoch 8/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 5.1433 - val_loss: 5.0520\n",
      "Epoch 9/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 5.0507 - val_loss: 4.9468\n",
      "Epoch 10/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 4.9452 - val_loss: 4.8278\n",
      "Epoch 11/1000\n",
      "72000/72000 [==============================] - 5s 65us/step - loss: 4.8269 - val_loss: 4.6975\n",
      "Epoch 12/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 4.6962 - val_loss: 4.5524\n",
      "Epoch 13/1000\n",
      "72000/72000 [==============================] - 5s 68us/step - loss: 4.5517 - val_loss: 4.4003\n",
      "Epoch 14/1000\n",
      "72000/72000 [==============================] - 6s 77us/step - loss: 4.3971 - val_loss: 4.2375\n",
      "Epoch 15/1000\n",
      "72000/72000 [==============================] - 5s 70us/step - loss: 4.2308 - val_loss: 4.0604\n",
      "Epoch 16/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 4.0561 - val_loss: 3.8796\n",
      "Epoch 17/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 3.8748 - val_loss: 3.6983\n",
      "Epoch 18/1000\n",
      "72000/72000 [==============================] - 5s 68us/step - loss: 3.6901 - val_loss: 3.5146\n",
      "Epoch 19/1000\n",
      "72000/72000 [==============================] - 5s 67us/step - loss: 3.5077 - val_loss: 3.3316\n",
      "Epoch 20/1000\n",
      "72000/72000 [==============================] - 5s 71us/step - loss: 3.3240 - val_loss: 3.1453\n",
      "Epoch 21/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 3.1334 - val_loss: 2.9493\n",
      "Epoch 22/1000\n",
      "72000/72000 [==============================] - 5s 66us/step - loss: 2.9372 - val_loss: 2.7408\n",
      "Epoch 23/1000\n",
      "72000/72000 [==============================] - 5s 71us/step - loss: 2.7295 - val_loss: 2.5557\n",
      "Epoch 24/1000\n",
      "72000/72000 [==============================] - 5s 73us/step - loss: 2.5467 - val_loss: 2.3630\n",
      "Epoch 25/1000\n",
      "72000/72000 [==============================] - 5s 74us/step - loss: 2.3493 - val_loss: 2.1817\n",
      "Epoch 26/1000\n",
      "72000/72000 [==============================] - 5s 75us/step - loss: 2.1711 - val_loss: 1.9918\n",
      "Epoch 27/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 1.9849 - val_loss: 1.8317\n",
      "Epoch 28/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 1.8229 - val_loss: 1.6482\n",
      "Epoch 29/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 1.6471 - val_loss: 1.4984\n",
      "Epoch 30/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 1.5044 - val_loss: 1.3435\n",
      "Epoch 31/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 1.3543 - val_loss: 1.2162\n",
      "Epoch 32/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 1.2275 - val_loss: 1.0942\n",
      "Epoch 33/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 1.0971 - val_loss: 0.9855\n",
      "Epoch 34/1000\n",
      "72000/72000 [==============================] - 4s 56us/step - loss: 0.9877 - val_loss: 0.8818\n",
      "Epoch 35/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.8810 - val_loss: 0.8011\n",
      "Epoch 36/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.7912 - val_loss: 0.7050\n",
      "Epoch 37/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.7042 - val_loss: 0.6317\n",
      "Epoch 38/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.6276 - val_loss: 0.5700\n",
      "Epoch 39/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.5676 - val_loss: 0.5061\n",
      "Epoch 40/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.5029 - val_loss: 0.4497\n",
      "Epoch 41/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4501 - val_loss: 0.4022\n",
      "Epoch 42/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.4019 - val_loss: 0.3619\n",
      "Epoch 43/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.3645 - val_loss: 0.3222\n",
      "Epoch 44/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.3263 - val_loss: 0.3049\n",
      "Epoch 45/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.3026 - val_loss: 0.2651\n",
      "Epoch 46/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.2707 - val_loss: 0.2529\n",
      "Epoch 47/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.2512 - val_loss: 0.2295\n",
      "Epoch 48/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.2262 - val_loss: 0.2128\n",
      "Epoch 49/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.2094 - val_loss: 0.1941\n",
      "Epoch 50/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.1928 - val_loss: 0.1787\n",
      "Epoch 51/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.1811 - val_loss: 0.1667\n",
      "Epoch 52/1000\n",
      "72000/72000 [==============================] - 4s 56us/step - loss: 0.1694 - val_loss: 0.1551\n",
      "Epoch 53/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.1576 - val_loss: 0.1474\n",
      "Epoch 54/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.1490 - val_loss: 0.1335\n",
      "Epoch 55/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.1419 - val_loss: 0.1296\n",
      "Epoch 56/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.1327 - val_loss: 0.1314\n",
      "Epoch 57/1000\n",
      "72000/72000 [==============================] - 5s 68us/step - loss: 0.1253 - val_loss: 0.1155\n",
      "Epoch 58/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.1169 - val_loss: 0.1129\n",
      "Epoch 59/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.1130 - val_loss: 0.1025\n",
      "Epoch 60/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.1085 - val_loss: 0.1017\n",
      "Epoch 61/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.1054 - val_loss: 0.1000\n",
      "Epoch 62/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0990 - val_loss: 0.0896\n",
      "Epoch 63/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0955 - val_loss: 0.0943\n",
      "Epoch 64/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.0916 - val_loss: 0.0846\n",
      "Epoch 65/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0877 - val_loss: 0.0846\n",
      "Epoch 66/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0872 - val_loss: 0.0851\n",
      "Epoch 67/1000\n",
      "72000/72000 [==============================] - 5s 70us/step - loss: 0.0816 - val_loss: 0.0882\n",
      "Epoch 68/1000\n",
      "72000/72000 [==============================] - 5s 69us/step - loss: 0.0803 - val_loss: 0.0739\n",
      "Epoch 69/1000\n",
      "72000/72000 [==============================] - 5s 70us/step - loss: 0.0783 - val_loss: 0.0777\n",
      "Epoch 70/1000\n",
      "72000/72000 [==============================] - 5s 72us/step - loss: 0.0768 - val_loss: 0.0726\n",
      "Epoch 71/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.0732 - val_loss: 0.0660\n",
      "Epoch 72/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0691 - val_loss: 0.0675\n",
      "Epoch 73/1000\n",
      "72000/72000 [==============================] - 5s 74us/step - loss: 0.0689 - val_loss: 0.0710\n",
      "Epoch 74/1000\n",
      "72000/72000 [==============================] - 5s 66us/step - loss: 0.0661 - val_loss: 0.0679\n",
      "Epoch 75/1000\n",
      "72000/72000 [==============================] - 5s 73us/step - loss: 0.0654 - val_loss: 0.0658\n",
      "Epoch 76/1000\n",
      "72000/72000 [==============================] - 5s 66us/step - loss: 0.0619 - val_loss: 0.0658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.0618 - val_loss: 0.0613\n",
      "Epoch 78/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0625 - val_loss: 0.0564\n",
      "Epoch 79/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.0593 - val_loss: 0.0681\n",
      "Epoch 80/1000\n",
      "72000/72000 [==============================] - 5s 68us/step - loss: 0.0561 - val_loss: 0.0566\n",
      "Epoch 81/1000\n",
      "72000/72000 [==============================] - 5s 69us/step - loss: 0.0559 - val_loss: 0.0539\n",
      "Epoch 82/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0584 - val_loss: 0.0560\n",
      "Epoch 83/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0542 - val_loss: 0.0515\n",
      "Epoch 84/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0538 - val_loss: 0.0510\n",
      "Epoch 85/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0526 - val_loss: 0.0500\n",
      "Epoch 86/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0511 - val_loss: 0.0516\n",
      "Epoch 87/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0487 - val_loss: 0.0477\n",
      "Epoch 88/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0484 - val_loss: 0.0499\n",
      "Epoch 89/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0476 - val_loss: 0.0454\n",
      "Epoch 90/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0462 - val_loss: 0.0481\n",
      "Epoch 91/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0449 - val_loss: 0.0436\n",
      "Epoch 92/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0438 - val_loss: 0.0416\n",
      "Epoch 93/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0442 - val_loss: 0.0470\n",
      "Epoch 94/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0438 - val_loss: 0.0475\n",
      "Epoch 95/1000\n",
      "72000/72000 [==============================] - 5s 65us/step - loss: 0.0427 - val_loss: 0.0412\n",
      "Epoch 96/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0410 - val_loss: 0.0467\n",
      "Epoch 97/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0417 - val_loss: 0.0468\n",
      "Epoch 98/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0397 - val_loss: 0.0367\n",
      "Epoch 99/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0383 - val_loss: 0.0376\n",
      "Epoch 100/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0394 - val_loss: 0.0415\n",
      "Epoch 101/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0385 - val_loss: 0.0373\n",
      "Epoch 102/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0395 - val_loss: 0.0347\n",
      "Epoch 103/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0382 - val_loss: 0.0388\n",
      "Epoch 104/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0373 - val_loss: 0.0385\n",
      "Epoch 105/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0369 - val_loss: 0.0331\n",
      "Epoch 106/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0354 - val_loss: 0.0326\n",
      "Epoch 107/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0358 - val_loss: 0.0358\n",
      "Epoch 108/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0356 - val_loss: 0.0367\n",
      "Epoch 109/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0352 - val_loss: 0.0318\n",
      "Epoch 110/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0343 - val_loss: 0.0381\n",
      "Epoch 111/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0342 - val_loss: 0.0312\n",
      "Epoch 112/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0331 - val_loss: 0.0334\n",
      "Epoch 113/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0327 - val_loss: 0.0307\n",
      "Epoch 114/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0323 - val_loss: 0.0297\n",
      "Epoch 115/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0304 - val_loss: 0.0325\n",
      "Epoch 116/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0319 - val_loss: 0.0337\n",
      "Epoch 117/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0315 - val_loss: 0.0302\n",
      "Epoch 118/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0306 - val_loss: 0.0308\n",
      "Epoch 119/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0301 - val_loss: 0.0294\n",
      "Epoch 120/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0300 - val_loss: 0.0311\n",
      "Epoch 121/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0291 - val_loss: 0.0295\n",
      "Epoch 122/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0300 - val_loss: 0.0301\n",
      "Epoch 123/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0295 - val_loss: 0.0287\n",
      "Epoch 124/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0286 - val_loss: 0.0293\n",
      "Epoch 125/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0277 - val_loss: 0.0276\n",
      "Epoch 126/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0297 - val_loss: 0.0297\n",
      "Epoch 127/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0277 - val_loss: 0.0319\n",
      "Epoch 128/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0272 - val_loss: 0.0262\n",
      "Epoch 129/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0279 - val_loss: 0.0260\n",
      "Epoch 130/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0265 - val_loss: 0.0240\n",
      "Epoch 131/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0277 - val_loss: 0.0290\n",
      "Epoch 132/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0259 - val_loss: 0.0262\n",
      "Epoch 133/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0266 - val_loss: 0.0276\n",
      "Epoch 134/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0245 - val_loss: 0.0220\n",
      "Epoch 135/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0261 - val_loss: 0.0236\n",
      "Epoch 136/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0261 - val_loss: 0.0250\n",
      "Epoch 137/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0257 - val_loss: 0.0224\n",
      "Epoch 138/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0255 - val_loss: 0.0237\n",
      "Epoch 139/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0256 - val_loss: 0.0237\n",
      "Epoch 140/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.0249 - val_loss: 0.0266\n",
      "Epoch 141/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0251 - val_loss: 0.0216\n",
      "Epoch 142/1000\n",
      "72000/72000 [==============================] - 5s 68us/step - loss: 0.0247 - val_loss: 0.0243\n",
      "Epoch 143/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.0249 - val_loss: 0.0222\n",
      "Epoch 144/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0254 - val_loss: 0.0236\n",
      "Epoch 145/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0236 - val_loss: 0.0222\n",
      "Epoch 146/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0238 - val_loss: 0.0213\n",
      "Epoch 147/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.0241 - val_loss: 0.0217\n",
      "Epoch 148/1000\n",
      "72000/72000 [==============================] - 5s 71us/step - loss: 0.0233 - val_loss: 0.0182\n",
      "Epoch 149/1000\n",
      "72000/72000 [==============================] - 5s 72us/step - loss: 0.0222 - val_loss: 0.0238\n",
      "Epoch 150/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0219 - val_loss: 0.0206\n",
      "Epoch 151/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.0222 - val_loss: 0.0252\n",
      "Epoch 152/1000\n",
      "72000/72000 [==============================] - 5s 71us/step - loss: 0.0227 - val_loss: 0.0221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.0223 - val_loss: 0.0233\n",
      "Epoch 154/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0215 - val_loss: 0.0239\n",
      "Epoch 155/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0210 - val_loss: 0.0214\n",
      "Epoch 156/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0230 - val_loss: 0.0196\n",
      "Epoch 157/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0213 - val_loss: 0.0201\n",
      "Epoch 158/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0210 - val_loss: 0.0211\n",
      "Epoch 159/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0223 - val_loss: 0.0207\n",
      "Epoch 160/1000\n",
      "72000/72000 [==============================] - 4s 56us/step - loss: 0.0210 - val_loss: 0.0204\n",
      "Epoch 161/1000\n",
      "72000/72000 [==============================] - 4s 58us/step - loss: 0.0217 - val_loss: 0.0218\n",
      "Epoch 162/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0212 - val_loss: 0.0181\n",
      "Epoch 163/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0201 - val_loss: 0.0197\n",
      "Epoch 164/1000\n",
      "72000/72000 [==============================] - 6s 77us/step - loss: 0.0202 - val_loss: 0.0223\n",
      "Epoch 165/1000\n",
      "72000/72000 [==============================] - 5s 70us/step - loss: 0.0193 - val_loss: 0.0185\n",
      "Epoch 166/1000\n",
      "72000/72000 [==============================] - 5s 68us/step - loss: 0.0193 - val_loss: 0.0207\n",
      "Epoch 167/1000\n",
      "72000/72000 [==============================] - 5s 72us/step - loss: 0.0205 - val_loss: 0.0211\n",
      "Epoch 168/1000\n",
      "72000/72000 [==============================] - 5s 71us/step - loss: 0.0197 - val_loss: 0.0185\n",
      "Epoch 169/1000\n",
      "72000/72000 [==============================] - 5s 67us/step - loss: 0.0202 - val_loss: 0.0186\n",
      "Epoch 170/1000\n",
      "72000/72000 [==============================] - 5s 70us/step - loss: 0.0192 - val_loss: 0.0161\n",
      "Epoch 171/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0189 - val_loss: 0.0182\n",
      "Epoch 172/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0185 - val_loss: 0.0195\n",
      "Epoch 173/1000\n",
      "72000/72000 [==============================] - 4s 58us/step - loss: 0.0196 - val_loss: 0.0207\n",
      "Epoch 174/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0187 - val_loss: 0.0189\n",
      "Epoch 175/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0184 - val_loss: 0.0187\n",
      "Epoch 176/1000\n",
      "72000/72000 [==============================] - 5s 65us/step - loss: 0.0185 - val_loss: 0.0213\n",
      "Epoch 177/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0183 - val_loss: 0.0207\n",
      "Epoch 178/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0190 - val_loss: 0.0159\n",
      "Epoch 179/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0179 - val_loss: 0.0209\n",
      "Epoch 180/1000\n",
      "72000/72000 [==============================] - 4s 56us/step - loss: 0.0185 - val_loss: 0.0164\n",
      "Epoch 181/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0192 - val_loss: 0.0183\n",
      "Epoch 182/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0180 - val_loss: 0.0164\n",
      "Epoch 183/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0171 - val_loss: 0.0183\n",
      "Epoch 184/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0176 - val_loss: 0.0193\n",
      "Epoch 185/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0178 - val_loss: 0.0176\n",
      "Epoch 186/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 187/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.0176 - val_loss: 0.0184\n",
      "Epoch 188/1000\n",
      "72000/72000 [==============================] - 4s 58us/step - loss: 0.0169 - val_loss: 0.0180\n",
      "Epoch 189/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.0176 - val_loss: 0.0203\n",
      "Epoch 190/1000\n",
      "72000/72000 [==============================] - 5s 71us/step - loss: 0.0167 - val_loss: 0.0194\n",
      "Epoch 191/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0164 - val_loss: 0.0188\n",
      "Epoch 192/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.0179 - val_loss: 0.0174\n",
      "Epoch 193/1000\n",
      "72000/72000 [==============================] - 5s 72us/step - loss: 0.0167 - val_loss: 0.0179\n",
      "Epoch 194/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.0176 - val_loss: 0.0141\n",
      "Epoch 195/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0161 - val_loss: 0.0157\n",
      "Epoch 196/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0166 - val_loss: 0.0186\n",
      "Epoch 197/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.0161 - val_loss: 0.0162\n",
      "Epoch 198/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0175 - val_loss: 0.0132\n",
      "Epoch 199/1000\n",
      "72000/72000 [==============================] - 5s 68us/step - loss: 0.0166 - val_loss: 0.0171\n",
      "Epoch 200/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.0159 - val_loss: 0.0173\n",
      "Epoch 201/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.0162 - val_loss: 0.0129\n",
      "Epoch 202/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0150 - val_loss: 0.0170\n",
      "Epoch 203/1000\n",
      "72000/72000 [==============================] - 4s 58us/step - loss: 0.0162 - val_loss: 0.0166\n",
      "Epoch 204/1000\n",
      "72000/72000 [==============================] - 4s 56us/step - loss: 0.0154 - val_loss: 0.0148\n",
      "Epoch 205/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.0152 - val_loss: 0.0146\n",
      "Epoch 206/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0155 - val_loss: 0.0169\n",
      "Epoch 207/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0155 - val_loss: 0.0168\n",
      "Epoch 208/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.0173 - val_loss: 0.0138\n",
      "Epoch 209/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.0152 - val_loss: 0.0153\n",
      "Epoch 210/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0154 - val_loss: 0.0176\n",
      "Epoch 211/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.0156 - val_loss: 0.0134\n",
      "Epoch 212/1000\n",
      "72000/72000 [==============================] - 5s 75us/step - loss: 0.0156 - val_loss: 0.0149\n",
      "Epoch 213/1000\n",
      "72000/72000 [==============================] - 6s 85us/step - loss: 0.0146 - val_loss: 0.0159\n",
      "Epoch 214/1000\n",
      "72000/72000 [==============================] - 5s 70us/step - loss: 0.0151 - val_loss: 0.0141\n",
      "Epoch 215/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0152 - val_loss: 0.0141\n",
      "Epoch 216/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0154 - val_loss: 0.0153\n",
      "Epoch 217/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0165 - val_loss: 0.0155\n",
      "Epoch 218/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0147 - val_loss: 0.0145\n",
      "Epoch 219/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0148 - val_loss: 0.0140\n",
      "Epoch 220/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0154 - val_loss: 0.0118\n",
      "Epoch 221/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0142 - val_loss: 0.0127\n",
      "Epoch 222/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0151 - val_loss: 0.0148\n",
      "Epoch 223/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0147 - val_loss: 0.0171\n",
      "Epoch 224/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0152 - val_loss: 0.0130\n",
      "Epoch 225/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0136 - val_loss: 0.0122\n",
      "Epoch 226/1000\n",
      "72000/72000 [==============================] - 5s 65us/step - loss: 0.0149 - val_loss: 0.0152\n",
      "Epoch 227/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0141 - val_loss: 0.0133\n",
      "Epoch 228/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0144 - val_loss: 0.0132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/1000\n",
      "72000/72000 [==============================] - 5s 68us/step - loss: 0.0147 - val_loss: 0.0172\n",
      "Epoch 230/1000\n",
      "72000/72000 [==============================] - 5s 75us/step - loss: 0.0136 - val_loss: 0.0165\n",
      "Epoch 231/1000\n",
      "72000/72000 [==============================] - 5s 69us/step - loss: 0.0150 - val_loss: 0.0147\n",
      "Epoch 232/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0145 - val_loss: 0.0186\n",
      "Epoch 233/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0145 - val_loss: 0.0135\n",
      "Epoch 234/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.0146 - val_loss: 0.0144\n",
      "Epoch 235/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.0144 - val_loss: 0.0158\n",
      "Epoch 236/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0138 - val_loss: 0.0145\n",
      "Epoch 237/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0144 - val_loss: 0.0131\n",
      "Epoch 238/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.0140 - val_loss: 0.0104\n",
      "Epoch 239/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.0144 - val_loss: 0.0120\n",
      "Epoch 240/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.0135 - val_loss: 0.0125\n",
      "Epoch 241/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0132 - val_loss: 0.0138\n",
      "Epoch 242/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0145 - val_loss: 0.0150\n",
      "Epoch 243/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0139 - val_loss: 0.0138\n",
      "Epoch 244/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0140 - val_loss: 0.0121\n",
      "Epoch 245/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.0121 - val_loss: 0.0143\n",
      "Epoch 246/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0145 - val_loss: 0.0117\n",
      "Epoch 247/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0145 - val_loss: 0.0120\n",
      "Epoch 248/1000\n",
      "72000/72000 [==============================] - 4s 56us/step - loss: 0.0134 - val_loss: 0.0115\n",
      "Epoch 249/1000\n",
      "72000/72000 [==============================] - 4s 56us/step - loss: 0.0136 - val_loss: 0.0143\n",
      "Epoch 250/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0136 - val_loss: 0.0158\n",
      "Epoch 251/1000\n",
      "72000/72000 [==============================] - 5s 65us/step - loss: 0.0133 - val_loss: 0.0143\n",
      "Epoch 252/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 0.0143 - val_loss: 0.0156\n",
      "Epoch 253/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0138 - val_loss: 0.0141\n",
      "Epoch 254/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.0130 - val_loss: 0.0152\n",
      "Epoch 255/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0128 - val_loss: 0.0125\n",
      "Epoch 256/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0139 - val_loss: 0.0134\n",
      "Epoch 257/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0132 - val_loss: 0.0102\n",
      "Epoch 258/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0134 - val_loss: 0.0138\n",
      "Epoch 259/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.0135 - val_loss: 0.0143\n",
      "Epoch 260/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0135 - val_loss: 0.0160\n",
      "Epoch 261/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.0138 - val_loss: 0.0133\n",
      "Epoch 262/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.0134 - val_loss: 0.0152\n",
      "Epoch 263/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0131 - val_loss: 0.0125\n",
      "Epoch 264/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.0128 - val_loss: 0.0106\n",
      "Epoch 265/1000\n",
      "72000/72000 [==============================] - 5s 64us/step - loss: 0.0135 - val_loss: 0.0136\n",
      "Epoch 266/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.0130 - val_loss: 0.0134\n",
      "Epoch 267/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0128 - val_loss: 0.0110\n",
      "Epoch 268/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0133 - val_loss: 0.0121\n",
      "Epoch 269/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0134 - val_loss: 0.0152\n",
      "Epoch 270/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0127 - val_loss: 0.0119\n",
      "Epoch 271/1000\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.0127 - val_loss: 0.0139\n",
      "Epoch 272/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0127 - val_loss: 0.0121\n",
      "Epoch 273/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0132 - val_loss: 0.0117\n",
      "Epoch 274/1000\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.0129 - val_loss: 0.0137\n",
      "Epoch 275/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0134 - val_loss: 0.0116\n",
      "Epoch 276/1000\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.0123 - val_loss: 0.0153\n",
      "Epoch 277/1000\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.0136 - val_loss: 0.0108\n",
      "Epoch 00277: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f384b52b438>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder8_8_tap_4l.fit(train_data256, train_data256,\n",
    "                       epochs=1000,\n",
    "                       batch_size=1000*M,\n",
    "                       shuffle=True,\n",
    "                       validation_data=(valid_data256,\n",
    "                                        valid_data256),\n",
    "                       callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 8000 samples\n",
      "Epoch 1/1000\n",
      "72000/72000 [==============================] - 6s 81us/step - loss: 5.5475 - val_loss: 5.5126\n",
      "Epoch 2/1000\n",
      "72000/72000 [==============================] - 4s 52us/step - loss: 5.5129 - val_loss: 5.4760\n",
      "Epoch 3/1000\n",
      "72000/72000 [==============================] - 4s 58us/step - loss: 5.4760 - val_loss: 5.4379\n",
      "Epoch 4/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 5.4376 - val_loss: 5.3922\n",
      "Epoch 5/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 5.3909 - val_loss: 5.3309\n",
      "Epoch 6/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 5.3311 - val_loss: 5.2667\n",
      "Epoch 7/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 5.2652 - val_loss: 5.1820\n",
      "Epoch 8/1000\n",
      "72000/72000 [==============================] - 5s 67us/step - loss: 5.1813 - val_loss: 5.0857\n",
      "Epoch 9/1000\n",
      "72000/72000 [==============================] - 5s 69us/step - loss: 5.0844 - val_loss: 4.9722\n",
      "Epoch 10/1000\n",
      "72000/72000 [==============================] - 5s 71us/step - loss: 4.9698 - val_loss: 4.8402\n",
      "Epoch 11/1000\n",
      "72000/72000 [==============================] - 5s 66us/step - loss: 4.8378 - val_loss: 4.6901\n",
      "Epoch 12/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 4.6883 - val_loss: 4.5258\n",
      "Epoch 13/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 4.5211 - val_loss: 4.3437\n",
      "Epoch 14/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 4.3403 - val_loss: 4.1538\n",
      "Epoch 15/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 4.1522 - val_loss: 3.9728\n",
      "Epoch 16/1000\n",
      "72000/72000 [==============================] - 5s 64us/step - loss: 3.9661 - val_loss: 3.7791\n",
      "Epoch 17/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 3.7713 - val_loss: 3.5344\n",
      "Epoch 18/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 3.5287 - val_loss: 3.3464\n",
      "Epoch 19/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 3.3385 - val_loss: 3.0760\n",
      "Epoch 20/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 3.0742 - val_loss: 2.8892\n",
      "Epoch 21/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 2.8907 - val_loss: 2.6319\n",
      "Epoch 22/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 2.6318 - val_loss: 2.4446\n",
      "Epoch 23/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 2.4516 - val_loss: 2.2072\n",
      "Epoch 24/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 2.2103 - val_loss: 2.0421\n",
      "Epoch 25/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 2.0381 - val_loss: 1.8290\n",
      "Epoch 26/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 1.8192 - val_loss: 1.6730\n",
      "Epoch 27/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 1.6591 - val_loss: 1.4730\n",
      "Epoch 28/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 1.4644 - val_loss: 1.3293\n",
      "Epoch 29/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 1.3209 - val_loss: 1.1578\n",
      "Epoch 30/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 1.1513 - val_loss: 1.0441\n",
      "Epoch 31/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 1.0403 - val_loss: 0.9268\n",
      "Epoch 32/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.9201 - val_loss: 0.8113\n",
      "Epoch 33/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.8224 - val_loss: 0.7111\n",
      "Epoch 34/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.7083 - val_loss: 0.6317\n",
      "Epoch 35/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.6326 - val_loss: 0.5685\n",
      "Epoch 36/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.5574 - val_loss: 0.4888\n",
      "Epoch 37/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.4881 - val_loss: 0.4421\n",
      "Epoch 38/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.4326 - val_loss: 0.4068\n",
      "Epoch 39/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.3891 - val_loss: 0.3525\n",
      "Epoch 40/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.3404 - val_loss: 0.3218\n",
      "Epoch 41/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.3142 - val_loss: 0.2789\n",
      "Epoch 42/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.2791 - val_loss: 0.2662\n",
      "Epoch 43/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.2506 - val_loss: 0.2277\n",
      "Epoch 44/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.2291 - val_loss: 0.2142\n",
      "Epoch 45/1000\n",
      "72000/72000 [==============================] - 5s 64us/step - loss: 0.2123 - val_loss: 0.1897\n",
      "Epoch 46/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.1921 - val_loss: 0.1784\n",
      "Epoch 47/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.1782 - val_loss: 0.1742\n",
      "Epoch 48/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.1644 - val_loss: 0.1492\n",
      "Epoch 49/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.1544 - val_loss: 0.1486\n",
      "Epoch 50/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.1464 - val_loss: 0.1436\n",
      "Epoch 51/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.1319 - val_loss: 0.1302\n",
      "Epoch 52/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.1290 - val_loss: 0.1245\n",
      "Epoch 53/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.1191 - val_loss: 0.1087\n",
      "Epoch 54/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.1128 - val_loss: 0.1102\n",
      "Epoch 55/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.1084 - val_loss: 0.1020\n",
      "Epoch 56/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.1024 - val_loss: 0.0918\n",
      "Epoch 57/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.0984 - val_loss: 0.0953\n",
      "Epoch 58/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0922 - val_loss: 0.0909\n",
      "Epoch 59/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0881 - val_loss: 0.0817\n",
      "Epoch 60/1000\n",
      "72000/72000 [==============================] - 5s 64us/step - loss: 0.0855 - val_loss: 0.0832\n",
      "Epoch 61/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.0821 - val_loss: 0.0845\n",
      "Epoch 62/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0804 - val_loss: 0.0783\n",
      "Epoch 63/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0775 - val_loss: 0.0711\n",
      "Epoch 64/1000\n",
      "72000/72000 [==============================] - 5s 65us/step - loss: 0.0728 - val_loss: 0.0735\n",
      "Epoch 65/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0726 - val_loss: 0.0716\n",
      "Epoch 66/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0676 - val_loss: 0.0618\n",
      "Epoch 67/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0658 - val_loss: 0.0656\n",
      "Epoch 68/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0661 - val_loss: 0.0616\n",
      "Epoch 69/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.0598 - val_loss: 0.0629\n",
      "Epoch 70/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0610 - val_loss: 0.0550\n",
      "Epoch 71/1000\n",
      "72000/72000 [==============================] - 5s 67us/step - loss: 0.0618 - val_loss: 0.0572\n",
      "Epoch 72/1000\n",
      "72000/72000 [==============================] - 5s 64us/step - loss: 0.0569 - val_loss: 0.0563\n",
      "Epoch 73/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0569 - val_loss: 0.0564\n",
      "Epoch 74/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0541 - val_loss: 0.0532\n",
      "Epoch 75/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0520 - val_loss: 0.0518\n",
      "Epoch 76/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0529 - val_loss: 0.0527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0514 - val_loss: 0.0532\n",
      "Epoch 78/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0488 - val_loss: 0.0497\n",
      "Epoch 79/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0484 - val_loss: 0.0457\n",
      "Epoch 80/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0480 - val_loss: 0.0481\n",
      "Epoch 81/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0468 - val_loss: 0.0489\n",
      "Epoch 82/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0475 - val_loss: 0.0424\n",
      "Epoch 83/1000\n",
      "72000/72000 [==============================] - 5s 67us/step - loss: 0.0447 - val_loss: 0.0440\n",
      "Epoch 84/1000\n",
      "72000/72000 [==============================] - 5s 68us/step - loss: 0.0449 - val_loss: 0.0394\n",
      "Epoch 85/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0432 - val_loss: 0.0373\n",
      "Epoch 86/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0431 - val_loss: 0.0378\n",
      "Epoch 87/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0428 - val_loss: 0.0413\n",
      "Epoch 88/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.0421 - val_loss: 0.0423\n",
      "Epoch 89/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0390 - val_loss: 0.0431\n",
      "Epoch 90/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0412 - val_loss: 0.0365\n",
      "Epoch 91/1000\n",
      "72000/72000 [==============================] - 5s 64us/step - loss: 0.0392 - val_loss: 0.0386\n",
      "Epoch 92/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0403 - val_loss: 0.0412\n",
      "Epoch 93/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0379 - val_loss: 0.0408\n",
      "Epoch 94/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0388 - val_loss: 0.0381\n",
      "Epoch 95/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.0379 - val_loss: 0.0320\n",
      "Epoch 96/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0397 - val_loss: 0.0386\n",
      "Epoch 97/1000\n",
      "72000/72000 [==============================] - 5s 65us/step - loss: 0.0356 - val_loss: 0.0347\n",
      "Epoch 98/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.0349 - val_loss: 0.0373\n",
      "Epoch 99/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0349 - val_loss: 0.0371\n",
      "Epoch 100/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0348 - val_loss: 0.0364\n",
      "Epoch 101/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0338 - val_loss: 0.0381\n",
      "Epoch 102/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.0347 - val_loss: 0.0374\n",
      "Epoch 103/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0334 - val_loss: 0.0395\n",
      "Epoch 104/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0324 - val_loss: 0.0392\n",
      "Epoch 105/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0319 - val_loss: 0.0326\n",
      "Epoch 106/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.0323 - val_loss: 0.0331\n",
      "Epoch 107/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0332 - val_loss: 0.0293\n",
      "Epoch 108/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0330 - val_loss: 0.0303\n",
      "Epoch 109/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0323 - val_loss: 0.0267\n",
      "Epoch 110/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0305 - val_loss: 0.0283\n",
      "Epoch 111/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.0296 - val_loss: 0.0293\n",
      "Epoch 112/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0306 - val_loss: 0.0313\n",
      "Epoch 113/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0320 - val_loss: 0.0301\n",
      "Epoch 114/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0294 - val_loss: 0.0274\n",
      "Epoch 115/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0292 - val_loss: 0.0320\n",
      "Epoch 116/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0283 - val_loss: 0.0329\n",
      "Epoch 117/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0284 - val_loss: 0.0319\n",
      "Epoch 118/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.0281 - val_loss: 0.0292\n",
      "Epoch 119/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0275 - val_loss: 0.0281\n",
      "Epoch 120/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0263 - val_loss: 0.0251\n",
      "Epoch 121/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0258 - val_loss: 0.0288\n",
      "Epoch 122/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.0278 - val_loss: 0.0284\n",
      "Epoch 123/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0274 - val_loss: 0.0293\n",
      "Epoch 124/1000\n",
      "72000/72000 [==============================] - 5s 66us/step - loss: 0.0266 - val_loss: 0.0282\n",
      "Epoch 125/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0256 - val_loss: 0.0276\n",
      "Epoch 126/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0263 - val_loss: 0.0255\n",
      "Epoch 127/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0267 - val_loss: 0.0297\n",
      "Epoch 128/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0266 - val_loss: 0.0276\n",
      "Epoch 129/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0252 - val_loss: 0.0231\n",
      "Epoch 130/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0251 - val_loss: 0.0224\n",
      "Epoch 131/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0253 - val_loss: 0.0228\n",
      "Epoch 132/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0251 - val_loss: 0.0196\n",
      "Epoch 133/1000\n",
      "72000/72000 [==============================] - 5s 65us/step - loss: 0.0249 - val_loss: 0.0248\n",
      "Epoch 134/1000\n",
      "72000/72000 [==============================] - 5s 70us/step - loss: 0.0251 - val_loss: 0.0244\n",
      "Epoch 135/1000\n",
      "72000/72000 [==============================] - 5s 74us/step - loss: 0.0251 - val_loss: 0.0208\n",
      "Epoch 136/1000\n",
      "72000/72000 [==============================] - 5s 67us/step - loss: 0.0235 - val_loss: 0.0243\n",
      "Epoch 137/1000\n",
      "72000/72000 [==============================] - 5s 76us/step - loss: 0.0237 - val_loss: 0.0207\n",
      "Epoch 138/1000\n",
      "72000/72000 [==============================] - 6s 79us/step - loss: 0.0245 - val_loss: 0.0235\n",
      "Epoch 139/1000\n",
      "72000/72000 [==============================] - 6s 78us/step - loss: 0.0240 - val_loss: 0.0240\n",
      "Epoch 140/1000\n",
      "72000/72000 [==============================] - 5s 75us/step - loss: 0.0219 - val_loss: 0.0243\n",
      "Epoch 141/1000\n",
      "72000/72000 [==============================] - 6s 85us/step - loss: 0.0236 - val_loss: 0.0187\n",
      "Epoch 142/1000\n",
      "72000/72000 [==============================] - 6s 84us/step - loss: 0.0225 - val_loss: 0.0252\n",
      "Epoch 143/1000\n",
      "72000/72000 [==============================] - 6s 84us/step - loss: 0.0228 - val_loss: 0.0230\n",
      "Epoch 144/1000\n",
      "72000/72000 [==============================] - 6s 81us/step - loss: 0.0224 - val_loss: 0.0212\n",
      "Epoch 145/1000\n",
      "72000/72000 [==============================] - 6s 82us/step - loss: 0.0223 - val_loss: 0.0201\n",
      "Epoch 146/1000\n",
      "72000/72000 [==============================] - 6s 84us/step - loss: 0.0225 - val_loss: 0.0227\n",
      "Epoch 147/1000\n",
      "72000/72000 [==============================] - 6s 86us/step - loss: 0.0225 - val_loss: 0.0208\n",
      "Epoch 148/1000\n",
      "72000/72000 [==============================] - 6s 89us/step - loss: 0.0219 - val_loss: 0.0212\n",
      "Epoch 149/1000\n",
      "72000/72000 [==============================] - 5s 74us/step - loss: 0.0220 - val_loss: 0.0188\n",
      "Epoch 150/1000\n",
      "72000/72000 [==============================] - 5s 66us/step - loss: 0.0215 - val_loss: 0.0204\n",
      "Epoch 151/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0206 - val_loss: 0.0216\n",
      "Epoch 152/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.0216 - val_loss: 0.0201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0210 - val_loss: 0.0204\n",
      "Epoch 154/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0218 - val_loss: 0.0197\n",
      "Epoch 155/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0214 - val_loss: 0.0204\n",
      "Epoch 156/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0206 - val_loss: 0.0214\n",
      "Epoch 157/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0211 - val_loss: 0.0206\n",
      "Epoch 158/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0203 - val_loss: 0.0183\n",
      "Epoch 159/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0203 - val_loss: 0.0238\n",
      "Epoch 160/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0189 - val_loss: 0.0207\n",
      "Epoch 161/1000\n",
      "72000/72000 [==============================] - 5s 64us/step - loss: 0.0204 - val_loss: 0.0165\n",
      "Epoch 162/1000\n",
      "72000/72000 [==============================] - 6s 80us/step - loss: 0.0215 - val_loss: 0.0201\n",
      "Epoch 163/1000\n",
      "72000/72000 [==============================] - 5s 70us/step - loss: 0.0193 - val_loss: 0.0188\n",
      "Epoch 164/1000\n",
      "72000/72000 [==============================] - 5s 76us/step - loss: 0.0204 - val_loss: 0.0175\n",
      "Epoch 165/1000\n",
      "72000/72000 [==============================] - 5s 65us/step - loss: 0.0199 - val_loss: 0.0176\n",
      "Epoch 166/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0184 - val_loss: 0.0200\n",
      "Epoch 167/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0204 - val_loss: 0.0186\n",
      "Epoch 168/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.0192 - val_loss: 0.0213\n",
      "Epoch 169/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0187 - val_loss: 0.0164\n",
      "Epoch 170/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0182 - val_loss: 0.0184\n",
      "Epoch 171/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0182 - val_loss: 0.0186\n",
      "Epoch 172/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0175 - val_loss: 0.0191\n",
      "Epoch 173/1000\n",
      "72000/72000 [==============================] - 5s 64us/step - loss: 0.0180 - val_loss: 0.0193\n",
      "Epoch 174/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.0186 - val_loss: 0.0167\n",
      "Epoch 175/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0191 - val_loss: 0.0154\n",
      "Epoch 176/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0172 - val_loss: 0.0173\n",
      "Epoch 177/1000\n",
      "72000/72000 [==============================] - 5s 71us/step - loss: 0.0179 - val_loss: 0.0145\n",
      "Epoch 178/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0170 - val_loss: 0.0211\n",
      "Epoch 179/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0175 - val_loss: 0.0208\n",
      "Epoch 180/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0167 - val_loss: 0.0192\n",
      "Epoch 181/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0170 - val_loss: 0.0171\n",
      "Epoch 182/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.0183 - val_loss: 0.0176\n",
      "Epoch 183/1000\n",
      "72000/72000 [==============================] - 5s 64us/step - loss: 0.0186 - val_loss: 0.0204\n",
      "Epoch 184/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0177 - val_loss: 0.0208\n",
      "Epoch 185/1000\n",
      "72000/72000 [==============================] - 5s 64us/step - loss: 0.0168 - val_loss: 0.0180\n",
      "Epoch 186/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0159 - val_loss: 0.0162\n",
      "Epoch 187/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0169 - val_loss: 0.0192\n",
      "Epoch 188/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0165 - val_loss: 0.0161\n",
      "Epoch 189/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0171 - val_loss: 0.0164\n",
      "Epoch 190/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0165 - val_loss: 0.0163\n",
      "Epoch 191/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0166 - val_loss: 0.0177\n",
      "Epoch 192/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 193/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0169 - val_loss: 0.0148\n",
      "Epoch 194/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0160 - val_loss: 0.0189\n",
      "Epoch 195/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0157 - val_loss: 0.0147\n",
      "Epoch 196/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0157 - val_loss: 0.0149\n",
      "Epoch 197/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0161 - val_loss: 0.0125\n",
      "Epoch 198/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0159 - val_loss: 0.0222\n",
      "Epoch 199/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0164 - val_loss: 0.0147\n",
      "Epoch 200/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0170 - val_loss: 0.0132\n",
      "Epoch 201/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0159 - val_loss: 0.0134\n",
      "Epoch 202/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0160 - val_loss: 0.0161\n",
      "Epoch 203/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0151 - val_loss: 0.0170\n",
      "Epoch 204/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0156 - val_loss: 0.0169\n",
      "Epoch 205/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.0154 - val_loss: 0.0161\n",
      "Epoch 206/1000\n",
      "72000/72000 [==============================] - 5s 63us/step - loss: 0.0153 - val_loss: 0.0147\n",
      "Epoch 207/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0160 - val_loss: 0.0196\n",
      "Epoch 208/1000\n",
      "72000/72000 [==============================] - 5s 67us/step - loss: 0.0152 - val_loss: 0.0134\n",
      "Epoch 209/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0150 - val_loss: 0.0124\n",
      "Epoch 210/1000\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.0159 - val_loss: 0.0193\n",
      "Epoch 211/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0146 - val_loss: 0.0150\n",
      "Epoch 212/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0146 - val_loss: 0.0157\n",
      "Epoch 213/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.0155 - val_loss: 0.0164\n",
      "Epoch 214/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0149 - val_loss: 0.0145\n",
      "Epoch 215/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0151 - val_loss: 0.0143\n",
      "Epoch 216/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0136 - val_loss: 0.0117\n",
      "Epoch 217/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0143 - val_loss: 0.0180\n",
      "Epoch 218/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.0142 - val_loss: 0.0144\n",
      "Epoch 219/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0144 - val_loss: 0.0156\n",
      "Epoch 220/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0150 - val_loss: 0.0146\n",
      "Epoch 221/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0136 - val_loss: 0.0126\n",
      "Epoch 222/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0142 - val_loss: 0.0128\n",
      "Epoch 223/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0147 - val_loss: 0.0156\n",
      "Epoch 224/1000\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.0139 - val_loss: 0.0141\n",
      "Epoch 225/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0144 - val_loss: 0.0162\n",
      "Epoch 226/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0133 - val_loss: 0.0141\n",
      "Epoch 227/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0143 - val_loss: 0.0123\n",
      "Epoch 228/1000\n",
      "72000/72000 [==============================] - 5s 67us/step - loss: 0.0134 - val_loss: 0.0148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0139 - val_loss: 0.0129\n",
      "Epoch 230/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0144 - val_loss: 0.0119\n",
      "Epoch 231/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.0134 - val_loss: 0.0151\n",
      "Epoch 232/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0155 - val_loss: 0.0137\n",
      "Epoch 233/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0143 - val_loss: 0.0133\n",
      "Epoch 234/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0140 - val_loss: 0.0129\n",
      "Epoch 235/1000\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.0133 - val_loss: 0.0167\n",
      "Epoch 236/1000\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.0126 - val_loss: 0.0168\n",
      "Epoch 00236: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f384b52bb70>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder8_8_tap_5l.fit(train_data256, train_data256,\n",
    "                       epochs=1000,\n",
    "                       batch_size=1000*M,\n",
    "                       shuffle=True,\n",
    "                       validation_data=(valid_data256,\n",
    "                                        valid_data256),\n",
    "                       callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/1000\n",
      "7200000/7200000 [==============================] - 20s 3us/step - loss: 1.7905 - val_loss: 0.8764\n",
      "Epoch 2/1000\n",
      "7200000/7200000 [==============================] - 21s 3us/step - loss: 0.4575 - val_loss: 0.2147\n",
      "Epoch 3/1000\n",
      "7200000/7200000 [==============================] - 20s 3us/step - loss: 0.1323 - val_loss: 0.0809\n",
      "Epoch 4/1000\n",
      "7200000/7200000 [==============================] - 22s 3us/step - loss: 0.0584 - val_loss: 0.0423\n",
      "Epoch 5/1000\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 0.0332 - val_loss: 0.0261\n",
      "Epoch 6/1000\n",
      "7200000/7200000 [==============================] - 21s 3us/step - loss: 0.0216 - val_loss: 0.0179\n",
      "Epoch 7/1000\n",
      "7200000/7200000 [==============================] - 20s 3us/step - loss: 0.0153 - val_loss: 0.0130\n",
      "Epoch 8/1000\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 0.0112 - val_loss: 0.0098\n",
      "Epoch 9/1000\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 0.0086 - val_loss: 0.0075\n",
      "Epoch 10/1000\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 0.0067 - val_loss: 0.0061\n",
      "Epoch 11/1000\n",
      "7200000/7200000 [==============================] - 22s 3us/step - loss: 0.0054 - val_loss: 0.0048\n",
      "Epoch 12/1000\n",
      "7200000/7200000 [==============================] - 18s 3us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 13/1000\n",
      "7200000/7200000 [==============================] - 20s 3us/step - loss: 0.0037 - val_loss: 0.0033\n",
      "Epoch 14/1000\n",
      "7200000/7200000 [==============================] - 23s 3us/step - loss: 0.0031 - val_loss: 0.0028\n",
      "Epoch 15/1000\n",
      "7200000/7200000 [==============================] - 20s 3us/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 16/1000\n",
      "7200000/7200000 [==============================] - 21s 3us/step - loss: 0.0024 - val_loss: 0.0022\n",
      "Epoch 17/1000\n",
      "7200000/7200000 [==============================] - 23s 3us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 18/1000\n",
      "7200000/7200000 [==============================] - 20s 3us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 19/1000\n",
      "7200000/7200000 [==============================] - 20s 3us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 20/1000\n",
      "7200000/7200000 [==============================] - 20s 3us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 21/1000\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 22/1000\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 23/1000\n",
      "7200000/7200000 [==============================] - 20s 3us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 24/1000\n",
      "7200000/7200000 [==============================] - 20s 3us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 25/1000\n",
      "7200000/7200000 [==============================] - 20s 3us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 26/1000\n",
      "7200000/7200000 [==============================] - 22s 3us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 27/1000\n",
      "7200000/7200000 [==============================] - 21s 3us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 28/1000\n",
      "7200000/7200000 [==============================] - 20s 3us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 29/1000\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 9.9581e-04 - val_loss: 0.0010\n",
      "Epoch 30/1000\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 9.7107e-04 - val_loss: 9.5233e-04\n",
      "Epoch 31/1000\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 9.6171e-04 - val_loss: 8.3524e-04\n",
      "Epoch 32/1000\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 9.1468e-04 - val_loss: 8.6523e-04\n",
      "Epoch 33/1000\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 9.0541e-04 - val_loss: 9.7068e-04\n",
      "Epoch 34/1000\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 8.9190e-04 - val_loss: 8.8578e-04\n",
      "Epoch 35/1000\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 8.8760e-04 - val_loss: 9.1321e-04\n",
      "Epoch 36/1000\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 8.7563e-04 - val_loss: 8.6942e-04\n",
      "Epoch 37/1000\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 8.6669e-04 - val_loss: 8.1673e-04\n",
      "Epoch 38/1000\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 8.2314e-04 - val_loss: 8.4260e-04\n",
      "Epoch 39/1000\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 8.2898e-04 - val_loss: 8.8512e-04\n",
      "Epoch 40/1000\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 8.2971e-04 - val_loss: 8.1457e-04\n",
      "Epoch 41/1000\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 8.2004e-04 - val_loss: 8.9759e-04\n",
      "Epoch 42/1000\n",
      "7200000/7200000 [==============================] - 16s 2us/step - loss: 7.8958e-04 - val_loss: 8.2086e-04\n",
      "Epoch 43/1000\n",
      "7200000/7200000 [==============================] - 15s 2us/step - loss: 8.0297e-04 - val_loss: 7.3083e-04\n",
      "Epoch 44/1000\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 7.9910e-04 - val_loss: 7.9023e-04\n",
      "Epoch 45/1000\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 8.0325e-04 - val_loss: 8.3187e-04\n",
      "Epoch 46/1000\n",
      "7200000/7200000 [==============================] - 16s 2us/step - loss: 7.6900e-04 - val_loss: 7.4783e-04\n",
      "Epoch 47/1000\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 7.9947e-04 - val_loss: 9.0511e-04\n",
      "Epoch 48/1000\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 7.7810e-04 - val_loss: 7.4545e-04\n",
      "Epoch 49/1000\n",
      "7200000/7200000 [==============================] - 20s 3us/step - loss: 7.5657e-04 - val_loss: 7.5247e-04\n",
      "Epoch 50/1000\n",
      "7200000/7200000 [==============================] - 20s 3us/step - loss: 7.6456e-04 - val_loss: 7.6282e-04\n",
      "Epoch 51/1000\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 7.4585e-04 - val_loss: 8.8851e-04\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1937878048>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder7_4.fit(train_data16, train_data16,\n",
    "                epochs=1000,\n",
    "                batch_size=1000*M,\n",
    "                shuffle=True,\n",
    "                validation_data=(valid_data16,\n",
    "                                 valid_data16),\n",
    "                callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder2.save('second_qpsk_model.model')\n",
    "# autoencoder2.save_weights('second_qpsk_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the validation loss to the end of the name\n",
    "# autoencoder2_2.save('./models/autoencoder_2_2_leaky_relu.model')\n",
    "# autoencoder2_2.save_weights('./models/autoencoder_2_2_leaky_relu.h5')\n",
    "\n",
    "# autoencoder2_2.save('./models/autoencoder_2_2_leaky_relu_0.0047.model')\n",
    "# autoencoder2_2.save_weights('./models/autoencoder_2_2_leaky_relu_0.0047.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder2_4.save('./models/autoencoder_2_4_leaky_relu.model')\n",
    "# autoencoder2_4.save_weights('./models/autoencoder_2_4_leaky_relu.h5')\n",
    "\n",
    "# autoencoder2_4.save('./models/autoencoder_2_4_leaky_relu0.5170.model')\n",
    "# autoencoder2_4.save_weights('./models/autoencoder_2_4_leaky_relu0.5170.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder8_8.save('./models/autoencoder_8_8_leaky_relu1.0618.model')\n",
    "# autoencoder8_8.save_weights('./models/autoencoder_8_8_leaky_relu1.0618.h5')\n",
    "\n",
    "# autoencoder8_8.save('./models/autoencoder_8_8_leaky_relu1.2535.model')\n",
    "# autoencoder8_8.save_weights('./models/autoencoder_8_8_leaky_relu1.2535.h5')\n",
    "\n",
    "# Changed from 8_4 to 8_8\n",
    "# autoencoder8_8.save('./models/autoencoder_8_8_leaky_relu0.0750.model')\n",
    "# autoencoder8_8.save_weights('./models/autoencoder_8_8_leaky_relu0.0750.h5')\n",
    "\n",
    "# autoencoder8_8.save('./models/autoencoder_8_8_leaky_relu0.0149.model')\n",
    "# autoencoder8_8.save_weights('./models/autoencoder_8_8_leaky_relu0.0149.h5')\n",
    "\n",
    "# autoencoder8_8.save('./models/autoencoder_8_8_leaky_relu0.0123.model')\n",
    "# autoencoder8_8.save_weights('./models/autoencoder_8_8_leaky_relu0.0123.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder8_8_tap.save('./models/autoencoder_8_8_tap_leaky_relu0.0349.model')\n",
    "# autoencoder8_8_tap.save_weights('./models/autoencoder_8_8_tap_leaky_relu0.0349.h5')\n",
    "\n",
    "# autoencoder8_8_tap.save('./models/autoencoder_8_8_tap_leaky_relu0.0162.model')\n",
    "# autoencoder8_8_tap.save_weights('./models/autoencoder_8_8_tap_leaky_relu0.0162.h5')\n",
    "\n",
    "# autoencoder8_8_tap.save('./models/autoencoder_8_8_tap_leaky_relu0.0144.model')\n",
    "# autoencoder8_8_tap.save_weights('./models/autoencoder_8_8_tap_leaky_relu0.0144.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to add final validation losses to all of these\n",
    "# autoencoder8_8_tap_2l.save('./models/autoencoder8_8_tap_2l0.0166.model')\n",
    "# autoencoder8_8_tap_2l.save_weights('./models/autoencoder8_8_tap_2l0.0166.h5')\n",
    "\n",
    "# autoencoder8_8_tap_3l.save('./models/autoencoder8_8_tap_3l0.0111.model')\n",
    "# autoencoder8_8_tap_3l.save_weights('./models/autoencoder8_8_tap_3l0.0111.h5')\n",
    "\n",
    "# autoencoder8_8_tap_4l.save('./models/autoencoder8_8_tap_4l0.0108.model')\n",
    "# autoencoder8_8_tap_4l.save_weights('./models/autoencoder8_8_tap_4l0.0108.h5')\n",
    "\n",
    "# autoencoder8_8_tap_5l.save('./models/autoencoder8_8_tap_5l0.0168.model')\n",
    "# autoencoder8_8_tap_5l.save_weights('./models/autoencoder8_8_tap_5l0.0168.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder7_4.save('./models/autoencoder_7_4_leaky_relu8.8851e-4.model')\n",
    "# autoencoder7_4.save_weights('./models/autoencoder_7_4_leaky_relu8.8851e-4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder2_2.load_weights('./models/autoencoder_2_2_leaky_relu_0.0047.h5', by_name=True)\n",
    "autoencoder2_4.load_weights('./models/autoencoder_2_4_leaky_relu.h5', by_name=True)\n",
    "autoencoder8_8.load_weights('./models/autoencoder_8_8_leaky_relu0.0123.h5', by_name=True)\n",
    "# autoencoder8_8_tap.load_weights('./models/autoencoder_8_8_tap_leaky_relu0.0144.h5', by_name=True)\n",
    "autoencoder7_4.load_weights('./models/autoencoder_7_4_leaky_relu8.8851e-4.h5', by_name=True)\n",
    "\n",
    "autoencoder8_8_tap_2l.load_weights('./models/autoencoder8_8_tap_2l0.0166.h5', by_name=True)\n",
    "autoencoder8_8_tap_3l.load_weights('./models/autoencoder8_8_tap_3l0.0111.h5', by_name=True)\n",
    "autoencoder8_8_tap_4l.load_weights('./models/autoencoder8_8_tap_4l0.0108.h5', by_name=True)\n",
    "autoencoder8_8_tap_5l.load_weights('./models/autoencoder8_8_tap_5l0.0168.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## QPSK\n",
    "# autoencoder_loaded, transmitter_loaded, reciever_loaded, autoencoder_symbs_loaded, k, Nc, Nr = make_model(4, 2, sigma, \"relu\")\n",
    "# # autoencoder_loaded = load_model('first_qpsk_model.model')\n",
    "# autoencoder_loaded.load_weights('second_qpsk_model.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 16-QAM\n",
    "# autoencoder_loaded, transmitter_loaded, reciever_loaded, \\\n",
    "#     autoencoder_symbs_loaded, k, Nc, Nr = make_model(4, 2, sigma, \"relu\")\n",
    "# autoencoder_loaded.load_weights('./models/second_qpsk_model.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_one_hot_messages4 = np.diag(np.ones(4))\n",
    "all_one_hot_messages16 = np.diag(np.ones(16))\n",
    "all_one_hot_messages256 = np.diag(np.ones(256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Autoencoder2\n",
    "# # 10 epochs, sigma = 7, M=4,Nc=1\n",
    "# # Batch size = 1000*M (=4)\n",
    "# autoencoder2_2.predict(all_one_hot_messages4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The new ones\n",
    "# # Autoencoder2\n",
    "# # 10 epochs, sigma = 7, M=4,Nc=1\n",
    "# # Batch size = 1000*M (=4)\n",
    "# autoencoder2_2.predict(all_one_hot_messages4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Autoencoder2\n",
    "# # 10 epochs, sigma = 7, M=4,Nc=1\n",
    "# # Batch size = 1000*M (=4)\n",
    "# autoencoder_loaded.predict(all_one_hot_messages4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16-QAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Autoencoder16\n",
    "# # 10 epochs, sigma = 7, M=16,Nc=1\n",
    "# # Batch size = M (=4)\n",
    "# autoencoder2_4.predict(all_one_hot_messages16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Autoencoder16\n",
    "# # 10 epochs, sigma = 7, M=16,Nc=1\n",
    "# # Batch size = M (=4)\n",
    "# autoencoder7_4.predict(all_one_hot_messages16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the channel symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally for channel symbols I want to plot the in phase and quaternary symbols as my x and y axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape of the channel symbols section\n",
    "It is currently (M,Nc,2), but it should either be (M,2) or (Nc,2), I don't really get how the n comes into it. I think normally n is the number of bits you'd be allowed to transmit with, so n bits allows $2^n$ different complex channel symbols. But I can have continuous values. So what is the point of the the n. <br>\n",
    "Potentially what I need to do is to reduce the channel symbol layer to only (1,2), then it will have to find continuous values using only two dimensions, which is kind of what I want as I only have two different axes. <br>\n",
    "Alternatively I could make R the ratio between the k and the number of real bits used as opposed to the number of complex bits used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weird encoding\n",
    "For some reason it's never using the 11 option. Only 00, 01 and 10. I don't know why this is but I supect it's something to do with the regularisation.<br>\n",
    "Because of the regularisation I'd expect it to be able to give cc instead of 11, where $c=\\sqrt{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QPSK Constellation Diagrams\n",
    "#### Autoencoder2 (changed back to l2_normalise)\n",
    "axis = 2, 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M=4,Nc=1,Nr=2\n",
      "(4, 1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.9896787 ,  0.14330396]],\n",
       "\n",
       "       [[ 0.9832694 , -0.18215774]],\n",
       "\n",
       "       [[-0.16840957, -0.98571706]],\n",
       "\n",
       "       [[ 0.12707114,  0.9918936 ]]], dtype=float32)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"M={M},Nc={Nc},Nr={Nr}\")\n",
    "print(transmitter2.predict(all_one_hot_messages).shape)\n",
    "#(M,Nc,2)\n",
    "transmitter2.predict(all_one_hot_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_constellation_diagram(all_one_hot_messages, transmitter2, f\"M={M},Nc={Nc},Nr={Nr}\")\n",
    "# plt.savefig(\"./figures/qpsk_constellation_diagram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_constellation_diagram(all_one_hot_messages4, transmitter4, f\"M={M},Nc={Nc4},Nr={Nr4}\")\n",
    "# plt.savefig(\"./figures/qpsk_constellation_diagram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoder4 trained from fresh2\n",
    "axis = 2, 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M=4,Nc=1,Nr=2\n",
      "(4, 1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.8381607 ,  0.5454234 ]],\n",
       "\n",
       "       [[-0.0331063 ,  0.9994519 ]],\n",
       "\n",
       "       [[ 0.83851695,  0.54487556]],\n",
       "\n",
       "       [[ 0.9236054 , -0.3833446 ]]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"M={M},Nc={Nc4},Nr={Nr4}\")\n",
    "print(transmitter4.predict(all_one_hot_messages4).shape)\n",
    "#(M,Nc,2)\n",
    "transmitter4.predict(all_one_hot_messages4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_constellation_diagram(all_one_hot_messages4, transmitter4, f\"M={M},Nc={Nc4},Nr={Nr4}\")\n",
    "# plt.savefig(\"./figures/qpsk_constellation_diagram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoder4 trained from fresh2\n",
    "Same as above, but trained for 10 epochs (an extra 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M=4,Nc=1,Nr=2\n",
      "(4, 1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.83112544,  0.556085  ]],\n",
       "\n",
       "       [[ 0.02088728,  0.9997818 ]],\n",
       "\n",
       "       [[ 0.785271  ,  0.61915225]],\n",
       "\n",
       "       [[ 0.92870617, -0.37081653]]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"M={M},Nc={Nc4},Nr={Nr4}\")\n",
    "print(transmitter4.predict(all_one_hot_messages4).shape)\n",
    "#(M,Nc,2)\n",
    "transmitter4.predict(all_one_hot_messages4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_constellation_diagram(all_one_hot_messages4, transmitter4, f\"M={M},Nc={Nc4},Nr={Nr4}\")\n",
    "# plt.savefig(\"./figures/qpsk_constellation_diagram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoder4 trained from fresh3\n",
    "Same as above, but trained for 100 epochs (an extra 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M=4,Nc=1,Nr=2\n",
      "(4, 1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.81807053,  0.5751178 ]],\n",
       "\n",
       "       [[ 0.05629861,  0.998414  ]],\n",
       "\n",
       "       [[ 0.87932265,  0.47622645]],\n",
       "\n",
       "       [[ 0.88242936, -0.47044495]]], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"M={M},Nc={Nc4},Nr={Nr4}\")\n",
    "print(transmitter4.predict(all_one_hot_messages4).shape)\n",
    "#(M,Nc,2)\n",
    "transmitter4.predict(all_one_hot_messages4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_constellation_diagram(all_one_hot_messages4, transmitter4, f\"M={M},Nc={Nc4},Nr={Nr4}\")\n",
    "# plt.savefig(\"./figures/qpsk_constellation_diagram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoder4adam\n",
    "Trained over 10 epochs using an Adam optimiser as used in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M=4,Nc=1,Nr=2\n",
      "(4, 1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.99388415, -0.11042722]],\n",
       "\n",
       "       [[ 0.45482168,  0.89058256]],\n",
       "\n",
       "       [[ 0.6019942 , -0.79850054]],\n",
       "\n",
       "       [[-0.99388415, -0.11042722]]], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"M={M},Nc={Nc4},Nr={Nr4}\")\n",
    "print(transmitter4adam.predict(all_one_hot_messages4).shape)\n",
    "#(M,Nc,2)\n",
    "transmitter4adam.predict(all_one_hot_messages4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_constellation_diagram(all_one_hot_messages4, transmitter4adam, f\"M={M},Nc={Nc4},Nr={Nr4}\")\n",
    "# plt.savefig(\"./figures/qpsk_constellation_diagram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New training set, 1 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M=4,Nc=1,Nr=2\n",
      "(4, 1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.08172178, -2.2294135 ]],\n",
       "\n",
       "       [[-4.237483  , -3.2946773 ]],\n",
       "\n",
       "       [[ 1.7539062 ,  2.044017  ]],\n",
       "\n",
       "       [[-1.433624  ,  1.4104607 ]]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"M={M},Nc={Nc},Nr={Nr}\")\n",
    "print(transmitter.predict(all_one_hot_messages).shape)\n",
    "#(M,Nc,2)\n",
    "transmitter.predict(all_one_hot_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_constellation_diagram(all_one_hot_messages, f\"M={M},Nc={Nc},Nr={Nr}\")\n",
    "# plt.savefig(\"./figures/wrong_constellation_diagram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16-QAM Constellation Diagrams\n",
    "#### Autoencoder16\n",
    "10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M=16,Nc=1,Nr=2\n",
      "(16, 1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.8166406 ,  0.5771466 ]],\n",
       "\n",
       "       [[-0.8171146 ,  0.5764752 ]],\n",
       "\n",
       "       [[-0.81698024,  0.57666564]],\n",
       "\n",
       "       [[-0.81669366,  0.5770714 ]],\n",
       "\n",
       "       [[-0.8170382 ,  0.5765834 ]],\n",
       "\n",
       "       [[-0.81692266,  0.57674736]],\n",
       "\n",
       "       [[-0.81657773,  0.57723564]],\n",
       "\n",
       "       [[-0.8168798 ,  0.5768078 ]],\n",
       "\n",
       "       [[-0.81660795,  0.5771928 ]],\n",
       "\n",
       "       [[-0.81739414,  0.5760787 ]],\n",
       "\n",
       "       [[-0.81669426,  0.57707053]],\n",
       "\n",
       "       [[-0.8165637 ,  0.5772553 ]],\n",
       "\n",
       "       [[-0.81745994,  0.5759855 ]],\n",
       "\n",
       "       [[-0.8167853 ,  0.5769417 ]],\n",
       "\n",
       "       [[-0.8165637 ,  0.5772553 ]],\n",
       "\n",
       "       [[-0.8165637 ,  0.5772553 ]]], dtype=float32)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"M={M},Nc={Nc},Nr={Nr}\")\n",
    "print(transmitter16.predict(all_one_hot_messages16).shape)\n",
    "#(M,Nc,2)\n",
    "transmitter16.predict(all_one_hot_messages16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_constellation_diagram(all_one_hot_messages16, transmitter16, f\"M={M},Nc={Nc},Nr={Nr}\")\n",
    "# plt.savefig(\"./figures/qpsk_constellation_diagram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoder16\n",
    "24 epochs, validation loss converged to 2.7728 and stopped decreasing. Interestingly the points are not exactly the same as they were when the QPSK case had inputs mapping to the same channel symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M=16,Nc=1,Nr=2\n",
      "(16, 1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.816641  ,  0.577146  ]],\n",
       "\n",
       "       [[-0.8171153 ,  0.57647425]],\n",
       "\n",
       "       [[-0.81697845,  0.5766682 ]],\n",
       "\n",
       "       [[-0.8166949 ,  0.5770695 ]],\n",
       "\n",
       "       [[-0.81704015,  0.57658076]],\n",
       "\n",
       "       [[-0.81692564,  0.5767431 ]],\n",
       "\n",
       "       [[-0.8165782 ,  0.57723486]],\n",
       "\n",
       "       [[-0.81688267,  0.5768038 ]],\n",
       "\n",
       "       [[-0.81660825,  0.5771923 ]],\n",
       "\n",
       "       [[-0.81739527,  0.57607704]],\n",
       "\n",
       "       [[-0.81669563,  0.5770686 ]],\n",
       "\n",
       "       [[-0.8165637 ,  0.5772553 ]],\n",
       "\n",
       "       [[-0.8174593 ,  0.5759863 ]],\n",
       "\n",
       "       [[-0.8167832 ,  0.57694477]],\n",
       "\n",
       "       [[-0.8165637 ,  0.5772553 ]],\n",
       "\n",
       "       [[-0.8165637 ,  0.5772553 ]]], dtype=float32)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"M={M},Nc={Nc},Nr={Nr}\")\n",
    "print(transmitter16.predict(all_one_hot_messages16).shape)\n",
    "#(M,Nc,2)\n",
    "transmitter16.predict(all_one_hot_messages16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_constellation_diagram(all_one_hot_messages16, transmitter16, f\"M={M},Nc={Nc},Nr={Nr}\")\n",
    "# plt.savefig(\"./figures/qpsk_constellation_diagram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Leaky Relu Autoencoder \n",
    "##### Autoencoder2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M=4,Nc=1,Nr=2\n",
      "(4, 1, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"M={4},Nc={Nc2_2},Nr={Nr2_2}\")\n",
    "print(transmitter2_2.predict(all_one_hot_messages4).shape)\n",
    "#(M,Nc,2)\n",
    "transmitter2_2.predict(all_one_hot_messages4)\n",
    "\n",
    "# The constellation diagram\n",
    "plot_constellation_diagram(all_one_hot_messages4, transmitter2_2, f\"M={M},Nc={Nc2_2},Nr={Nr2_2}\")\n",
    "# plt.savefig(\"./figures/leaky_relu_qpsk_constellation_diagram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Autoencoder2_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M=256,Nc=1,Nr=2\n",
      "(16, 1, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"M={M},Nc={Nc2_4},Nr={Nr2_4}\")\n",
    "print(transmitter2_4.predict(all_one_hot_messages16).shape)\n",
    "#(M,Nc,2)\n",
    "transmitter2_4.predict(all_one_hot_messages16)\n",
    "\n",
    "# The constellation diagram\n",
    "plot_constellation_diagram(all_one_hot_messages16, transmitter2_4, f\"M={M},Nc={Nc2_4},Nr={Nr2_4}\")\n",
    "# plt.savefig(\"./figures/leaky_relu_16_QAM_constellation_diagram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Autoencoder7_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tSNE_constellation_diagram(all_one_hot_messages16, transmitter7_4)\n",
    "# plt.savefig(\"./figures/leaky_relu_7_4_constellation_diagram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_noisy_tSNE_constellation_diagram(all_one_hot_messages16, channel_sym_with_noise7_4, 10, dot_size=1)\n",
    "# plt.savefig(\"./figures/leaky_relu_7_4_constellation_diagram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztnX9wW9W1779b/tFOoD9CIyaJC5EdW3Lpi+SUzi0tfzy4TRrT2oEbSQ4tgb7eSx0msR1iyzSWXeg0WBKxHSAkqR1umTuFWEaW02KFNg7MlNd5nfs6WL8MLchOSsLrndu5U36UmwHiH2e9P5x9IhsnsaOj3+szo4nkHEl7Szrfs9baa68liAgMw+QvunQPgGGY9MIiwDB5DosAw+Q5LAIMk+ewCDBMnsMiwDB5TsIiIIS4QQjxWyHEn4QQfxRC7NJiYAzDpAaRaJ6AEGIVgFVEFBJCfAZAEMBdRPQnLQbIMExySdgSIKL/JKLQhfv/DeANACWJvi7DMKmhUMsXE0IYAKwH8IcF/q8eQD0AfPrTn775xhtv1PKtMwJFUaDT5WaYJVfnlqvzGh8f/xsR6RdzbMLugPpCQlwL4H8D6CSiY5c71mQyUSwW0+R9M4lXXnkFt912W7qHkRRydW65Oi8hRJCIvrqYYzWRQCFEEYAhAEevJAAMw2QWWqwOCAA/B/AGEe1PfEgMw6QSLSyBWwHcC+AfhRCRC7dva/C6DMOkgIQDg0T0fwAIDcbCMEwayL2wKMMwS4JFgGHyHBYBhslzWAQYJs9hEWCYPIdFgGHyHBYBhslzWAQYJs9hEWCYPIdFgGHyHBYBhslzWAQYJs9hEWCYPIdFgGHyHBYBhslzWAQYJs9hEWCYPIdFgGHyHBYBhslzWAQYJs9hEWCYPIdFgGHyHBYBhslzWASYhCAiRCIRaNXTkkk9LAJMQkSjUVitVoTDYTz//PNQFCXdQ2KWCIsAkxAWiwVDQ0MYHx/Htm3bMDg4yNZBlsEiwFwV8kQHgKqqKthsNuzYsQNbtmyBz+dT/2UhyHwS7kXI5CeRSAS1tbUYHh6GTqfDG2+8gUOHDmHlypU4ePAgGhoa0NzcjLfeegutra147bXXYLFYMNvEmskk2BJgrgoiwuTkJGKxGKxWKwDguuuum3Pl/+ijj9De3o7u7m5YrVZEo9F0DZe5DGwJMEuGiDA+Po7i4mIYjUa4XC5YrVacPXsWBw8eRGNjI/r6+nDo0CEIIWC327Fp0yZYLJZ0D51ZABYBZslEo1E4nU709PRAp9Ohra0NZ86cwYEDBzA9PY2NGzeitLQUdrsdOt2ssVlVVZXmUTOXgt0BZslYLBYcO3YMdrsdRIT6+nr09fWhqakJJ06cUIWhq6sLiqLwakGGw5YAsyQURcHg4CDsdjvGxsawefNmKIoCm82GJ598EmvWrIHJZEJ9fT3a29uxZs0aCCHgdDoxNDTEFkEGwiLALInBwUHcc889OHPmDBwOB7q7u0FEaG1txfnz57Fz504UFRXh17/+NUpLS1FRUQGbzQa3280xgQyFRYBZEna7HWfOnEFfXx8MBgPa29vh9/sRCATwxhtvoKGhAYqiQAgBk8kEs9mMY8eO8fJgBsMiwCwJnU6Hhx56CJs2bYLZbIbJZJpzgi9btgzd3d0QQsBqtbILkAWwCGQARIRoNAqz2ayupVdVVWXslVMIoZ7Y8Sd4VVUVjh8/rpr9Q0ND7AJkAbw6kEZk1DwcDqOmpgY+nw+1tbWora3NysQaKQ5CiDn3mcyGLYE0IK/8RASbzYbOzk4AgNFoRCAQAICcuoLO32fAwpBZsCWQYogIPp9PTbUdGhpCXV0dAoEAhBCwWCwgopzalhuNRrPawsl1WARSiBSAtrY2uFwu9WovzWebzQafz4cNGzbgnnvuwe7du3NCCCwWCwKBAAKBQE5ZOLkCuwMpIN78lwJgNBoRiUSwefNmdHd3o66uDkNDQ1AUBcuWLcOmTZtw6NAhfOMb38DWrVvTPYWEEEJg/fr16R4GcwnYEkgBkUgENTU1ICIMDQ0BAGw2G2KxGD766CM0NjbOMf+Hh4fhcDjw3HPPwW63p3PoTB7AIpBCZECspaUFjz76qPq38+fPo7GxERs3bsTmzZsxMTGBuro6VFZWqhtwmLkQEcLhMMLhMO9JSBD+haUAuX4u19RlDKCjowMtLS349Kc/jd27d6OgoAD79u1DRUUF/H4/+8+4dCFTDjZqB4tAiqmqqkIgEEBFRQU6OzvxrW99C8XFxSgrK0NxcTEAoLq6GkTES2m46EpFIhEoiqK6TRxs1A5NREAI8YwQ4r+EEK9r8Xq5hqzI6/P51L99+9vfRnNzMyYmJhAIBLB161YcP34cQgi8++67GB8fT+OIMw8iQldXF+655x74fD7OO9AQrVYH/g3AQQC/0Oj1cgqLxQKXy4W2tjYYjUbEYjEUFBTAZrPB6XTC5XIBmP1Bm81m6HQ6DgheQFpOb775Jp566ins3bsXRITa2loQEfbv34+6ujoWggTQRASI6HdCCIMWr5WLCCFQV1cHk8kERVHgcDjQ1NSEI0eOYPv27Whubsb09DRGRkawfv36rF8S1BJ5cu/atQtEhNLSUjidTjQ0NGDNmjVwOp0wmUy8SSkBUpYnIISoB1APAHq9Hq+88kqq3jplnDt37orzmpiYwPnz53HdddfB6XSitLQUNTU1+OUvf4nR0VH8/e9/T81gl8hi5pYsJiYmQES466678P7772PDhg144okn8NOf/hT33HMP/va3v+Hhhx/GbbfdtuTVlHTOK2MgIk1uAAwAXl/MsUajkXKR3/72t1c8RlEUCofDNDMzQ+FwmEKhEJWWltLAwAApipL8QV4li5mb1szMzNDAwABNT0/TwMAArV69mvR6Pa1atYqamppodHSUysrKyOPxUGFhIXk8niV/humYVyoAMEqLPHc5YzDFyN11kUgEVqsVfr+fi25cgsHBQWzbtg1nzpxBa2srKioqEIvF8NZbb+Hhhx/GypUr1Y5HRITe3l5s2rSJXYMlwkuEaUK276qqquII9yWQXY1+9rOfYWxsDEIItLa2wmAw4LOf/SyeeuopTExMwG63w2AwqGLKLA2tlgi9AP4dgEkI8RchxL9o8bq5DO+3vzKvvfYajh8/jscee0zdXTk1NQWj0YjDhw9jeHgYiqJg7969aGpq4tyKq0QTESCi7xLRKiIqIqIvEtHPtXhdJr+R1pJcAhRCoKioCBMTE3A4HHj55Zdx33334Q9/+APnViQAxwSYjEXWV5CJQWazGd3d3epGK9nkxGq14tZbb+XciquERYDJaOQeAQDo6emBw+FQk4S+8pWv4OabbwYAzq1IABYBJqORewTkfaPRqN6PRqO8qqIBLAJMRiMLktCFwiwymCqXWLmkeeLwEiGTFcTvJgQuBg15STBxWASYrGwYykus2sEiwHziKpuJzC/MwmgHxwTyHCJCLBbLeCsgvusRoy1sCeQxdKEEutPpxP79+/kky1PYEshDZKSdLpRAd7vdqKurAwCEw2EAXLEnn2ARyCNkADAWi6kVjWSEXYqCTMxh/zt/YBHII2T2HRGhqakJTqcTbrcbALBlyxZs374dv/rVr3Dq1CmYzWbVYuBluNyGRSCPsFgsGB4eRiwWg9FoxJo1a9DW1oahoSE88MAD6OjoAAAcPHgQJpMJQgg1IYfJXTgwmMPQvAYdQgjodDq0trbizjvvhNFohMvlAhGpDVG++c1vYmpqCkTECTl5AlsCOUw0GkVNTQ2mpqbUIqbxufjSLSgqKsL+/fvR19c3W26qsFB9Pufm5z5sCVyG+Zl02ZZZZ7FY0NPTozY1AS7m4q9fvx5CCBQXF8NqtaoxgSNHjqCpqQkAYLVaubtPHsAicAnkGnr8iZDqzLpERUeWOu/p6VnQpK+qqkJjYyMOHz6Mnp4eHDhwAPX19ejr64MQAn6/P76QLJOjsAjEIU862e6qubkZnZ2dWLdu3ZyuwakiXnSuVhDGxsbgdDoxNjYG4GKcIBQKIRwOY8OGDfjFL34xp1CH2+1WRaO2tjaj04mZxOGYQByyXVhnZycaGxuhKAqMRiP8fj+2bduGZ599do4/nUpfWY5tqVtn5wf35DLh1NQUpqamsGzZMlitVhw+fBh79+7FxMQE2tvbYTKZkjUVJtNYbG1yLW+Z1ndgfi+A0dFRWr58Oen1egoGgzQ6Okput5umpqZoYGCASktLKRwOf+J1tK5hHz+uYDBIXq+XZmZmEn7NUChE/f39tGrVKnK73XTjjTdSU1MTvfrqq7R69Wryer2kKIr6/oqi5Gx9/lydF5bQd4BFgIjC4TCVlZWpJ3YwGCS9Xk9Hjx4lr9dLq1evppKSEhoYGKCysrJLNgpJ1g8qFArR9ddfTyUlJQuKj2zScSWBiD+p5zf2KCsrI6/XSyUlJRQKhT7x3Fw9WXJ1XksRAXYHcNFkNpvNqv9bVFSEs2fP4tChQ+jq6oJOp4PNZoPJZJpzXDJy7OmC/y9fHwAKCwvR0NCAdevWIRKJzFm6k006gMvX2ot3KQDA6XQCAOx2uzqvyspKzgvINxarFlreMs0SkASDQbr++utpdHSUvF4v6fV6WrFiBfX3988xk8PhMJWUlHziyqzVVSUcDqstt/r7+ykYDM4Zw3xr5GosAUVRLuvazCdXr5i5Oi8swRLg1YE4xsfH8c477+Cll16C0WhEcXExmpubQUSYnJyEw+FQE2gCgQACgYDaFCORpbz458tbV1cXZmZmsGvXLtxxxx0AZpf8ZJZfW1ubunSp0+mwdevWKzbjjK/GI5cPM6VrT6KfIZMAi1ULLW+ZagnMzMyQx+Mhg8FAwWCQPB6PGg/o7++n/v5+Gh0dpVAoNCcmIGMKTz/99FW9b3xMQt6XsYjOzk5asWIFHT16VH3f+Ct6qkjGFTN+HqFQ6JLxiGTClgDHBFSICGNjY3A4HDAYDACA3t5eNDQ0wGAwwGQyoba2Fh999BGWLVs2Z6utjCm89957V/Xe8TGJaDQKv98Ps9kMAFAUBTqdDg8++CCefPJJEFHW19cjuljPwGaz8QaldLNYtdDylomWgLwCy0j5fEtALqtJP13r1YF4Hz0UCtHAwACVlJTQ6tWrqbGxkb7whS/QZz7zGdLr9bR69eqUtzLX8oopP+tQKKQugYZCobRYOmwJ8BKhSvyafCgUIq/XS6WlpeT1etUf55VM1kR+UPEiNDo6qroAcj2/oaGBdDodNTQ0UH9//5wlzVSg5ckSH8icL37hcJhCoVDK5sciwCKwIOFwmEpLSz9xtb3SFSqRH9T09DR5PB6anp4mr9dLBQUF5Ha7aXR0lPR6Pb366qvk8XjmnCzZaglIMZUCW1paSm63m/r7+1M+v/nzSke8JRmwCCTIQkk1i8nUS+REGRgYoMLCQjWBp6mpiQwGg5rAI7MF4zMbs1EEFEVRl19XrVpF/f395HK5aMWKFWqCllwWTYcIzE8cy1ZYBDQk/uS8EomcKPFiE2+JyL+ny1yWLHVulxJPaQX09/erArd8+XI1TbuxsZF0Oh2tWLEiJXGP+fNaiuhnMiwCGpIqSyCe+Uk9MmgmT/5scAcuJZ5SBGRC1tGjR2nVqlV09OhRcrvdZDAYVNcgFUI3f17z4z7z4xfZ4iqwCKSJZASZ4s3TdP4IE7UE5m/S8nq9VFhYSF6vV920Fb/yEi9+yZzvlURgYGCACgoKyOPxpMUCu1qWIgKcMZjhxG8Fzqb8gPlZjNFoFFu2bMHg4CAsFgvq6urw3HPPwW63A5jN1nz33XfVYxVFwcmTJ7Fly5akVzciulhHAgACgYCaA2Kz2bBz50709fVhZGREHX9OsVi10PLGlkD2kejc4pcC4339ePdALo+WlJSQ2+2mwsJC8ng8SbcE5ueIxLtbMj7T1NREOp0u6ePRCnDGIJNpyL0KwOzuRZPJNKc4irQcZAekDRs2QAgBh8ORNMuHiHDq1Cn84Ac/QGdnJ8rLy+H3+wHM9mHo7OwEgDkZnL29vdi0aVNuNWZZrFpoeWNLIPvQOugpYwOynsH09LT62Ov1qjkDyfS/w+EwrVq1Ss3O1Ov1FAqFaGZmhtxuNy1fvpwKCgpoYGBgjrWQa5YAi4CGsAgsnviNUjIPQp5kcpt2sk84RVHoyJEjc7Zqy2DkihUr6POf/zy5XK6sWxkgYneAyVCILrY1kwFPGYwzGo3qJiqj0Qgg+U1R5ZZqu90Ov9+P48ePq+OU/28wGEBEGBwchM1my8leDLw6wKSM+OrJcqVj/fr1OH78ONavX4+qqirodLo5fRGSibwSSp9/ZGQENTU1IJrtyFRYWIjW1lZ0d3dj27Zt6O7uzsleDGwJMGlFikE6iEajeOSRR/D4448jFouho6MD11xzDWKxGA4ePIgnnnhCLStnMBhgs9mwadOmnFsiZEuASRlVVVUZ1fLcYrHg/vvvR3NzMxRFwd69e3HNNdfg7NmzAGZXLNrb2zE0NIS6ujoUFBRkTZ7GUmBLgEkZ6bzqL4QQAjfccAOmpqbQ2tqKQCAAnU6H3t5e9PT0qIlMzc3NMBqNWL9+fZpHnBxYBJi8pry8HCMjI+rjvr4+eDwe1NXVqTUdZZfmXIVFIEeJj8TnmvmqJbJBKzD7mcnCq/Izk01bc/kz1CQmIISoFkLEhBCnhBB7tHhNJjFkjwFZy48r+V6ZhfZmZFocIxkkLAJCiAIAhwDcAeAmAN8VQtyU6OsyiRFfvNTn86VkI04ukk2btq4WLSyBfwBwioj+TESTAAYA3KnB6zIJIH+8sitxfKdhholHJGoiCiFsAKqJ6P4Lj+8F8DUiaph3XD2AegDQ6/U3+3y+hN43Ezl37hyuvfbatI6BiHD69GmsXbsWQgh1kwwwGwS72itaJswtGeTqvG6//fYgEX11UQcvNr/4UjcANgD/Gvf4XgAHL/cc3juQPBaqiKxF3bxMmFsyyNV5IcVFRf4DwA1xj7944W9MiqC4wJ+iKPjwww/VnHwAMJvNcLlc6nbY+OOZhcmnz0gLEXgVQIUQolQIUQzgbgDDGrwus0jiVwImJibwwQcfYHx8XK2WMzg4iLa2NoyNjQGYm8PPLEw+fUYJ5wkQ0bQQogHACIACAM8Q0R8THhmzaOJLkJnNZgghUFFRAavVivr6evT19XFgkLk0i/UbtLxxTCA5zMzMkNfrpWAwqBb6NBgMc0qZX22br3TPLVlcal7ZVj9gPuBCo/kFESEcDmPfvn3Ytm0bNm3aBJ/Ph4qKChw7dgx1dXUYGxtTXYZ8WPtOlPjPiHI8PsBpwzlANBpFbW0tFEXBjh07cMstt8DhcGBychLNzc0wm83qvnl2CZaOjLkMDQ3lZOYgWwI5gMViwfDwMJqamhAIBCCEwM6dO3H+/Hm0t7ejq6sLNpsNANQ0YmbxxMdcchEWgSxGmqnA7N73I0eO4IEHHoDD4cDBgwdx33334brrrlNXCABkbGWcTDa5c919Yncgi4k3U2UuQHl5OW644QacPXsWTz/9NL73ve/hkUcewdq1a1FXV5exV7RIJILa2loMDw9Dp9NlzO5HyoPdmCwCWUy8mRqJRNDU1KQGsoqKitDT04Py8nKsXLkS5eXlAJDxPu34+LhazSfdYyUi+Hw+OJ3OjBhPsmARyGLmV+opKipCd3c3jEYjJiYmYDQasXnzZkxOTuLxxx/HyMhIRlTHkVdXs9msuiZmsxnd3d0oLy/Ho48+ipmZGYTD4bSa4ZFIBM3Nzejp6ZljPeWadcAxgRyhqqoKL774Iu6++27odDo4HA4AwPDwMHbv3o2ioqI0j/Ai0o0ZHBxEbW0tqqur4fP50NTUhO985zvYtWsX7rjjDtTW1iISiaQsVhAflyAixGIxAEBFRQV8Pp+aii37Kvp8voyMYSwVtgRyhIXq98m6+k8//TR6enoAzP7Q03X1kgFKq9UKl8sFm80GRVGwa9cu1YXZt28f3n77bWzYsAGnT58GEcFms6XEHJfi5Pf7MT4+DqfTiZ6eHrz00kv48Y9/DEVR1OrDLpcLLS0tOVF7kEVgEWSb+Ser4UgT9tixYyk9mSTzP7fBwUFs27YNZ86cwZEjR2A0GiGEQFFRESorK3H8+HGMjIygo6MDf/3rXxEIBDA0NJSyYKaMsRAR2tra4Ha7YTQa0dbWhh07dkBRFNx7772qRTA5OZkTlgCnDS+CxW7FzbTU2vjU1/lpsEtNi72auc3/3GQq89TUFA0MDFAwGKTS0lLyeDxqWnNpaSk1Njaqrcni052TwULziv9sZmZmyOPxUGlpKfX395Ner6ejR4/S9ddfT3q9noLBYEamF4N7EWqHoihqf7orfdGZJgKXE6+l1hi4mrldSmjkye71etUehPIzDoVCFAwG1ZoIWtRCuBzz5yW/72AwSMFgkLxeL61Zs4Y8Ho8qXq+++irp9Xrq7++nYDBIq1evJq/Xm1FCwCKgAfIHvJQvOdNE4HJX+1RYApd6T2kRyJM/HA5TKBSi0tJStUNx/EaoVFoC4XBY7VCs1+uppKSEmpqa1LGVlZWpFkEwGKRQKKQel8wOyktlKSLAMYFLIJNXurq6MDU1BYfDASGEWo8+G7hcs490NAKRn2kgEFAbe6xbtw5+vx/l5eWor6/Hnj17cObMGRw4cABCiJRX+o1fqpyYmAAA7NmzB3fddResVitMJhMURUFxcbH6nBMnTkAIkZFJWIuBRWABiGaXh4gIJpMJJ06cwMmTJ7Fnz2w19WwSgkyCiNRgmiyA+tZbb6G9vR2f+9znUFBQgJaWFvT19aGnpwcmkymlJxbRbPfh9vZ2uFwudHR0YHBwEHfddRcOHz6Mr3/966isrITFYlFXW1IdbE0KizUZtLxlqjsgzdVQKERlZWU0MDCg/q2srIw8Hg8ZDAbyer0LxggyzR3QEi3mFl//UH6uo6OjpNfrqbOzk1atWkX9/f2Lir9oRfy8ZKwivv5CKBQig8FAHo+HRkdH1YBlWVkZBYNB9dhMAxwTuDrkyS791JmZGTVIFAqFaHp6mjweD5WUlMwJXMkfLIvA5VlotWJqakoNunm93k8USU028fNaKE4SH8eQ43O5XOpvIplBy0RgEbhK4r/wUCikfuky6BMKhWj16tXq1Wr+j4BFYPHEW1eFhYU0MDCwYKXkZLPYecnvvrGxkQoKClRrMJVWy1JYighwTCAOGSwLh8Oorq7G5OQkDh06hC996UuwWCwIhUL4+OOPQUSoqqqCoiior6/HunXr0j30rEMm5qxbtw4GgwF2u10NBGZCgE1mN9rtduh0s9n1Qgh8//vfx6233oqKigo1HpDt8SEWgUswNTWF999/H2fOnFG3tk5MTOD999/Hrl27cNNNNyEWi6GjowMGgwFbt25N95CzivjVifjPLlMCbDK7EZgdX3wW5s033wwiytht2UuFRWAes5YU8NJLL2FkZAQejwfnzp1TlwcBwGg0wmw2Q1EUPPvss+pyF5M7yO9U/jt/STUdS6zJgncRziMajcJms+HUqVMoLS3Fhx9+iIaGBthsNkSjUTXfPRKJYPPmzQCQ9eYg80l0Oh22bt2qugK5TO7PcIlYLBa4XC60tbXBaDRi7969uOWWW9RintXV1aitrUUsFsNHH32E5ubmjCzXxTCLhd2BeUiz32QygYjw+OOP491338Vzzz2HQCAAotmtuIqioKCg4BMFJxgm22ARWADp7ymKgieffFIua6o+oKzdX1xcjMrKyqS5A0TZtYWZyU7YHbgMY2Nj6OjoQEFBAex2O6LRqFp4IhV57fE9BhkmWbAlcBnkWrbZbJ6Txy6XhpJ9dc71evdMZsAicBnil4Hir/ipWBpiV4BJFewOZBhEs8Uuw+Fw3rTGZtILi0CGIeMA4+PjAC42Gw2Hw1AUJemVd6UIybzyTO0KxGgHi0AaWegkk3EAu92Onp4eEBGqq6tRU1OjVupNZqBQilAkEsHzzz+PmpoahMNhnDp1isUgR2ERSCPx0X9FUfD888+rm5Nee+01OJ1OnDx5EoqioLu7G3a7XQ0Uan2Vlq9nNpvh9/sRi8XQ0tKCyclJnDx5Eg8//LDazFRaJiwKOcJitxtqecvUrcSJstTttvH71wcGBqiwsFCtvCu3Mq9evZpWrFih1jiU9fmCwaCme9njaykMDAzMKQS6Zs0a2rJlC01PT6s1+DKtpt7Vkqvbv8H1BNJDIj+o+NLWso6BLFwhK9mEw2Hq7+8nnU5HnZ2dmuxll0I0PT1NAwMDNDo6OqewqqIo5PF4SKfTqZWWMnkf/VJhEeB6AmmH6GJ78dbWVmzatAkzMzOYmpoCEUGn081JY5a3J554AnfccUfCy4jSJXG5XHA6nfjhD3+oNjUNh8MQQsDhcODcuXOoqKgAgKzvuMPMhWMCaUZuTKqtrcXY2BgsFgvGx8dBRDh58iSsVivGxsYAADU1NQCAo0eP4sSJEwCQcKAwPhDpcrnQ19eHhoYGNDU1qZulBgcHIYRQsyaZHGOxJoOWN3YHLhJvXkt/f+XKlfTZz36WVq5cqRay9Hq9tGLFijm++FJ7B1xpHLLZhiyo2d/fr8Ylli9fTo2NjTQ6OpoTboCE3QGOCWhKIj8oGRw0GAzU2NhIq1atUttwxbe/knECrU/EUCg0p7WWFCbZCOT++++ngoIC0uv1OREQlLAIELsDmUI0GkVbWxu2b9+OY8eOqXX3o9Eodu3ahXfeeQc6nU7tipsMs1xRFDUWMT4+ji1btqC7uxttbW1YuXIlnn32WZw4cYL3MuQYLAIZgsViwbFjx7Bx40ZMTk4CmG1sEYvF8Jvf/AZHjx6F3W4HEcHv92t+IlZVVeHAgQMoLi5W23I/8MAD6Ovrw/bt29Hb2wuTyaRW2iHibMJcgUUgQ5CblXQ6HYqLi2EymdSIfUFBAe6++2689tprsNlsEEJovqlICIHKykoUFRXBaDTC5XKhpaUFbrcbGzduBACMj4+rgUje5pxDLNZv0PLGMYFLo2U78aUiA5MyMChbcI+OjtLTTz+t5hLMzMwkfSypgmMCHBPIOKRFIK/28v78/0sGsj+gbMR59uxZvPPOO5iYmMDatWuUmkm/AAAJBUlEQVTh9/vhdDoxNjaW9LEwqYOThRiV+IYgQghYrVaUlZXBbrfjmWeegcvlgtvt5sBgjsGWAKMir+5y89Lrr7+ult1eu3Ytjh07lvKOzMQByKSTkAgIIexCiD8KIRQhxFe1GhSTXhYqa5Yu858DkMknUUvgdQBbAPxOg7EwGUIm+fsWiwV+v/9idtsF5lsIbDFcPQmJABG9QUQxrQbDMPORAdL5CVLSQvD5fCCiORYDC8LSEFp8UEKIVwA4iGj0MsfUA6gHAL1ef7PP50v4fTONc+fO4dprr033MJJCOudGRDh9+jTWrl2r7nA8deoU3n77bfzsZz+D2+1GeXm5eszp06fx8MMP46c//SnKy8sv+9q5+p3dfvvtQSJanIt+pTVEAC9j1uyff7sz7phXAHx1seuSnCeQfWTK3OQei7KyMrXuQvzmq5mZmTn3r0SmzEtroGU9ASLacDVKxDBaQ0Tw+XzYs2cPPB6Pmj1pNpvR1dWFjo4OAIDJZILT6YTJZMqZzsHJhPMEmKwhfpNVRUUFotEompub8fvf/x4vvPACHn30UTV20NnZCbPZnO4hZwWJLhH+kxDiLwC+DuBFIcSINsNimIsQkVpy3eVy4eDBg6itrcXIyAjOnTuHw4cPY8eOHWhtbYXf70dtbS0cDodajIW5PAlZAkT0SwC/1GgsDLMgsvoSAHR1dWHfvn04c+YM9u/fj+LiYuzZswcbN25EJBJBS0sLuru7UVlZyZmNi4TdASbjsVgsCAQCahn0qakpdelw9+7dMBgMsNlscLlcAIDKykqug7gEWASYjEcIgfXr16OqqgomkwmKoiAWi82xBnp6emA0GhEIBDgYuERYBJisY2JiAg899BAmJyeh0+lUAdi8eTMCgUBGZDpmEywCTNYgYwNEpJ74QghYLBbI5LNYLJYxKc/ZAosAkzXI2IC8PzY2BrPZDJ/Ph5aWFjQ2NsLpdKKyspJdgiXAIsBkDTI2AACRSARWqxX19fV48sknMT09jW9961uorq7mVYElwiLAZCUWiwWdnZ1oaWlBT0+PevVnN2DpsAgwWYkQAiaTSS2QykuCVw+LAHNVEF3soZiuK3BVVRWOHz/O5n+CcHkxZlHIk54ubD2P76GYrqo/mVT8JJthS4BZFKdPn4bb7Ybf7wcwKwrDw8PqfSLikzFLYUuAWRB55VcUBZFIBGVlZRgaGgIw2x25uroaQoiktkVjUgOLALMgslzX4OAgrFYr/vznP8NsNuPNN9/Ezp07UVxcDGDhoqRMdsHuADMHulCvz2w2qwU+/X4/3nvvPXR1daG9vR3Lly+Hw+FQT3y51Zd99OyERYCZg7QAhoaG1AYkbrcbH3zwAXp7e7Fjxw587WtfQ2trK2688UYIIdDS0oLp6WkUFRXh+PHjnK2XZbAIMHOYb9673W44nU786Ec/wvbt23HgwAHccsst+Pjjj/Hggw+iqKgI3d3dAGbLerFbkH1wTICZg9yQI0t3G41G+P1+CCHw1FNP4eOPPwYAfOpTn0JdXR1eeOEFmEwmtLa2JqVbMpN8WASYTxAfFLTZbBgfH8fatWvR09ODZcuWwWQyoa6uDr29vXj55ZdBRJicnOQ6/1kKuwPMJ5AugSzUuWfPHtx77734yU9+ApPJhFgshhdeeAE7duxAb28vbrzxxjSPmEkEFgHmE8gov3QH3G43du/ejS9/+cswGo1ob2/HY489BpvNhm984xsoLy9HcXExuwJZCrsDzCWJRCKora2F0WjED3/4QzidTgDA0NAQ6urq1O7FOp2OVwWyGLYEmCsihMDtt9+OLVu2wGw2IxqNIhwOg4jQ2dkJi8UCnY6vJ9kKiwBzSeJ36b3yyitqByCHw4HJyUkAQHFxMVfyyXJYBJhLImMDAHDq1Cl0dHSgsLAQPT09qKiowPj4OOcG5ABswzGLRgrA1q1bUVBQgI6ODuh0Og4IZjlsCTCLory8HC+++CIsFouaUMQbh3IDFgFmUcS7Bgs9ZrIXdgcYJs9hEWCYPIdFgGHyHBYBhslzWAQYJs9hEWCYPIdFgGHyHBYBhslzWAQYJs9hEWCYPIdFgGHyHBYBhslzWAQYJs9hEWCYPIdFgGHyHBYBhslzWAQYJs9hEWCYPIdFgGHynIREQAjRJYR4UwgxJoT4pRDi81oNjGGY1JCoJfASgP9BRGYA4wDaEh8SwzCpJCERIKKTRDR94eH/BfDFxIfEMEwq0bLk+D8DeP5S/ymEqAdQf+HheSHE6xq+d6awAsDf0j2IJJGrc8vVeZkWe6AgossfIMTLAFYu8F/tRPTChWPaAXwVwBa60gvOHj9KRF9d7CCzhVydF5C7c+N5LcISIKINV3iz/wWgBsA3FyMADMNkFgm5A0KIagAPAfifRPShNkNiGCaVJLo6cBDAZwC8JISICCF6F/m8Iwm+b6aSq/MCcndueT+vK8YEGIbJbThjkGHyHBYBhslz0iYCuZpyLISwCyH+KIRQhBBZv/QkhKgWQsSEEKeEEHvSPR6tEEI8I4T4r1zLVxFC3CCE+K0Q4k8Xfoe7rvScdFoCuZpy/DqALQB+l+6BJIoQogDAIQB3ALgJwHeFEDeld1Sa8W8AqtM9iCQwDaCFiG4CcAuAnVf6ztImArmackxEbxBRLN3j0Ih/AHCKiP5MRJMABgDcmeYxaQIR/Q7Au+keh9YQ0X8SUejC/f8G8AaAkss9J1NiAv8M4DfpHgTzCUoA/L+4x3/BFX5QTOYghDAAWA/gD5c7Tsu9AwsNYrEpx9MAjiZzLFqymHkxTDoRQlwLYAjAg0T0weWOTaoI5GrK8ZXmlUP8B4Ab4h5/8cLfmAxGCFGEWQE4SkTHrnR8OlcHZMrxZk45zlheBVAhhCgVQhQDuBvAcJrHxFwGIYQA8HMAbxDR/sU8J50xgatNOc5ohBD/JIT4C4CvA3hRCDGS7jFdLRcCtw0ARjAbYPIR0R/TOyptEEJ4Afw7AJMQ4i9CiH9J95g04lYA9wL4xwvnVUQI8e3LPYHThhkmz8mU1QGGYdIEiwDD5DksAgyT57AIMEyewyLAMHkOiwDD5DksAgyT5/x/LSaVphI8a8cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_noisy_tSNE_constellation_diagram(all_one_hot_messages16, transmitter7_4, 50)\n",
    "# plt.savefig(\"./figures/leaky_relu_7_4_constellation_diagram_with_noise.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_one_hot_messages4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9790156e-01, 7.8870235e-12, 1.3947172e-09, 2.0983834e-03],\n",
       "       [5.3004594e-11, 9.9997914e-01, 1.3348944e-05, 7.4885997e-06],\n",
       "       [3.5164376e-08, 5.1944462e-06, 9.9999475e-01, 4.0935007e-13],\n",
       "       [7.0919750e-09, 6.8828302e-07, 1.2722455e-14, 9.9999928e-01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder2_2.predict(all_one_hot_messages4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder_symbs2_2.predict(all_one_hot_messages4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_one_hot_messages16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.92787510e-01, 1.40685699e-20, 5.06430686e-01, 3.05124101e-25,\n",
       "        1.66253777e-30, 1.44952301e-28, 2.49058974e-16, 6.24700648e-15,\n",
       "        6.87050230e-30, 1.63454262e-21, 4.19203396e-04, 8.92331187e-09,\n",
       "        1.96602035e-25, 6.59680464e-27, 5.58420143e-10, 3.62585561e-04],\n",
       "       [5.13307366e-16, 8.91508460e-01, 2.38053710e-22, 8.69752467e-28,\n",
       "        4.35590335e-15, 7.47841540e-08, 1.28394918e-29, 1.03250392e-01,\n",
       "        4.15661166e-20, 1.94905525e-29, 5.72838499e-10, 1.75538353e-05,\n",
       "        5.22350799e-03, 4.55332437e-25, 3.50579108e-27, 1.19300051e-25],\n",
       "       [2.61616372e-02, 5.98945164e-21, 9.17930007e-01, 3.38189821e-19,\n",
       "        4.74490474e-27, 1.48997321e-26, 7.67533398e-11, 5.68717720e-16,\n",
       "        2.91748348e-25, 2.06862662e-15, 7.88580564e-06, 2.20124086e-10,\n",
       "        1.89318610e-24, 9.10655374e-22, 7.97923258e-06, 5.58925085e-02],\n",
       "       [6.96431256e-24, 9.32148496e-24, 6.03967206e-18, 6.66092753e-01,\n",
       "        5.29381790e-12, 4.76329296e-16, 2.53212493e-04, 4.47703062e-27,\n",
       "        3.70918855e-07, 3.32047224e-01, 7.50497761e-25, 8.89097932e-27,\n",
       "        6.22986736e-19, 1.60645193e-03, 6.66264484e-08, 2.99061644e-12],\n",
       "       [2.59357908e-28, 9.43642820e-12, 9.24395721e-28, 2.50490615e-08,\n",
       "        6.87620044e-01, 3.67837842e-03, 5.63146215e-19, 2.05775223e-17,\n",
       "        3.08198571e-01, 1.72919269e-13, 7.08446633e-24, 5.79921303e-21,\n",
       "        3.89869996e-07, 5.02517505e-04, 1.01432407e-21, 7.50381138e-24],\n",
       "       [8.68888013e-28, 2.77319418e-06, 3.19837230e-30, 7.45475814e-15,\n",
       "        6.58711866e-02, 9.23775613e-01, 8.52909607e-25, 7.63876289e-12,\n",
       "        1.94464592e-05, 8.04651638e-20, 1.54107343e-21, 3.33325861e-17,\n",
       "        1.03310160e-02, 3.87521848e-10, 6.99013654e-27, 1.03087748e-28],\n",
       "       [3.23602672e-11, 1.16183457e-27, 5.38003769e-06, 4.44026010e-10,\n",
       "        1.73078042e-25, 1.29073225e-27, 5.01513928e-02, 2.49629275e-24,\n",
       "        4.71149354e-21, 2.89934833e-05, 1.16209448e-15, 2.84035372e-20,\n",
       "        1.63090425e-27, 7.66326854e-16, 9.38404083e-01, 1.14101702e-02],\n",
       "       [5.64456260e-11, 2.91464508e-01, 1.95291506e-16, 5.97909087e-25,\n",
       "        9.03963556e-15, 1.58784541e-08, 9.58921074e-25, 7.02180684e-01,\n",
       "        7.65980466e-19, 9.74675612e-26, 2.62322033e-06, 5.85880363e-03,\n",
       "        4.93309635e-04, 1.08747189e-22, 8.61466704e-22, 1.15359797e-19],\n",
       "       [5.29367364e-28, 3.36261964e-12, 3.70149329e-27, 1.13672471e-07,\n",
       "        4.78967637e-01, 1.53964106e-03, 4.04162474e-18, 9.69439134e-18,\n",
       "        5.17934918e-01, 1.00442105e-12, 7.45828937e-24, 4.63419820e-21,\n",
       "        1.33911314e-07, 1.55753444e-03, 6.93172264e-21, 4.58044796e-23],\n",
       "       [2.67633850e-16, 1.09451636e-21, 2.87802143e-11, 8.03532358e-03,\n",
       "        4.39062882e-14, 5.71438403e-17, 2.76273280e-01, 2.46993788e-22,\n",
       "        3.35620559e-10, 7.14097381e-01, 3.30388799e-18, 1.02960076e-20,\n",
       "        1.91324539e-18, 2.00078421e-06, 1.59133761e-03, 6.37030553e-07],\n",
       "       [6.03128448e-02, 5.70440362e-10, 2.04801454e-05, 5.11545713e-27,\n",
       "        1.00866027e-23, 3.01152783e-19, 1.67601332e-20, 8.31746547e-06,\n",
       "        6.28890760e-26, 7.40503288e-25, 9.09165561e-01, 3.04927994e-02,\n",
       "        1.31949488e-14, 1.34970881e-26, 4.97655743e-15, 1.11480347e-09],\n",
       "       [2.88494306e-07, 1.69904470e-05, 1.96583404e-14, 1.99368562e-37,\n",
       "        3.38363705e-27, 1.33011149e-18, 3.41650762e-32, 6.47148266e-02,\n",
       "        1.33390350e-31, 1.69652447e-36, 8.25970527e-03, 9.27008152e-01,\n",
       "        1.86404139e-11, 1.98959995e-35, 3.60528755e-26, 2.02867112e-20],\n",
       "       [1.10220029e-21, 1.97201055e-02, 1.23461046e-25, 4.50545096e-18,\n",
       "        1.60805284e-05, 9.43478346e-02, 2.40746898e-24, 3.64949665e-06,\n",
       "        8.62767524e-10, 2.81188417e-21, 1.40131670e-15, 4.67658794e-11,\n",
       "        8.85912418e-01, 1.82571836e-14, 3.49453006e-25, 4.71607242e-26],\n",
       "       [1.04449122e-22, 4.17109645e-11, 2.29910590e-21, 4.76137175e-05,\n",
       "        1.43681407e-01, 5.85002592e-04, 4.79097285e-13, 1.85208857e-15,\n",
       "        8.19608271e-01, 7.70444153e-09, 8.02715260e-20, 8.30493884e-18,\n",
       "        2.71734706e-07, 3.60773504e-02, 1.91531005e-15, 1.37251085e-17],\n",
       "       [3.81931500e-08, 5.64817845e-23, 4.73491877e-04, 9.76527526e-10,\n",
       "        4.15561474e-22, 9.17170788e-24, 1.42739592e-02, 9.88776355e-20,\n",
       "        1.58591224e-18, 1.13681353e-05, 6.49677404e-12, 7.11332287e-16,\n",
       "        2.06665543e-23, 2.92198991e-14, 8.50519061e-01, 1.34722158e-01],\n",
       "       [2.46130658e-05, 5.61440905e-24, 5.17775230e-02, 1.15673665e-14,\n",
       "        7.02711933e-26, 4.98154568e-27, 2.87842226e-06, 2.05159468e-19,\n",
       "        1.25107849e-22, 1.96774694e-10, 1.50255886e-09, 3.70361226e-14,\n",
       "        4.75101557e-26, 1.36307526e-18, 1.87562965e-02, 9.29438710e-01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder2_4.predict(all_one_hot_messages16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder_symbs2_4.predict(all_one_hot_messages16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess performance across a range of SNRs\n",
    "The O'Shea paper defined the variance of the noise to be $ = (2RE_b/N_0)^{1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f5affd482341e08c0b3c2bf75b728f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='1st loop', max=25, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 2131.674866437912/25\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "Eb_N0_dbs = np.arange(-4,8.5,0.5)\n",
    "bler = np.empty(Eb_N0_dbs.size)\n",
    "for i, ratio_db in enumerate(tqdm_notebook(Eb_N0_dbs, desc=\"1st loop\")):\n",
    "    print(f\"ratio_db = {ratio_db}, i = {i}/{Eb_N0_dbs.size}\", end=\"\\r\")\n",
    "    ## Get noise std_dev \n",
    "    noise_std = get_noise_sigma(ratio_db, R)\n",
    "    ## Make new model with loaded weights\n",
    "    autoencoder_loaded, transmitter_loaded, \\\n",
    "        reciever_loaded, autoencoder_symbs_loaded, \\\n",
    "        k_l, Nc_l, Nr_l \\\n",
    "        = make_model(M, R, noise_std, \"relu\")\n",
    "    autoencoder_loaded.load_weights('second_qpsk_model.h5', by_name=True)\n",
    "        \n",
    "    ## Check Accuracy on test set\n",
    "    pred_symbs = autoencoder_symbs_loaded.predict(test_data)\n",
    "    bler[i] = get_block_error_rate(test_data, pred_symbs)\n",
    "print(f\"Took {time() - t0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get performance of (2,2) and (8,8) leaky relu models across range fo SNRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='1st loop', max=25, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -4.0, i = 1/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -3.5, i = 2/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -3.0, i = 3/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -2.5, i = 4/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -2.0, i = 5/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -1.5, i = 6/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -1.0, i = 7/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -0.5, i = 8/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 0.0, i = 9/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 0.5, i = 10/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 1.0, i = 11/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 1.5, i = 12/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 2.0, i = 13/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 2.5, i = 14/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 3.0, i = 15/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 3.5, i = 16/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 4.0, i = 17/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 4.5, i = 18/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 5.0, i = 19/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 5.5, i = 20/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 6.0, i = 21/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 6.5, i = 22/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 7.0, i = 23/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 7.5, i = 24/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 8.0, i = 25/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Took 53.29376840591431\n"
     ]
    }
   ],
   "source": [
    "# (2,2)\n",
    "t0 = time()\n",
    "Eb_N0_dbs = np.arange(-4,8.5,0.5)\n",
    "bler = np.empty(Eb_N0_dbs.size)\n",
    "for i, ratio_db in enumerate(tqdm_notebook(Eb_N0_dbs, desc=\"1st loop\")):\n",
    "    print(f\"ratio_db = {ratio_db}, i = {i+1}/{Eb_N0_dbs.size}\", end=\"\\r\")\n",
    "    bler[i] = get_noise_bler(2**2, 2, ratio_db, \\\n",
    "                             './models/autoencoder_2_2_leaky_relu.h5', \\\n",
    "                             test_data4, act_f)\n",
    "print(f\"Took {time() - t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0876c65c984d778dd12f47d40d4cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='1st loop', max=25, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -4.0, i = 1/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -3.5, i = 2/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -3.0, i = 3/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -2.5, i = 4/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -2.0, i = 5/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -1.5, i = 6/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -1.0, i = 7/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -0.5, i = 8/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 0.0, i = 9/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 0.5, i = 10/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 1.0, i = 11/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 1.5, i = 12/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 2.0, i = 13/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 2.5, i = 14/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 3.0, i = 15/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 3.5, i = 16/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 4.0, i = 17/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 4.5, i = 18/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 5.0, i = 19/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 5.5, i = 20/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 6.0, i = 21/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 6.5, i = 22/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 7.0, i = 23/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 7.5, i = 24/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 8.0, i = 25/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Took 39.00256085395813\n"
     ]
    }
   ],
   "source": [
    "# (8,8)\n",
    "t0 = time()\n",
    "Eb_N0_dbs = np.arange(-4,8.5,0.5)\n",
    "bler = np.empty(Eb_N0_dbs.size)\n",
    "for i, ratio_db in enumerate(tqdm_notebook(Eb_N0_dbs, desc=\"1st loop\")):\n",
    "    print(f\"ratio_db = {ratio_db}, i = {i+1}/{Eb_N0_dbs.size}\", end=\"\\r\")\n",
    "    bler[i] = get_noise_bler(2**8, 2, ratio_db, \\\n",
    "                             './models/autoencoder_8_8_leaky_relu0.0750.h5', \\\n",
    "                             test_data256, act_f)\n",
    "print(f\"Took {time() - t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342ce8b31fac44bda5d2c153a427cfa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='1st loop', max=25, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -4.0, i = 1/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -3.5, i = 2/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -3.0, i = 3/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -2.5, i = 4/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -2.0, i = 5/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -1.5, i = 6/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -1.0, i = 7/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -0.5, i = 8/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 0.0, i = 9/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 0.5, i = 10/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 1.0, i = 11/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 1.5, i = 12/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 2.0, i = 13/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 2.5, i = 14/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 3.0, i = 15/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 3.5, i = 16/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 4.0, i = 17/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 4.5, i = 18/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 5.0, i = 19/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 5.5, i = 20/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 6.0, i = 21/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 6.5, i = 22/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 7.0, i = 23/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 7.5, i = 24/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 8.0, i = 25/25\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Took 729.8102967739105\n"
     ]
    }
   ],
   "source": [
    "# (7,4)\n",
    "t0 = time()\n",
    "Eb_N0_dbs = np.arange(-4,8.5,0.5)\n",
    "bler = np.empty(Eb_N0_dbs.size)\n",
    "for i, ratio_db in enumerate(tqdm_notebook(Eb_N0_dbs, desc=\"1st loop\")):\n",
    "    print(f\"ratio_db = {ratio_db}, i = {i+1}/{Eb_N0_dbs.size}\", end=\"\\r\")\n",
    "    bler[i] = get_non_complex_noise_bler(4, 7, ratio_db, \\\n",
    "                                         './models/autoencoder_7_4_leaky_relu8.8851e-4.h5', \\\n",
    "                                         test_data16, act_f)\n",
    "print(f\"Took {time() - t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Save\n",
    "# leaky_relu_autoencoder_2_2_bler = bler\n",
    "# np.save('./key_results/leaky_relu_autoencoder_2_2_bler.npy', leaky_relu_autoencoder_2_2_bler)\n",
    "\n",
    "# leaky_relu_autoencoder_8_8_bler = bler\n",
    "# np.save('./key_results/leaky_relu_autoencoder_8_8_bler.npy', leaky_relu_autoencoder_8_8_bler)\n",
    "\n",
    "# leaky_relu_autoencoder_7_4_bler = bler\n",
    "# np.save('./key_results/leaky_relu_autoencoder_7_4_bler.npy', leaky_relu_autoencoder_7_4_bler)\n",
    "\n",
    "## Load\n",
    "leaky_relu_autoencoder_2_2_bler = np.load('./key_results/leaky_relu_autoencoder_2_2_bler.npy')\n",
    "leaky_relu_autoencoder_8_8_bler = np.load('./key_results/leaky_relu_autoencoder_8_8_bler.npy')\n",
    "leaky_relu_autoencoder_7_4_bler = np.load('./key_results/leaky_relu_autoencoder_7_4_bler.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Save\n",
    "# autoencoder_2_2_bler = bler\n",
    "# np.save('./key_results/autoencoder_2_2_bler.npy', autoencoder_2_2_bler)\n",
    "## Load\n",
    "Eb_N0_dbs = np.arange(-4,8.5,0.5)\n",
    "autoencoder_2_2_bler = np.load('./key_results/autoencoder_2_2_bler.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogy(Eb_N0_dbs, autoencoder_2_2_bler, 'r.')\n",
    "plt.semilogy(Eb_N0_dbs, leaky_relu_autoencoder_2_2_bler, 'b.')\n",
    "plt.semilogy(Eb_N0_dbs, leaky_relu_autoencoder_8_8_bler, 'g.')\n",
    "plt.semilogy(Eb_N0_dbs, leaky_relu_autoencoder_7_4_bler, 'y.')\n",
    "plt.title(r'Autoencoder (2,2) BLER against $\\dfrac{E_b}{N_0}$')\n",
    "plt.xlabel(r'$\\dfrac{E_b}{N_0}$ [db]')\n",
    "plt.ylabel(\"Block error rate (BLER)\")\n",
    "plt.legend([\"Relu (2,2)\", \"Leaky-Relu (2,2)\", \"Leaky-Relu (8,8)\", \"Leaky-Relu (7,4)\"])\n",
    "plt.show()\n",
    "# plt.savefig(\"./figures/autoencoder_2_2_bler_EbNo_leaky_vs_relu.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Uncoded BPSK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " would be good to use this in the presentation\n",
    " https://en.wikipedia.org/wiki/Phase-shift_keying\n",
    " \n",
    " ##### Uncoded BPSK (2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "Eb_N0_dbs = np.arange(-4,8.5,0.5)\n",
    "bler_bpsk = np.empty(Eb_N0_dbs.size)\n",
    "for i, ratio_db in enumerate(tqdm_notebook(Eb_N0_dbs, desc=\"1st loop\")):\n",
    "#     print(f\"ratio_db = {ratio_db}, i = {i}/{Eb_N0_dbs.size}\")\n",
    "    ## Get noise std_dev \n",
    "    noise_std = get_noise_sigma(ratio_db, R)\n",
    "    # Encode, add noise and decode data, then get BLER\n",
    "    bler_bpsk[i] = bpsk_get_bler(noise_std, test_data4)\n",
    "print(f\"Took {time() - t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "Eb_N0_dbs = np.arange(-4,8.5,0.5)\n",
    "bler_bpsk = np.empty(Eb_N0_dbs.size)\n",
    "for i, ratio_db in enumerate(tqdm_notebook(Eb_N0_dbs, desc=\"1st loop\")):\n",
    "#     print(f\"ratio_db = {ratio_db}, i = {i}/{Eb_N0_dbs.size}\")\n",
    "    ## Get noise std_dev \n",
    "    noise_std = get_noise_sigma(ratio_db, R)\n",
    "    # Encode, add noise and decode data, then get BLER\n",
    "    bler_bpsk[i] = bpsk_get_bler(noise_std, test_data16)\n",
    "print(f\"Took {time() - t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "Eb_N0_dbs = np.arange(-4,8.5,0.5)\n",
    "bler_bpsk = np.empty(Eb_N0_dbs.size)\n",
    "for i, ratio_db in enumerate(tqdm_notebook(Eb_N0_dbs, desc=\"1st loop\")):\n",
    "#     print(f\"ratio_db = {ratio_db}, i = {i}/{Eb_N0_dbs.size}\")\n",
    "    ## Get noise std_dev \n",
    "    noise_std = get_noise_sigma(ratio_db, R)\n",
    "    # Encode, add noise and decode data, then get BLER\n",
    "    bler_bpsk[i] = bpsk_get_bler(noise_std, test_data256)\n",
    "print(f\"Took {time() - t0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uncoded BPSK (8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Save\n",
    "\n",
    "# bpsk_2_2_bler = bler_bpsk\n",
    "# np.save('./key_results/bpsk_2_2_bler.npy', bpsk_2_2_bler)\n",
    "\n",
    "# bpsk_4_4_bler = bler_bpsk\n",
    "# np.save('./key_results/bpsk_4_4_bler.npy', bpsk_4_4_bler_new)\n",
    "\n",
    "# bpsk_8_8_bler = bler_bpsk\n",
    "# np.save('./key_results/bpsk_8_8_bler.npy', bpsk_8_8_bler)\n",
    "\n",
    "# ## Load\n",
    "bpsk_2_2_bler = np.load('./key_results/bpsk_2_2_bler.npy')\n",
    "bpsk_4_4_bler = np.load('./key_results/bpsk_4_4_bler.npy')\n",
    "bpsk_8_8_bler = np.load('./key_results/bpsk_8_8_bler.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogy(Eb_N0_dbs, bpsk_2_2_bler, 'k--')\n",
    "plt.semilogy(Eb_N0_dbs, bpsk_4_4_bler, 'b-')\n",
    "plt.semilogy(Eb_N0_dbs, bpsk_8_8_bler, 'k-')\n",
    "plt.title(r'BPSK (4,4) BLER against $\\dfrac{E_b}{N_0}$')\n",
    "plt.xlabel(r'$\\dfrac{E_b}{N_0}$ [db]')\n",
    "plt.ylabel(\"Block error rate (BLER)\")\n",
    "plt.legend([\"Uncoded BPSK (2,2)\", \"Uncoded BPSK (4,4)\", \\\n",
    "            \"Uncoded BPSK (8,8)\"])\n",
    "plt.show()\n",
    "# plt.savefig(\"./figures/autoencoder_2_2_bler_EbNo.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Hamming Encoded BPSK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bler =  0.024232\n",
      "all_true =  False\n"
     ]
    }
   ],
   "source": [
    "std = 0.22\n",
    "# Encode the one_hot_encoded_vectors into bits\n",
    "test_data_enc = hamming_7_4_encode(test_data4, G)   \n",
    "# add noise\n",
    "noise = std * np.random.randn(test_data_enc.shape[0],\\\n",
    "                              test_data_enc.shape[1])\n",
    "r = test_data_enc + noise\n",
    "# Do error correction and decode message\n",
    "corrected_message = hamming_7_4_decode_and_correct(r, H,\\\n",
    "                                                   R)\n",
    "# Get Block error rate\n",
    "bler = get_block_error_rate(test_data4, corrected_message)\n",
    "all_true = (corrected_message == test_data4).all()\n",
    "print(\"bler = \", bler)\n",
    "print(\"all_true = \", all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0676f494da4ed792538680f0d4cd13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='1st loop', max=25, style=ProgressStyle(description_width='ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1624.589095592498825\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "# Eb_N0_dbs = np.arange(-4,-2,0.5)\n",
    "Eb_N0_dbs = np.arange(-4,8.5,0.5)\n",
    "bler_hamming_HD = np.empty(Eb_N0_dbs.size)\n",
    "for i, ratio_db in enumerate(tqdm_notebook(Eb_N0_dbs, desc=\"1st loop\")):\n",
    "    print(f\"ratio_db = {ratio_db}, i = {i}/{Eb_N0_dbs.size}\", end=\"\\r\")\n",
    "    ## Get noise std_dev \n",
    "    noise_std = get_noise_sigma(ratio_db, R)\n",
    "    ## Transmit Hamming encoded and BPSK\n",
    "    test_data_enc = hamming_7_4_encode(test_data4, G) \n",
    "    bpsk_encoded = bpsk_encode_vec(test_data_enc)\n",
    "    # Add AWGN noise\n",
    "    noise = noise_std * np.random.randn(bpsk_encoded.shape[0],\\\n",
    "                                        bpsk_encoded.shape[1])\n",
    "    received = bpsk_encoded + noise\n",
    "    # Decode using BPSK\n",
    "    bpsk_decoded = bpsk_decode_vec(received)\n",
    "\n",
    "    # Decode using BPSK\n",
    "    corrected_message = hamming_7_4_decode_and_correct(bpsk_decoded, H,\\\n",
    "                                                       R_ham)\n",
    "    # Check Accuracy on test set\n",
    "    bler_hamming_HD[i] = get_block_error_rate(test_data4, corrected_message)\n",
    "print(f\"Took {time() - t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Save\n",
    "# hamming_7_4_HD_bler = bler_hamming_HD\n",
    "# np.save('./key_results/hamming_7_4_HD_bler.npy', hamming_7_4_HD_bler)\n",
    "# ## Load\n",
    "hamming_7_4_HD_bler = np.load('./key_results/hamming_7_4_HD_bler.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogy(Eb_N0_dbs, hamming_7_4_HD_bler, 'b--')\n",
    "plt.title(r'Hamming encoded BPSK (4,4) BLER against $\\dfrac{E_b}{N_0}$')\n",
    "plt.xlabel(r'$\\dfrac{E_b}{N_0}$ [db]')\n",
    "plt.ylabel(\"Block error rate (BLER)\")\n",
    "plt.show()\n",
    "# plt.savefig(\"./figures/autoencoder_2_2_bler_EbNo.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamming Encoding with Hard Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamming Encoding with MLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Hamming, BPSK and autoencoder systems\n",
    "\n",
    "##### O'Shea Figure 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load three sets of results\n",
    "bpsk_4_4_bler = np.load('./key_results/bpsk_4_4_bler.npy')\n",
    "hamming_7_4_HD_bler = np.load('./key_results/hamming_7_4_HD_bler.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Uncoded BPSK (4,4)\n",
    "# Hamming (7,4) Hard Decision\n",
    "# Autoencoder (7,4)\n",
    "# Hamming (7,4) MLD\n",
    "\n",
    "plt.semilogy(Eb_N0_dbs, bpsk_4_4_bler, 'k-')\n",
    "plt.semilogy(Eb_N0_dbs, hamming_7_4_HD_bler, 'b--')\n",
    "plt.semilogy(Eb_N0_dbs, leaky_relu_autoencoder_7_4_bler, 'r.')\n",
    "plt.title(r'BLER against $\\dfrac{E_b}{N_0}$')\n",
    "plt.xlabel(r'$\\dfrac{E_b}{N_0}$ [db]')\n",
    "plt.ylabel(\"Block error rate (BLER)\")\n",
    "plt.legend(['Uncoded BPSK (4,4)', 'Hamming (7,4), Hard Decision', \\\n",
    "            'Autoencoder (7,4)'],\\\n",
    "           loc=\"lower left\")\n",
    "plt.show()\n",
    "# plt.savefig(\"./figures/bler_vs_eb_only_three.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### O'Shea Figure 3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load three sets of results\n",
    "bpsk_2_2_bler = np.load('./key_results/bpsk_2_2_bler.npy')\n",
    "bpsk_8_8_bler = np.load('./key_results/bpsk_8_8_bler.npy')\n",
    "leaky_relu_autoencoder_2_2_bler = np.load('./key_results/leaky_relu_autoencoder_2_2_bler.npy')\n",
    "leaky_relu_autoencoder_8_8_bler = np.load('./key_results/leaky_relu_autoencoder_8_8_bler.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/matplotlib/axes/_base.py:2575: UserWarning: aspect is not supported for Axes with xscale=linear, yscale=log\n",
      "  self.apply_aspect()\n",
      "/home/apsw/.local/lib/python3.6/site-packages/matplotlib/axes/_base.py:4371: UserWarning: aspect is not supported for Axes with xscale=linear, yscale=log\n",
      "  self.apply_aspect()\n",
      "/home/apsw/.local/lib/python3.6/site-packages/matplotlib/axes/_base.py:4371: UserWarning: aspect is not supported for Axes with xscale=linear, yscale=log\n",
      "  self.apply_aspect()\n",
      "/home/apsw/.local/lib/python3.6/site-packages/matplotlib/axes/_base.py:848: UserWarning: aspect is not supported for Axes with xscale=linear, yscale=log\n",
      "  self.apply_aspect()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(Eb_N0_dbs, bpsk_8_8_bler, 'k-')\n",
    "ax.semilogy(Eb_N0_dbs, leaky_relu_autoencoder_8_8_bler, \\\n",
    "            'ro-', markerfacecolor='none')\n",
    "ax.semilogy(Eb_N0_dbs, bpsk_2_2_bler, 'k--')\n",
    "ax.semilogy(Eb_N0_dbs, leaky_relu_autoencoder_2_2_bler, \\\n",
    "            'rs', markerfacecolor='none')\n",
    "ax.set_title(r'BLER against $\\dfrac{E_b}{N_0}$')\n",
    "ax.set_xlabel(r'$\\dfrac{E_b}{N_0}$ [db]')\n",
    "ax.set_ylabel(\"Block error rate (BLER)\")\n",
    "ax.grid(True)\n",
    "ax.set_aspect('equal', 'box')\n",
    "plt.legend(['Uncoded BPSK (8,8)', 'Autoencoder (8,8)',\\\n",
    "            'Uncoded BPSK (2,2)', 'Autoencoder (2,2)'], loc=\"lower left\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# ax.savefig(\"./figures/bler_vs_eb_only_three.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly at the moment I need to fix a few things:\n",
    "- Autoencoder (8,8) isn't doing well enough.\n",
    "- I need to have a bigger data set for test_data256 so I have a smoother line at error rates of 10^-5.\n",
    "- Need to work out why my BPSK is doing better than the one in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweeping across different activation functions\n",
    "\n",
    "- Tanh - ~linear for small inputs\n",
    "- Relu - captures non linearity well\n",
    "- Sigmoid, softmax, linear\n",
    "- Inception learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M = 4 # Number of one hot encoded messages\n",
    "# R = 2 # R = k/n\n",
    "\n",
    "# sigma = get_noise_sigma(7, R)\n",
    "# print(\"sigma = \",sigma)\n",
    "# all_one_hot_messagesTmp = np.diag(np.ones(M))\n",
    "# autoencoderTmp, transmitterTmp, recieverTmp, autoencoder_symbsTmp, kTmp, NcTmp, NrTmp = make_model(M, R, sigma, \"relu\")\n",
    "\n",
    "# # Fit the model\n",
    "# autoencoderTmp.fit(train_data4, train_data4,\n",
    "#                 epochs=2,\n",
    "#                 batch_size=1000*M,\n",
    "#                 shuffle=True,\n",
    "#                 validation_data=(valid_data4,\n",
    "#                                  valid_data4))\n",
    "# # Get the test BLER\n",
    "# pred_symbs_tmp = autoencoder_symbsTmp.predict(test_data4)\n",
    "# bler_tmp = get_block_error_rate(test_data4, pred_symbs_tmp)\n",
    "# print(\"bler_tmp = \", bler_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma =  0.22334179607548157\n",
      "\n",
      "\n",
      "act_f =  leaky_relu\n",
      "\n",
      "i =  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/5\n",
      "7200000/7200000 [==============================] - 23s 3us/step - loss: 0.3564 - val_loss: 0.0422\n",
      "Epoch 2/5\n",
      "7200000/7200000 [==============================] - 22s 3us/step - loss: 0.0208 - val_loss: 0.0114\n",
      "Epoch 3/5\n",
      "7200000/7200000 [==============================] - 21s 3us/step - loss: 0.0086 - val_loss: 0.0065\n",
      "Epoch 4/5\n",
      "7200000/7200000 [==============================] - 23s 3us/step - loss: 0.0057 - val_loss: 0.0052\n",
      "Epoch 5/5\n",
      "7200000/7200000 [==============================] - 22s 3us/step - loss: 0.0049 - val_loss: 0.0048\n",
      "\n",
      "i =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/5\n",
      "7200000/7200000 [==============================] - 23s 3us/step - loss: 0.4023 - val_loss: 0.0340\n",
      "Epoch 2/5\n",
      "7200000/7200000 [==============================] - 23s 3us/step - loss: 0.0180 - val_loss: 0.0105\n",
      "Epoch 3/5\n",
      "7200000/7200000 [==============================] - 22s 3us/step - loss: 0.0080 - val_loss: 0.0065\n",
      "Epoch 4/5\n",
      "7200000/7200000 [==============================] - 20s 3us/step - loss: 0.0057 - val_loss: 0.0053\n",
      "Epoch 5/5\n",
      "7200000/7200000 [==============================] - 24s 3us/step - loss: 0.0050 - val_loss: 0.0047\n",
      "\n",
      "i =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/5\n",
      "7200000/7200000 [==============================] - 23s 3us/step - loss: 0.3726 - val_loss: 0.0314\n",
      "Epoch 2/5\n",
      "7200000/7200000 [==============================] - 22s 3us/step - loss: 0.0169 - val_loss: 0.0100\n",
      "Epoch 3/5\n",
      "7200000/7200000 [==============================] - 21s 3us/step - loss: 0.0077 - val_loss: 0.0063\n",
      "Epoch 4/5\n",
      "7200000/7200000 [==============================] - 21s 3us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 5/5\n",
      "7200000/7200000 [==============================] - 21s 3us/step - loss: 0.0049 - val_loss: 0.0047\n",
      "\n",
      "i =  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/5\n",
      "7200000/7200000 [==============================] - 23s 3us/step - loss: 0.2901 - val_loss: 0.0320\n",
      "Epoch 2/5\n",
      "7200000/7200000 [==============================] - 21s 3us/step - loss: 0.0177 - val_loss: 0.0106\n",
      "Epoch 3/5\n",
      "7200000/7200000 [==============================] - 22s 3us/step - loss: 0.0081 - val_loss: 0.0066\n",
      "Epoch 4/5\n",
      "7200000/7200000 [==============================] - 21s 3us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 5/5\n",
      "7200000/7200000 [==============================] - 21s 3us/step - loss: 0.0049 - val_loss: 0.0047\n",
      "\n",
      "i =  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/5\n",
      "7200000/7200000 [==============================] - 22s 3us/step - loss: 0.3623 - val_loss: 0.0290\n",
      "Epoch 2/5\n",
      "7200000/7200000 [==============================] - 22s 3us/step - loss: 0.0160 - val_loss: 0.0096\n",
      "Epoch 3/5\n",
      "7200000/7200000 [==============================] - 22s 3us/step - loss: 0.0077 - val_loss: 0.0061\n",
      "Epoch 4/5\n",
      "7200000/7200000 [==============================] - 21s 3us/step - loss: 0.0055 - val_loss: 0.0051\n",
      "Epoch 5/5\n",
      "7200000/7200000 [==============================] - 22s 3us/step - loss: 0.0048 - val_loss: 0.0046\n",
      "\n",
      "i =  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/5\n",
      "7200000/7200000 [==============================] - 24s 3us/step - loss: 0.2703 - val_loss: 0.0320\n",
      "Epoch 2/5\n",
      "7200000/7200000 [==============================] - 21s 3us/step - loss: 0.0183 - val_loss: 0.0111\n",
      "Epoch 3/5\n",
      "7200000/7200000 [==============================] - 21s 3us/step - loss: 0.0085 - val_loss: 0.0067\n",
      "Epoch 4/5\n",
      "7200000/7200000 [==============================] - 24s 3us/step - loss: 0.0059 - val_loss: 0.0055\n",
      "Epoch 5/5\n",
      "7200000/7200000 [==============================] - 20s 3us/step - loss: 0.0049 - val_loss: 0.0048\n",
      "\n",
      "i =  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/5\n",
      "7200000/7200000 [==============================] - 23s 3us/step - loss: 0.3671 - val_loss: 0.0407\n",
      "Epoch 2/5\n",
      "7200000/7200000 [==============================] - 21s 3us/step - loss: 0.0223 - val_loss: 0.0130\n",
      "Epoch 3/5\n",
      "7200000/7200000 [==============================] - 22s 3us/step - loss: 0.0095 - val_loss: 0.0074\n",
      "Epoch 4/5\n",
      "7200000/7200000 [==============================] - 23s 3us/step - loss: 0.0062 - val_loss: 0.0055\n",
      "Epoch 5/5\n",
      "7200000/7200000 [==============================] - 23s 3us/step - loss: 0.0050 - val_loss: 0.0048\n",
      "\n",
      "i =  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/5\n",
      "7200000/7200000 [==============================] - 17s 2us/step - loss: 0.3045 - val_loss: 0.0310\n",
      "Epoch 2/5\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.0169 - val_loss: 0.0103\n",
      "Epoch 3/5\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.0078 - val_loss: 0.0063\n",
      "Epoch 4/5\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 5/5\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.0049 - val_loss: 0.0046\n",
      "\n",
      "i =  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/5\n",
      "7200000/7200000 [==============================] - 16s 2us/step - loss: 0.4320 - val_loss: 0.0375\n",
      "Epoch 2/5\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.0195 - val_loss: 0.0111\n",
      "Epoch 3/5\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.0084 - val_loss: 0.0067\n",
      "Epoch 4/5\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.0058 - val_loss: 0.0054\n",
      "Epoch 5/5\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.0049 - val_loss: 0.0049\n",
      "\n",
      "i =  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/5\n",
      "7200000/7200000 [==============================] - 20s 3us/step - loss: 0.5139 - val_loss: 0.0965\n",
      "Epoch 2/5\n",
      "7200000/7200000 [==============================] - 16s 2us/step - loss: 0.0365 - val_loss: 0.0161\n",
      "Epoch 3/5\n",
      "7200000/7200000 [==============================] - 21s 3us/step - loss: 0.0112 - val_loss: 0.0080\n",
      "Epoch 4/5\n",
      "7200000/7200000 [==============================] - 16s 2us/step - loss: 0.0067 - val_loss: 0.0055\n",
      "Epoch 5/5\n",
      "7200000/7200000 [==============================] - 15s 2us/step - loss: 0.0052 - val_loss: 0.0045\n",
      "\n",
      "\n",
      "act_f =  tanh\n",
      "\n",
      "i =  0\n",
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/5\n",
      "7200000/7200000 [==============================] - 19s 3us/step - loss: 0.3689 - val_loss: 0.0460\n",
      "Epoch 2/5\n",
      "7200000/7200000 [==============================] - 16s 2us/step - loss: 0.0244 - val_loss: 0.0135\n",
      "Epoch 3/5\n",
      "7200000/7200000 [==============================] - 15s 2us/step - loss: 0.0097 - val_loss: 0.0071\n",
      "Epoch 4/5\n",
      "7200000/7200000 [==============================] - 16s 2us/step - loss: 0.0062 - val_loss: 0.0054\n",
      "Epoch 5/5\n",
      "7200000/7200000 [==============================] - 14s 2us/step - loss: 0.0051 - val_loss: 0.0047\n",
      "\n",
      "i =  1\n",
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/5\n",
      "7200000/7200000 [==============================] - 18s 3us/step - loss: 0.3618 - val_loss: 0.0425\n",
      "Epoch 2/5\n",
      "7200000/7200000 [==============================] - 15s 2us/step - loss: 0.0227 - val_loss: 0.0125\n",
      "Epoch 3/5\n",
      "7200000/7200000 [==============================] - 16s 2us/step - loss: 0.0090 - val_loss: 0.0068\n",
      "Epoch 4/5\n",
      "7200000/7200000 [==============================] - 15s 2us/step - loss: 0.0059 - val_loss: 0.0052\n",
      "Epoch 5/5\n",
      "7200000/7200000 [==============================] - 16s 2us/step - loss: 0.0050 - val_loss: 0.0048\n",
      "\n",
      "i =  2\n",
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/5\n",
      "7200000/7200000 [==============================] - 18s 3us/step - loss: 0.3659 - val_loss: 0.0697\n",
      "Epoch 2/5\n",
      "7200000/7200000 [==============================] - 16s 2us/step - loss: 0.0369 - val_loss: 0.0188\n",
      "Epoch 3/5\n",
      "7200000/7200000 [==============================] - 17s 2us/step - loss: 0.0126 - val_loss: 0.0086\n",
      "Epoch 4/5\n",
      "7200000/7200000 [==============================] - 18s 2us/step - loss: 0.0069 - val_loss: 0.0057\n",
      "Epoch 5/5\n",
      "7200000/7200000 [==============================] - 16s 2us/step - loss: 0.0053 - val_loss: 0.0049\n",
      "\n",
      "i =  3\n",
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/5\n",
      "7200000/7200000 [==============================] - 22s 3us/step - loss: 0.4702 - val_loss: 0.0789\n",
      "Epoch 2/5\n",
      "7200000/7200000 [==============================] - 21s 3us/step - loss: 0.0416 - val_loss: 0.0221\n",
      "Epoch 3/5\n",
      "7200000/7200000 [==============================] - 17s 2us/step - loss: 0.0150 - val_loss: 0.0103\n",
      "Epoch 4/5\n",
      "7200000/7200000 [==============================] - 16s 2us/step - loss: 0.0079 - val_loss: 0.0062\n",
      "Epoch 5/5\n",
      "7200000/7200000 [==============================] - 17s 2us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "\n",
      "i =  4\n",
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/5\n",
      "7200000/7200000 [==============================] - 17s 2us/step - loss: 0.4052 - val_loss: 0.0646\n",
      "Epoch 2/5\n",
      "7200000/7200000 [==============================] - 15s 2us/step - loss: 0.0307 - val_loss: 0.0151\n",
      "Epoch 3/5\n",
      "7200000/7200000 [==============================] - 15s 2us/step - loss: 0.0105 - val_loss: 0.0075\n",
      "Epoch 4/5\n",
      "7200000/7200000 [==============================] - 21s 3us/step - loss: 0.0063 - val_loss: 0.0054\n",
      "Epoch 5/5\n",
      "7200000/7200000 [==============================] - 29s 4us/step - loss: 0.0050 - val_loss: 0.0048\n",
      "\n",
      "i =  5\n",
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/5\n",
      "7200000/7200000 [==============================] - 27s 4us/step - loss: 0.2521 - val_loss: 0.0422\n",
      "Epoch 2/5\n",
      "7200000/7200000 [==============================] - 25s 3us/step - loss: 0.0230 - val_loss: 0.0130\n",
      "Epoch 3/5\n",
      "7200000/7200000 [==============================] - 25s 3us/step - loss: 0.0094 - val_loss: 0.0071\n",
      "Epoch 4/5\n",
      "7200000/7200000 [==============================] - 28s 4us/step - loss: 0.0060 - val_loss: 0.0053\n",
      "Epoch 5/5\n",
      "7200000/7200000 [==============================] - 26s 4us/step - loss: 0.0050 - val_loss: 0.0048\n",
      "\n",
      "i =  6\n",
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/5\n",
      "7200000/7200000 [==============================] - 28s 4us/step - loss: 0.4304 - val_loss: 0.0499\n",
      "Epoch 2/5\n",
      "7200000/7200000 [==============================] - 26s 4us/step - loss: 0.0260 - val_loss: 0.0139\n",
      "Epoch 3/5\n",
      "7200000/7200000 [==============================] - 26s 4us/step - loss: 0.0100 - val_loss: 0.0074\n",
      "Epoch 4/5\n",
      "7200000/7200000 [==============================] - 27s 4us/step - loss: 0.0062 - val_loss: 0.0053\n",
      "Epoch 5/5\n",
      "7200000/7200000 [==============================] - 27s 4us/step - loss: 0.0050 - val_loss: 0.0048\n",
      "\n",
      "i =  7\n",
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/5\n",
      "7200000/7200000 [==============================] - 30s 4us/step - loss: 0.3321 - val_loss: 0.0548\n",
      "Epoch 2/5\n",
      "7200000/7200000 [==============================] - 27s 4us/step - loss: 0.0307 - val_loss: 0.0179\n",
      "Epoch 3/5\n",
      "7200000/7200000 [==============================] - 25s 3us/step - loss: 0.0130 - val_loss: 0.0095\n",
      "Epoch 4/5\n",
      "7200000/7200000 [==============================] - 16s 2us/step - loss: 0.0081 - val_loss: 0.0068\n",
      "Epoch 5/5\n",
      "7200000/7200000 [==============================] - 24s 3us/step - loss: 0.0059 - val_loss: 0.0054\n",
      "\n",
      "i =  8\n",
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/5\n",
      "7200000/7200000 [==============================] - 29s 4us/step - loss: 0.4041 - val_loss: 0.1142\n",
      "Epoch 2/5\n",
      "7200000/7200000 [==============================] - 25s 4us/step - loss: 0.0763 - val_loss: 0.0555\n",
      "Epoch 3/5\n",
      "7200000/7200000 [==============================] - 29s 4us/step - loss: 0.0490 - val_loss: 0.0449\n",
      "Epoch 4/5\n",
      "7200000/7200000 [==============================] - 27s 4us/step - loss: 0.0429 - val_loss: 0.0441\n",
      "Epoch 5/5\n",
      "7200000/7200000 [==============================] - 27s 4us/step - loss: 0.0400 - val_loss: 0.0355\n",
      "\n",
      "i =  9\n",
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/5\n",
      "7200000/7200000 [==============================] - 30s 4us/step - loss: 0.3083 - val_loss: 0.0401\n",
      "Epoch 2/5\n",
      "7200000/7200000 [==============================] - 28s 4us/step - loss: 0.0218 - val_loss: 0.0123\n",
      "Epoch 3/5\n",
      "7200000/7200000 [==============================] - 28s 4us/step - loss: 0.0090 - val_loss: 0.0072\n",
      "Epoch 4/5\n",
      "7200000/7200000 [==============================] - 27s 4us/step - loss: 0.0059 - val_loss: 0.0054\n",
      "Epoch 5/5\n",
      "7200000/7200000 [==============================] - 27s 4us/step - loss: 0.0050 - val_loss: 0.0046\n"
     ]
    }
   ],
   "source": [
    "# Initialise data structures\n",
    "# activation_funcs = [\"relu\", \"tanh\", \"sigmoid\", \"linear\", \"softmax\"]\n",
    "activation_funcs = [\"leaky_relu\",\"tanh\"]\n",
    "# activation_func_sweep_results = {\"relu\":[], \"tanh\":[], \"sigmoid\":[], \"linear\": [], \"softmax\":[], \"leaky_relu\":[]}\n",
    "activation_func_sweep_results = {\"relu\":[], \"tanh\":[], \"sigmoid\":[], \"linear\": [], \n",
    "                                 \"softmax\":[{},{},{},{},{},{},{},{}], \"leaky_relu\":[]}\n",
    "\n",
    "# Initialise Variables\n",
    "M = 4 # Number of one hot encoded messages\n",
    "R = 2 # R = k/n\n",
    "sigma = get_noise_sigma(7, R)\n",
    "print(\"sigma = \",sigma)\n",
    "all_one_hot_messagesTmp = np.diag(np.ones(M))\n",
    "\n",
    "for act_f in activation_funcs:\n",
    "    print(\"\\n\\nact_f = \", act_f)\n",
    "    for i in range(10):\n",
    "        print(\"\\ni = \", i)\n",
    "        \n",
    "        if(act_f == \"leaky_relu\"):\n",
    "            input_act = keras.layers.advanced_activations.LeakyReLU()\n",
    "        else:\n",
    "            input_act = act_f\n",
    "        \n",
    "        # Make a model\n",
    "        autoencoderTmp, transmitterTmp, recieverTmp,\\\n",
    "            autoencoder_symbsTmp, kTmp, NcTmp, NrTmp \\\n",
    "            = make_model(M, R, sigma, input_act)\n",
    "        \n",
    "        # Fit the model\n",
    "        autoencoderTmp.fit(train_data4, train_data4,\n",
    "                        epochs=5,\n",
    "                        batch_size=1000*M,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(valid_data4,\n",
    "                                         valid_data4))\n",
    "        # Append a dictionary to the list for this activation \n",
    "        # function\n",
    "        activation_func_sweep_results[act_f].append({})\n",
    "        # Get the test BLER\n",
    "        pred_symbs_tmp = autoencoder_symbsTmp.predict(test_data4)\n",
    "        bler_tmp = get_block_error_rate(test_data4, pred_symbs_tmp) \n",
    "        activation_func_sweep_results[act_f][i][\"bler\"] = bler_tmp\n",
    "#         # Save the model\n",
    "#         activation_func_sweep_results[act_f][i][\"model\"] = {}\n",
    "#         activation_func_sweep_results[act_f][i][\"model\"][\"autoencoder\"] = autoencoderTmp\n",
    "#         activation_func_sweep_results[act_f][i][\"model\"][\"transmitter\"] = transmitterTmp\n",
    "#         activation_func_sweep_results[act_f][i][\"model\"][\"autoencoder_symbs\"] = autoencoder_symbsTmp\n",
    "        # Save the constellation diagram\n",
    "        const_diag = transmitterTmp.predict(all_one_hot_messagesTmp)\n",
    "        activation_func_sweep_results[act_f][i][\"const_diag\"] = const_diag\n",
    "#         # Save the models\n",
    "#         auto_file_path = './models/activ_func_sweep/' + act_f + str(i) + \"autoencoder\"\n",
    "#         autoencoderTmp.save(auto_file_path+\".model\")\n",
    "#         autoencoderTmp.save_weights(auto_file_path+'.h5')\n",
    "        \n",
    "#         tx_file_path = './models/activ_func_sweep/' + act_f + str(i) + \"transmitter\"\n",
    "#         transmitterTmp.save(tx_file_path+\".model\")\n",
    "#         transmitterTmp.save_weights(tx_file_path+'.h5')\n",
    "        \n",
    "#         auto_symbs_file_path = './models/activ_func_sweep/' + act_f + str(i) + \"autoencoder_symbs\"\n",
    "#         autoencoder_symbsTmp.save(auto_symbs_file_path+\".model\")\n",
    "#         autoencoder_symbsTmp.save_weights(auto_symbs_file_path+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_blers_tmp = np.zeros((2,10))\n",
    "af_const_diags_tmp = np.zeros((2,10,4,1,2))\n",
    "\n",
    "for j, act_f in enumerate([\"leaky_relu\", \"tanh\"]):\n",
    "    for i in range(10):\n",
    "        af_blers_tmp[j,i] = activation_func_sweep_results[act_f][i][\"bler\"]\n",
    "        af_const_diags_tmp[j,i,:,:,:] = activation_func_sweep_results[act_f][i][\"const_diag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAEICAYAAABf40E1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEZRJREFUeJzt232QXXV9x/H3hyybUJJIkfhACI0PsG3SWhWMXKXjDsFO8AlwdAp2pDS2qbV2pKbjE1UZraLSaP/QqrSujriCiOIjDg8pK8Vc5alUs2AwUmwiQR4UkhXYZN1v/zhnneu6D3dzz+7dvd/PayaTvfec8zvf37nnfO7vPFxFBGaW1yHtLsDM2sshYJacQ8AsOYeAWXIOAbPkHAJmyTkE5jFJ50q6seF1SHrmQbb155Kuqa66Ga9/SNLT27X+hUDSfZJOnuv1LqgQkPQaSbeUO9QeSd8a22iSjpDUV27IfZLukvS2KdrqlnSBpB9J+qWke8rlV89i/RdI+txstd+wntVlYHSNvRcR/RHxp7Owrl5Jo+VnMiRpt6TLJT2vcb6IWBoRd1e9/rkk6R0N/Xxc0q8aXg+2u76DtWBCQNKbgX8F3g88GTgW+Dfg9HKWjwBLgT8AngC8Atg5RZNXlPO8ppz/j4FbgfWzUH6nuzcilgLLgJOAHwL/JWnWt2Vj0M22iHh/GWZLgdcD9bHXEbF2ruqoXETM+38UB+kQ8Oop5tkOnNFke6cCjwGrppjnaOBrwM8pwuSvG6ZdAFwOfBbYBwwCJzZMfyvw03LaDopg2QDsBw6Uffmfhr59CthTLvPPwKJy2rnAjQ3tBvDM8u+XAv8N7AV2ARc0zPd/5bxD5b/aBG29ALgZeKT8/wUN0waA9wLfKftwDXDUJNupF9g9wfsfBW6Zae3l9HOAnwAPAe8E7gFObdj2VwCfK5f/K2AdUAceLrfjR4Hucet+A/Cjsj/vBZ4BbCvbuLxx/ib3od/Yng3vfxzYXbZ7E3BSw7QPAP3ApWUd3wee3TD9PuAfKPblR8p5Z1TXQR1f7Ty4Z7DBNwAjQNcU8/wHxcH4l8Bx07T3AeDb08xzA8VIYwnwbOAB4JSGHfFx4CXAIuBC4LvltJ5yxz66fL0aeEbDcp8bt54rgU8ChwNPKnecv5loRxt3IPUCf0QxmnsW8DPKECzXGY3bq7Et4EjgF8BrgS7g7PL1E8vpA8CPgeOBw8rXH5hkO/UycQicAowCh8+w9jUUwXUy0A38C0VwNobAAeCMcvnDgBMoRiBdZd/vBM4bt92+CiwH1gLDwFbg6RQhfAfwFw3zPwycPM3+8RufTcP75wC/CxwKnF/uC4c27HePAi+m2G8+Agw0LHsfRfA+GVhB8eVz7mwfXwvldOCJwIMRMTLFPH9PkZxvBO6QtFPSaVO0t2eyhiStAl4IvDUiHo+I2ylC5pyG2W6MiKsi4lfAJRSnEwC/AhYDayQdGhH3RMSPJ1nPkymC5LyI+GVE3E+xY5w1RT8BiIiBiPhBRIxGxPcpvl1eNN1ypZcCP4qISyJiJCIupRjCv7xhnk9HxF0R8RjFN+Wzm2x7zL2AgCNmWPurgK9HxI0RsR94F8VB3KgeEV8pl38sIm6NiO+WfbmHIlTHb4sPRcTeiBik+Ka9JiLujohHgG8Bz2mo74iIuJGDEBGfjYhfRMQBilPXJ1KEzZj/jIhrG/ab8dv1IxHxs4h4ALhqgumVWygh8BBw1FTnf+XO8P6IOIFiw18OfFHSkZO099Qp1nc08POI2Nfw3k+AlQ2v72v4+1FgiaSuiNgJnEfxjXW/pMskHT3Jen6P4htjj6SHJT1MsQM/aYraAJD0fEnXS3pA0iMU56hHTbdcQ/9+Mu696fq3tMm2x6ykOHgfHj9hmtqPpvj2BCAiHqX4vBrtanwh6XhJ3ygvCu+lOPjGb4ufNfz92ASvZ9q/CUl6u6QdZb9+QTGSbKxluu3a6nafsYUSAnWKIdwZzcwcEWM7wuHA0yaY5TpgnaRjJmniXuBIScsa3juW4py9mfV/PiJOpjjIA/jg2KRxs+6i6NdR5bfPERGxPJq7yPR5imsWqyLiCcAnKL55J1rPePeWtTVqun9NOhO4LSJ+OcG0qWrfA/z6c5F0GEWoNxrfv49TjGSOi4jlwDsa2pszkl5MMSI9k2IEdCRFwMx5LTOxIEKgHLK9C/iYpDMk/Y6kQyWdJulDAJLeKel55a2/JcCbKL6FdkzQ3nXAtcCVkk6Q1CVpmaTXS9oYEbsoLhpdKGmJpGcBr6O4GDUlST2STpG0mOK6wWMU58ZQfPuslnRIWcceiotuWyQtl3SIpGdIamZYv4xitPK4pHUUdznGPFCuc7L78lcBx5e3XLsk/RnFufg3mljvpFRYKendFBfs3nEQtV8BvFzSCyR1U4yopjuIllFciBuS9PvA37bSjxYso7he8QDF9Yz3UIwE5rUFEQIAEbEFeDPwTxQbeRfF+f9XxmYBPg08SPFN92LgpRExNEmTr6I4GL5AcSV2O3AixSgBiotlq8u2rgTeXYbHdBZTXAB6kGJo9yTg7eW0L5b/PyTptvLvcyh2mDsoho9XMPWpypg3AO+RtI8iIC8fm1AOod8HfKc8zTipccGIeAh4GbCZYqj9FuBlEfFgE+udyNGSxu5E3Exx0a83IiZ7OGmq2gcpvk0voxgVDAH3U4yYJvOPFEGyD/h3is/0oJX3/f/kIBb9OsUF5R8Dd1PsAw+0UstcUHlV0mxekrSUYkR3XET8b7vr6UQLZiRgeUh6eXnKdzjFLcIfUDwrYLOg5RCQtKq80nuHpEFJb6qiMEvtdIrTsHuB44CzwkPWWdPy6YCkpwJPjYjbyqvpt1I8+HFHFQWa2exqeSQQEXsi4rby730UT2utnHopM5svKv3xRfkLvOcA35tg2iZgE8CSJUtOOPbYY6tc9bwwOjrKIYd05mWWTu1bp/brrrvuejAiVjQzb2V3B8qruN8G3hcRX55q3p6entix47du3y94AwMD9Pb2truMWdGpfevUfkm6NSJObGbeSiJQ0qHAl4D+6QLAzOaXKu4OiOKnsHdGxIdbL8nM5lIVI4EXUvwk9RRJt5f/XlJBu2Y2B1q+MFj+5HJe/0DCzCbXeZdFzWxGHAJmyTkEzJJzCJgl5xAwS84hYJacQ8AsOYeAWXIOAbPkHAJmyTkEzJJzCJgl5xAwS84hYJacQ8AsOYeAWXIOAbPkHAJmyTkEzJJzCJgl5xAwS84hYJacQ8AsOYeAWXIOAbPkHAJmyTkEzJJzCJgl5xAwS84hYJacQ8AsOYeAWXIOAbPkHAJmyTkEzJJzCJgl5xAwS84hYJacQ8AsOYeAWXIOAbPkHAJmyTkEzJJzCJgl5xAwS84hYJZcJSEgqU/S/ZK2V9Gemc2dqkYCnwE2VNSWzUODg4NceOGF1Ov1dpdiFeuqopGIuEHS6irasvmnXq+zefNmRkZG6O7uZuvWrdRqtXaXZRWpJASaIWkTsAlgxYoVDAwMzNWq58zQ0FBH9qu/v58DBw4wOjrK8PAwfX19DA8Pt7usSnTqZzYjEVHJP2A1sL2ZeY8//vjoRNdff327S5gV27Zti8WLF8eiRYvisMMOi23btrW7pMp06mcG3BJNHrtzNhKwhatWq7Flyxb27t1Lb2+vTwU6jEPAmrJ27Vp6e3vbXYbNgqpuEV4K1IEeSbslva6Kds1s9lV1d+DsKtoxs7nnJwbNknMImCXnEDBLziFglpxDwCw5h4BZcg4Bs+QcAmbJOQTMknMImCXnEDBLziFglpxDwCw5h4BZcg4Bs+QcAmbJOQTMknMImCXnEDBLziFglpxDwCw5h4BZcg4Bs+QcAmbJOQTMknMImCXnEDBLziFglpxDwCw5h4BZcg4Bs+QcAmbJOQTMknMImCXnEDBLziFglpxDwCw5h4BZcg4Bs+QcAmbJOQTMknMImCXnEDBLziFglpxDwCy5SkJA0gZJOyTtlPS2KtpcaOr1Ov39/dTr9XaXYjYjLYeApEXAx4DTgDXA2ZLWtNruQlKv11m/fj19fX2sX7/eQWALShUjgXXAzoi4OyL2A5cBp1fQ7oIxMDDA/v37GR0dZf/+/QwMDLS7JGuCR2+FrgraWAnsani9G3j++JkkbQI2AaxYsaKjDpTly5fT1dVFRNDV1cXy5cs7qn8AQ0NDHdWnwcFBNm/ezIEDB7jkkkvYsmULa9eubXdZbVFFCDQlIi4GLgbo6emJ3t7euVr1rOvt7eW5z30ufX19bNy4kVqt1u6SKjcwMEAnfWb1ep2RkRFGR0cZGRlh7969HdW/magiBH4KrGp4fUz5Xiq1Wo3h4eGODIBO1NvbS3d3N8PDw3R3d6cNAKjmmsDNwHGSniapGzgL+FoF7ZrNmlqtxtatW9m4cSNbt25NHd4tjwQiYkTSG4GrgUVAX0QMtlyZ2Szz6K1QyTWBiLgKuKqKtsxsbvmJQbPkHAJmyTkEzJJzCJgl5xAwS84hYJacQ8AsOYeAWXIOAbPkHAJmyTkEzJJzCJgl5xAwS84hYJacQ8AsOYeAWXIOAbPkHAJmyTkEzJJzCJgl5xAwS84hYJacQ8AsOYeAWXIOAbPkHAJmyTkEzJJzCJgl5xAwS84hYJacQ8AsOYeAWXIOAbPkHAJmyTkEzJJzCJgl5xAwS84hYJacQ8AsOYeAWXIOAbPkHAJmyTkEzJJzCJgl5xAwS66lEJD0akmDkkYlnVhVUWY2d7paXH478ErgkxXUYvNMvV5nYGCA3t7edpdis6ilEIiIOwEkVVONzRv1ep3169ezf/9+uru7ueiiixwGHarVkUDTJG0CNgGsWLGCgYGBuVr1nBkaGuqYfvX39zM8PMzo6CjDw8PcdNNNrF27tt1lVa6TPrODNW0ISLoOeMoEk86PiK82u6KIuBi4GKCnpyc68Vulk4bOixcvpr+//9cjgXXr1nVM3xp10md2sKYNgYg4dS4KsfmlVquxdevWXx8kw8PD7S7JZsmcnQ7YwlOr1ajVagDph8ydrNVbhGdK2g3UgG9KurqassxsrrR6d+BK4MqKajGzNvATg2bJOQTMknMImCXnEDBLziFglpxDwCw5h4BZcg4Bs+QcAmbJOQTMknMImCXnEDBLziFglpxDwCw5h4BZcg4Bs+QcAmbJOQTMknMImCXnEDBLziFglpxDwCw5h4BZcg4Bs+QcAmbJOQTMknMImCXnEDBLziFglpxDwCw5h4BZcg4Bs+QcAmbJOQTMknMImCXnEDBLziFglpxDwCw5h4BZcg4Bs+QcAmbJOQTMknMImCXnEDBLziFglpxDwCy5lkJA0kWSfijp+5KulHREVYWZ2dxodSRwLfCHEfEs4C7g7a2XZGZzqaUQiIhrImKkfPld4JjWSzKzudRVYVsbgS9MNlHSJmBT+XJY0vYK1z1fHAU82O4iZkmn9q1T+9XT7IyKiKlnkK4DnjLBpPMj4qvlPOcDJwKvjOkaLOa/JSJObLbIhaJT+wWd2zf3q4mRQEScOs3KzgVeBqxvJgDMbH5p6XRA0gbgLcCLIuLRakoys7nU6t2BjwLLgGsl3S7pE00ud3GL652vOrVf0Ll9S9+vaa8JmFln8xODZsk5BMySa1sIdOojx5JeLWlQ0qikBX/rSdIGSTsk7ZT0tnbXUxVJfZLu77TnVSStknS9pDvK/fBN0y3TzpFApz5yvB14JXBDuwtplaRFwMeA04A1wNmS1rS3qsp8BtjQ7iJmwQiwOSLWACcBfzfdZ9a2EOjUR44j4s6I2NHuOiqyDtgZEXdHxH7gMuD0NtdUiYi4Afh5u+uoWkTsiYjbyr/3AXcCK6daZr5cE9gIfKvdRdhvWQnsani9m2l2KJs/JK0GngN8b6r5qvztwERFNPvI8QjQP5u1VKmZfpm1k6SlwJeA8yJi71TzzmoIdOojx9P1q4P8FFjV8PqY8j2bxyQdShEA/RHx5enmb+fdgbFHjl/hR47nrZuB4yQ9TVI3cBbwtTbXZFOQJOBTwJ0R8eFmlmnnNYGDfeR4XpN0pqTdQA34pqSr213TwSov3L4RuJriAtPlETHY3qqqIelSoA70SNot6XXtrqkiLwReC5xSHle3S3rJVAv4sWGz5ObL3QEzaxOHgFlyDgGz5BwCZsk5BMyScwiYJecQMEvu/wGTkwxOLRhrIgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Means\")\n",
    "print(af_blers_tmp.mean(axis=1))\n",
    "print(\"Mins\")\n",
    "print(af_blers_tmp.min(axis=1))\n",
    "\n",
    "tmp = af_const_diags_tmp[0,0]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(tmp[:,:,0], tmp[:,:,1],\\\n",
    "        'k.')\n",
    "ax.set_title(\"CS Constellation Diagram: Tanh\")\n",
    "ax.set_aspect('equal', 'box')\n",
    "plt.xticks([-2,-1,0,1,2])\n",
    "plt.yticks([-2,-1,0,1,2])\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('./key_results/af_blers_tanh_vs_relu.npy', af_blers_tmp)\n",
    "# np.save('./key_results/af_const_diags_tanh_vs_relu.npy', af_const_diags_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_blers_tmp2 = np.load('./key_results/af_blers_tanh_vs_relu.npy')\n",
    "af_const_diags_tmp2 = np.load('./key_results/af_const_diags_tanh_vs_relu.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data from the models saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialise data structures\n",
    "# activation_funcs = [\"relu\", \"tanh\", \"sigmoid\", \"linear\", \"softmax\"]\n",
    "# # activation_funcs = [\"softmax\"]\n",
    "# activation_func_sweep_results = {\"relu\":[], \"tanh\":[], \"sigmoid\":[], \"linear\": [], \"softmax\":[], \"leakyrelu\":[]}\n",
    "# af_blers = np.zeros((5,10))\n",
    "# af_const_diags = np.zeros((5,10,4,1,2))\n",
    "\n",
    "# # Initialise Variables\n",
    "# M = 4 # Number of one hot encoded messages\n",
    "# R = 2 # R = k/n\n",
    "# sigma = get_noise_sigma(7, R)\n",
    "# print(\"sigma = \",sigma)\n",
    "# all_one_hot_messagesTmp = np.diag(np.ones(M))\n",
    "\n",
    "# for j, act_f in enumerate(activation_funcs):\n",
    "#     print(\"\\nact_f = \", act_f)\n",
    "#     #     for i in range(10):\n",
    "#     for i in range(10):\n",
    "#         print(\"i = \", i)\n",
    "                \n",
    "#         # Load a model's autoencoder, transmitter and \n",
    "#         # autoencoder_symbs\n",
    "#         auto_file_path = './models/activ_func_sweep/' + act_f + str(i) + \"autoencoder\"\n",
    "#         autoencoderTmp, transmitterTmp, recieverTmp, autoencoder_symbsTmp, kTmp, NcTmp, NrTmp = make_model(4, 2, sigma, act_f)\n",
    "#         # autoencoder_loaded = load_model('first_qpsk_model.model')\n",
    "#         autoencoderTmp.load_weights(auto_file_path+'.h5', by_name=True)\n",
    "        \n",
    "#         # Append a dictionary to the list for this activation \n",
    "#         # function\n",
    "#         activation_func_sweep_results[act_f].append({})\n",
    "#         # Get the test BLER\n",
    "#         pred_symbs_tmp = autoencoder_symbsTmp.predict(test_data4)\n",
    "#         bler_tmp = get_block_error_rate(test_data4, pred_symbs_tmp) \n",
    "#         activation_func_sweep_results[act_f][i][\"bler\"] = bler_tmp\n",
    "#         af_blers[j,i] = bler_tmp\n",
    "        \n",
    "#         # Save the constellation diagram\n",
    "#         const_diag = transmitterTmp.predict(all_one_hot_messagesTmp)\n",
    "#         activation_func_sweep_results[act_f][i][\"const_diag\"] = const_diag\n",
    "#         af_const_diags[j,i,:,:,:] = const_diag\n",
    "        \n",
    "#         # Save the models in the dictionary\n",
    "#         activation_func_sweep_results[act_f][i][\"model\"] = {}\n",
    "#         activation_func_sweep_results[act_f][i][\"model\"][\"autoencoder\"] = autoencoderTmp\n",
    "#         activation_func_sweep_results[act_f][i][\"model\"][\"transmitter\"] = transmitterTmp\n",
    "#         activation_func_sweep_results[act_f][i][\"model\"][\"autoencoder_symbs\"] = autoencoder_symbsTmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('./key_results/af_blers.npy', af_blers)\n",
    "# np.save('./key_results/af_const_diags.npy', af_const_diags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the test errors for the five activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_blers = np.load('./key_results/af_blers.npy')\n",
    "af_const_diags = np.load('./key_results/af_const_diags.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.mins\n",
      "[0.0578365 0.0015135 0.294128  0.0015285 0.294504 ]\n",
      "np.means\n",
      "[0.55835335 0.00155635 0.29458565 0.00156515 0.4387    ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MAYBE CHANGE TO A SEMILOGY GRAPH INSTEAD\n",
    "plt.semilogy(np.zeros(10), af_blers[0,:], 'r.')\n",
    "plt.semilogy(1*np.ones(10), af_blers[1,:], 'bx')\n",
    "plt.semilogy(2*np.ones(10), af_blers[2,:], 'g*')\n",
    "plt.semilogy(3*np.ones(10), af_blers[3,:], 'ko')\n",
    "plt.semilogy(4*np.ones(10), af_blers[4,:], 'mx')\n",
    "plt.title(\"Test errors for different activation functions\")\n",
    "plt.ylabel(\"Test error\")\n",
    "plt.legend([\"relu\", \"tanh\", \"sigmoid\", \"linear\", \"softmax\"], loc=\"lower right\")\n",
    "print(\"np.mins\")\n",
    "print(af_blers.min(axis=1))\n",
    "print(\"np.means\")\n",
    "print(af_blers.mean(axis=1))\n",
    "# plt.savefig(\"./figures/first_act_f_comparisson_logy.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Tanh has the lowest bler out of all the activation functions, marginally ahead of linear. I want to check leaky relu as well though as that looked promising in the small amount of stuff I ran for it. As sen below the constellation diagrams look fine for Tanh and Linear below. Leaky relu is marginally better than tanh. Going to use tanh and Relu from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAEICAYAAABf40E1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAERlJREFUeJzt3X+QXWV9x/H3hyRLKAlSICAJoUGEbaOlMERkgY47BDrhl/wYmYIdKYU2tZYOVDvKj6qMVkJtlf4B1dKKjoIgohFEGH5sXSl2sfwoxSSYECg0gQAJP7P8cEny7R/PWea67t69m3v23rvn+bxmMtm75znPeZ6z537uc55z7r2KCMwsXzu0uwFm1l4OAbPMOQTMMucQMMucQ8Ascw4Bs8w5BDqYpLMl3VvzOCS9ezvr+iNJd5bXuglvf1DSu9q1/alA0rOSjmr1dqdUCEj6sKQHigNqg6Tbh3eapF0lXVPsyM2S1ki6sE5dXZIulfSYpNckPVmsv2AS23+ppGsnq/6a7SwoAmP68O8i4rqI+INJ2FavpG3F32RQ0npJN0p6X225iJgVEU+Uvf1WknRxTT/flLS15vHKdrdve02ZEJD0ceCfgMuAvYB9gX8GTi6KXAHMAn4HeAfwQWBtnSpvKsp8uCj/e8CDwOJJaH7VPRMRs4DZwOHAL4D/kDTp+7I26CZbRFxWhNks4KPAwPDjiHhPq9pRuojo+H+kJ+kgcHqdMiuAUxqs7xjgDWB+nTJzgVuAF0lh8mc1yy4FbgS+CWwGVgKLapZ/Cni6WLaaFCxLgCHgraIv/1PTt68BG4p1/g6YViw7G7i3pt4A3l38fALw38CrwDrg0ppy/1eUHSz+9YxS1xHA/cArxf9H1CzrBz4P/LTow53AHmPsp15g/Si/vxJ4YKJtL5afBTwFvAB8GngSOKZm398EXFus/6fAYcAA8HKxH68EukZs+2PAY0V/Pg/sD/xnUceNteUbPIZ+ZX/W/P4rwPqi3v8CDq9ZdjlwHXB90Y5HgINrlj8L/DXpWH6lKDuhdm3X86udT+4J7PAlwBZgep0y/0Z6Mv4JcMA49V0O/GScMveQRhozgYOBjcDRNQfim8DxwDRgGXBfsay7OLDnFo8XAPvXrHftiO0sB/4F2BnYszhw/ny0A23EE6kX+F3SaO4g4DmKECy2GbX7q7YuYDfgJeAjwHTgzOLx7sXyfuBx4EBgp+Lx5WPsp15GD4GjgW3AzhNs+0JScB0FdAH/SArO2hB4CzilWH8n4FDSCGR60fdHgQtG7LebgV2A9wC/BPqAd5FCeBXwxzXlXwaOGuf4+JW/Tc3vzwJ+E5gBXFIcCzNqjrvXgWNJx80VQH/Nus+SgncvYA7pxefsyX5+TZXTgd2BTRGxpU6ZvyIl53nAKklrJR1Xp74NY1UkaT5wJPCpiHgzIh4mhcxZNcXujYjbImIr8C3S6QTAVmBHYKGkGRHxZEQ8PsZ29iIFyQUR8VpEPE86MM6o008AIqI/In4eEdsi4hHSq8sHxluvcALwWER8KyK2RMT1pCH8STVlvh4RayLiDdIr5cEN1j3sGUDArhNs+4eAH0bEvRExBHyG9CSuNRARPyjWfyMiHoyI+4q+PEkK1ZH74osR8WpErCS90t4ZEU9ExCvA7cAhNe3bNSLuZTtExDcj4qWIeIt06ro7KWyG/XtE3FVz3Izcr1dExHMRsRG4bZTlpZsqIfACsEe987/iYLgsIg4l7fgbge9K2m2M+vaus725wIsRsbnmd08B82oeP1vz8+vATEnTI2ItcAHpFet5STdImjvGdn6L9IqxQdLLkl4mHcB71mkbAJLeL+nHkjZKeoV0jrrHeOvV9O+pEb8br3+zGqx72DzSk/flkQvGaftc0qsnABHxOunvVWtd7QNJB0q6tZgUfpX05Bu5L56r+fmNUR5PtH+jknSRpNVFv14ijSRr2zLefm12v0/YVAmBAdIQ7pRGCkfE8IGwM7DfKEXuBg6TtM8YVTwD7CZpds3v9iWdszey/W9HxFGkJ3kAfz+8aETRdaR+7VG8+uwaEbtEY5NM3ybNWcyPiHcAXyW98o62nZGeKdpWq+H+NehU4KGIeG2UZfXavgF4++8iaSdSqNca2b+vkEYyB0TELsDFNfW1jKRjSSPSU0kjoN1IAdPytkzElAiBYsj2GeAqSadI+g1JMyQdJ+mLAJI+Lel9xaW/mcD5pFeh1aPUdzdwF7Bc0qGSpkuaLemjks6JiHWkSaNlkmZKOgg4lzQZVZekbklHS9qRNG/wBuncGNKrzwJJOxTt2ECadPuSpF0k7SBpf0mNDOtnk0Yrb0o6jHSVY9jGYptjXZe/DTiwuOQ6XdIfks7Fb21gu2NSMk/SZ0kTdhdvR9tvAk6SdISkLtKIarwn0WzSRNygpN8G/qKZfjRhNmm+YiNpPuNzpJFAR5sSIQAQEV8CPg78LWknryOd//9guAjwdWAT6ZXuWOCEiBgco8oPkZ4M3yHNxK4AFpFGCZAmyxYUdS0HPluEx3h2JE0AbSIN7fYELiqWfbf4/wVJDxU/n0U6YFaRho83Uf9UZdjHgM9J2kwKyBuHFxRD6C8APy1OMw6vXTEiXgBOBD5BGmp/EjgxIjY1sN3RzJU0fCXiftKkX29EjHVzUr22ryS9mt5AGhUMAs+TRkxj+RtSkGwG/pX0N91uxXX/39+OVX9ImlB+HHiCdAxsbKYtraBiVtKsI0maRRrRHRAR/9vu9lTRlBkJWD4knVSc8u1MukT4c9K9AjYJmg4BSfOLmd5VklZKOr+MhlnWTiadhj0DHACcER6yTpqmTwck7Q3sHREPFbPpD5Ju/FhVRgPNbHI1PRKIiA0R8VDx82bS3Vrz6q9lZp2i1DdfFO/AOwT42SjLlgJLAWbOnHnovvvuW+amO8K2bdvYYYdqTrNUtW9V7deaNWs2RcScRsqWdnWgmMX9CfCFiPh+vbLd3d2xevWvXb6f8vr7++nt7W13MyZFVftW1X5JejAiFjVStpQIlDQD+B5w3XgBYGadpYyrAyK9FfbRiPhy800ys1YqYyRwJOktqUdLerj4d3wJ9ZpZCzQ9MVi85bKj3yBhZmOr3rSomU2IQ8Ascw4Bs8w5BMwy5xAwy5xDwCxzDgGzzDkEzDLnEDDLnEPALHMOAbPMOQTMMucQMMucQ8Ascw4Bs8w5BMwy5xAwy5xDwCxzDgGzzDkEzDLnEDDLnEPALHMOAbPMOQTMMucQMMucQ8Ascw4Bs8w5BMwy5xAwy5xDwCxzDgGzzDkEzDLnEDDLnEPALHMOAbPMOQTMMucQMMucQ8Ascw4Bs8w5BMwy5xAwy5xDwCxzDgGzzDkEzDLnEDDLnEPALHOlhICkayQ9L2lFGfWZWeuUNRL4BrCkpLrMrIVKCYGIuAd4sYy6zKy1prdqQ5KWAksB5syZQ39/f6s23TKDg4OV7BdUt29V7ddEKCLKqUhaANwaEe8dr2x3d3esXr26lO12kv7+fnp7e9vdjElR1b5VtV+SHoyIRY2U9dUBs8w5BMwyV9YlwuuBAaBb0npJ55ZRr5lNvlImBiPizDLqMbPW8+mAdaSBgQGWLVvGwMBAu5tSeS27RGjWqIGBARYvXszQ0BBdXV309fXR09PT7mZVlkcC1nH6+/sZGhpi69atDA0NZX8df7I5BKzj9Pb20tXVxbRp0+jq6qrkdfxO4tMB6zg9PT309fW9fSOPTwUml0PAOlJPT4+f/C3i0wGzzDkEzDLnEDDLnEPALHMOAbPMOQTMMucQMMucQ8Ascw4Bs8w5BMwy5xAwy5xDwCxzDoEW8SflWKfyuwhbwJ+UY53MI4EW8CflWCdzCLSAPynHOplPB1rAn5Rjncwh0CL+pBzrVD4dMMucQ8Ascw4Bs8w5BMwy5xAwy5xDwCxzDgGzzDkEzDLnEDDLnEPALHMOAbPMOQTMMucQMMucQ8Ascw4Bs8w5BMwy5xAwy5xDwCxzDgGzzDkEzDLnEDDLXCkhIGmJpNWS1kq6sIw6zaw1mg4BSdOAq4DjgIXAmZIWNluvmbVGGSOBw4C1EfFERAwBNwAnl1CvmbVAGV8+Mg9YV/N4PfD+kYUkLQWWAsyZM6eS38c3ODhYyX5BdftW1X5NRMu+gSgirgauBuju7o4qfh/f8NeMVVFV+1bVfk1EGacDTwPzax7vU/zOzKaAMkLgfuAASftJ6gLOAG4poV4za4GmTwciYouk84A7gGnANRGxsumWmVlLlDInEBG3AbeVUZeZtZbvGDTLnEPALHMOAbPMOQTMMucQMMucQ8Ascw4Bs8w5BMwy5xAwy5xDwCxzDgGzzDkEzDLnEDDLnEPALHMOAbPMOQTMMucQMMucQ8Ascw4Bs8w5BMwy5xAwy5xDwCprYGCAZcuWMTAw0O6mdLSWfQ2ZWSsNDAywePFihoaG6Orqoq+vj56ennY3qyN5JGCV1N/fz9DQEFu3bmVoaCj7Lx2txyFgldTb20tXVxfTpk2jq6sr+y8drcenA1ZJPT099PX1vf2twz4VGJtDwCqrp6fHT/4G+HTALHMOAbPMOQTMMucQMMucQ8Ascw4Bs8w5BMwy5xAwy5xDwCxzDgGzzDkEzDLnEDDLnENgCvIn5liZ/C7CKcafmGNl80hgivEn5ljZHAJTjD8xx8rm04Epxp+YY2VzCExB/sQcK1NTpwOSTpe0UtI2SYvKapSZtU6zcwIrgNOAe0poi5m1QVOnAxHxKICkclpjZi3XsjkBSUuBpQBz5syp5KWtwcHBSvYLqtu3qvZrIsYNAUl3A+8cZdElEXFzoxuKiKuBqwG6u7ujipe2hmfsq6iqfatqvyZi3BCIiGNa0RAzaw/fLGSWuWYvEZ4qaT3QA/xI0h3lNMvMWqXZqwPLgeUltcXM2sCnA2aZcwiYZc4hYJY5h4BZ5hwCZplzCJhlziFgljmHgFnmHAJmmXMImGXOIWCWOYeAWeYcAmaZcwiYZc4hYJY5h4BZ5hwCZplzCJhlziFgljmHgFnmHAJmmXMImGXOIWCWOYeAWeYcAmaZcwiYZc4hYJY5h4BZ5hwCZplzCJhlziFgljmHgFnmHAJmmXMImGXOIWCWOYeAWeYcAmaZcwiYZc4hYJY5h4BZ5hwCZplzCJhlziFgljmHgFnmHAJmmXMImGWuqRCQ9A+SfiHpEUnLJe1aVsPMrDWaHQncBbw3Ig4C1gAXNd8kM2ulpkIgIu6MiC3Fw/uAfZpvkpm10vQS6zoH+M5YCyUtBZYWD38paUWJ2+4UewCb2t2ISVLVvlW1X92NFlRE1C8g3Q28c5RFl0TEzUWZS4BFwGkxXoWp/AMRsajRRk4VVe0XVLdv7lcDI4GIOGacjZ0NnAgsbiQAzKyzNHU6IGkJ8EngAxHxejlNMrNWavbqwJXAbOAuSQ9L+mqD613d5HY7VVX7BdXtW/b9GndOwMyqzXcMmmXOIWCWubaFQFVvOZZ0uqSVkrZJmvKXniQtkbRa0lpJF7a7PWWRdI2k56t2v4qk+ZJ+LGlVcRyeP9467RwJVPWW4xXAacA97W5IsyRNA64CjgMWAmdKWtjeVpXmG8CSdjdiEmwBPhERC4HDgb8c72/WthCo6i3HEfFoRKxudztKchiwNiKeiIgh4Abg5Da3qRQRcQ/wYrvbUbaI2BARDxU/bwYeBebVW6dT5gTOAW5vdyPs18wD1tU8Xs84B5R1DkkLgEOAn9UrV+Z7B0ZrRKO3HG8BrpvMtpSpkX6ZtZOkWcD3gAsi4tV6ZSc1BKp6y/F4/aqQp4H5NY/3KX5nHUzSDFIAXBcR3x+vfDuvDgzfcvxB33Lcse4HDpC0n6Qu4Azglja3yeqQJOBrwKMR8eVG1mnnnMD23nLc0SSdKmk90AP8SNId7W7T9iombs8D7iBNMN0YESvb26pySLoeGAC6Ja2XdG6721SSI4GPAEcXz6uHJR1fbwXfNmyWuU65OmBmbeIQMMucQ8Ascw4Bs8w5BMwy5xAwy5xDwCxz/w8BF2e4ImMaBwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tanh_channel_symbols = af_const_diags[1,0,:,:,:]\n",
    "linear_channel_symbols = af_const_diags[3,0,:,:,:]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(tanh_channel_symbols[:,:,0], tanh_channel_symbols[:,:,1],\\\n",
    "        'k.')\n",
    "ax.set_title(\"CS Constellation Diagram: Tanh\")\n",
    "ax.set_aspect('equal', 'box')\n",
    "plt.xticks([-2,-1,0,1,2])\n",
    "plt.yticks([-2,-1,0,1,2])\n",
    "plt.grid(True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(linear_channel_symbols[:,:,0], linear_channel_symbols[:,:,1],\\\n",
    "        'k.')\n",
    "ax.set_title(\"CS Constellation Diagram: Linear\")\n",
    "ax.set_aspect('equal', 'box')\n",
    "plt.xticks([-2,-1,0,1,2])\n",
    "plt.yticks([-2,-1,0,1,2])\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training two systems side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RTN (Radio transfomer networks)\n",
    "They found it consistently outperformed a normal autoencoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
