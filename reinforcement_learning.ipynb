{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning - Aoudia and Hoydis\n",
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Lambda, ELU, Reshape\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from keras import backend as K\n",
    "from keras.layers import GaussianNoise, advanced_activations\n",
    "from keras.engine.topology import Layer\n",
    "from keras.legacy import interfaces\n",
    "from keras.initializers import Zeros as kZeros\n",
    "from keras.utils import multi_gpu_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from time import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # confirm TensorFlow sees the GPU\n",
    "# from tensorflow.python.client import device_lib\n",
    "# assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# # confirm Keras sees the GPU\n",
    "# from keras import backend\n",
    "# assert len(backend.tensorflow_backend._get_available_gpus()) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is in a seperate box because it isn't running on the \n",
    "# # AWS server. \n",
    "# from tqdm import tqdm_notebook, tnrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Useful guides\n",
    "https://hub.packtpub.com/build-reinforcement-learning-agent-in-keras-tutorial/\n",
    "<br>\n",
    "https://medium.com/ml-everything/policy-based-reinforcement-learning-with-keras-4996015a0b1\n",
    "<br>\n",
    "\n",
    "##### Notes from paper\n",
    "- Loss function = Cross Entropy\n",
    "- Normalisation = Average L2 power constraint = 1. $\\mathbb{E}\\left[\\frac{1}{N}\\left\\Vert x\\right\\Vert_2^2\\right]$ = 1.\n",
    "- Trained at an SNR of 10dB for AWGN and an SNR of 20dB for RBF channel. $\\sigma_{\\pi}^2$ = 0.02 at training time.\n",
    "    - During RL exploration $\\mathbf{x_p} = \\mathbf{x} + \\mathbf{w}$, where each element of $\\mathbf{w}$ is i.i.d ~ $\\mathcal{N}(0,\\sigma_{\\pi}^2)$\n",
    "- M= 256, N=4. N= the number of complex channel uses, so n = 8. Therefore n,k = (8,8)\n",
    "- AWGN -> 1 hidden layer, size M, ReLu activation function\n",
    "- Rayleigh -> see diagram.\n",
    "    - Two layers (Dense(20,tanh)->Dense(2,linear)) calculate an estimate for $\\hat{h}$, then we divide the received signal by $\\hat{h}$ then have two layers for finding the received signal. Dense(M,ReLu) then Dense(M,Softmax). Then have the select maximum likelihood symbol layer.\n",
    "- SNR $ = \\frac{\\mathbb{E}\\left[\\frac{1}{N}\\left\\Vert x\\right\\Vert_2^2\\right]}{\\sigma^2}$, but because of the normalisation this is $\\frac{1}{\\sigma^2}$.\n",
    "- The RBF seems to be slow fading, but I could check for fast fading as well. Rayleigh fading is of the form $\\mathbf{r} = \\mathbf{s}*\\mathbf{h} + \\mathbf{n}$ where $\\mathbf{h}$ ~ $\\mathcal{N}(0,\\sigma_m^2)$, $\\mathbf{n}$ ~ $\\mathcal{N}(0,\\sigma_a^2)$. \n",
    "    - Slow fading: h stays the same across the whole minibatch\n",
    "    - Fast fading: h changes every sample\n",
    "- I'm going to guess that $\\sigma_m ~ \\frac{1}{3}$. Because this means that 98% of the time it's less than 1. Which sounds alright.\n",
    "- I could also try Ricean fading and see how the model performs against that.\n",
    "\n",
    "##### Work Log\n",
    "\n",
    "27/05/2019\n",
    "- Started researching RBF, think I can implement it in a custom layer.\n",
    "- Made lots of notes\n",
    "- Copied over functions from other notebook.\n",
    "\n",
    "28/05/2019\n",
    "- Made functions for making the Tx and Rx blocks for the unsupervised learning, however these will likely need to be edited.\n",
    "\n",
    "\n",
    "29/05/2019\n",
    "- Got food poisoning the night before and did no work.\n",
    "\n",
    "31/05/2019\n",
    "- Got the rbf supervised model fitting, but with no good results. \n",
    "- Copied the architecture from the paper exactly, however it was not effective. This involved:\n",
    "    - Adding layers for complex multiplication and division.\n",
    "    - Adding layers for seperately finding the $\\hat{h}$ as an expert feature.\n",
    "    - Adding a custom layer that added rbf fast fading \n",
    "- Initially I set the $sigma_m$ to 0.33, for the reasons laid out above, but the model wasn't making any progress training. So after some research I found it written online somewhere that it is often set to $\\frac{1}{\\sqrt{2}}$ to give unity fading gain on average. This seems wrong as you should expect to lose power in the channel. However, upon trying this the model trained vastly more successfully so I may stick with this, or train at a higher SNR if I don't use this.\n",
    "- Going to try debugging it for a while, then I may try using different numbers of layers and activation functions to try and get the two supervised lines in Figures 6a and 6b from the Aoudia and Hoydis paper. After this I can try and get the reinforcement learned lines giving the same results. \n",
    "\n",
    "03/06/2018\n",
    "- Converting the interim report into the introduction and background sections of the final report. Also writing the abstract.\n",
    "\n",
    "##### To do\n",
    "- Work out why the average power constraint is giving a power per symbol of 0.25.\n",
    "- Run overnight my best unsupervised AWGN model and check it looks the same as the spuervised line from 6a, plot it the same etc.\n",
    "\n",
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_likely(posterior_probs):\n",
    "    max_vals = K.max(posterior_probs, axis=1, keepdims=True) \n",
    "    max_vals = K.cast(max_vals, 'float32')\n",
    "    geT = K.greater_equal(posterior_probs, max_vals)\n",
    "    return K.cast(geT, 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_sigma(Eb_N0_db, Rr=None, Rc=None):\n",
    "    assert(not((Rr == None)&(Rc == None)))\n",
    "    if(Rr == None):\n",
    "        Rr = Rc/2.\n",
    "    Eb_N0 = 10.**(Eb_N0_db/10.)\n",
    "    return np.sqrt(1./(2.*Rr*Eb_N0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_set(M, total_size):\n",
    "    t0 = time()\n",
    "    all_one_hot_messages = np.diag(np.ones(M))\n",
    "    perc_train = 0.75\n",
    "    perc_valid = 0.1\n",
    "\n",
    "    ## Making Data Set\n",
    "    multiple = total_size//M\n",
    "    diff = total_size - (multiple * M)\n",
    "\n",
    "    ## Get quotient \n",
    "    ## Converted the array into a list because it is significantly\n",
    "    ## faster\n",
    "    l = []\n",
    "    all_one_hot_messages_lst = all_one_hot_messages.tolist()\n",
    "    for mult in range(multiple):\n",
    "        for i in range(M):\n",
    "            l.append([all_one_hot_messages_lst[i]])\n",
    "    data = np.concatenate(l)\n",
    "\n",
    "    # Add remainder\n",
    "    random_inds = np.random.choice(np.arange(M),size=diff, replace=False)\n",
    "    extra_rows = all_one_hot_messages[random_inds,:]\n",
    "    data = np.concatenate((data, extra_rows), axis=0)\n",
    "    np.random.shuffle(data)\n",
    "    file_path = \"./data/data\"+str(M)+\".npy\"\n",
    "    np.save(file_path, data)\n",
    "    print(f\"Took {time() - t0}s\")\n",
    "    return data, file_path, all_one_hot_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MostLikelySymbol(Layer):\n",
    "    \"\"\"Return the most likely symbol from a softmax input in the\n",
    "    one hot encoded form.\n",
    "\n",
    "    This layer is only active at test time as otherwise it would\n",
    "    stop gradient propogation during training. Also it is useful\n",
    "    to train with a softmax output to encourage a decisive \n",
    "    decision and because it means you can assess confidence.\n",
    "\n",
    "    # Arguments\n",
    "        None\n",
    "\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    \"\"\"\n",
    "\n",
    "    @interfaces.legacy_gaussiannoise_support\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MostLikelySymbol, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        def most_likely():\n",
    "            max_vals = K.max(inputs, axis=1, keepdims=True) \n",
    "            max_vals = K.cast(max_vals, 'float32')\n",
    "            geT = K.greater_equal(inputs, max_vals)\n",
    "            return K.cast(geT, 'float32')            \n",
    "        return K.in_train_phase(inputs, most_likely, training=training)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        base_config = super(MostLikelySymbol, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise2(Layer):\n",
    "    \"\"\"Apply additive zero-centered Gaussian noise at both traning\n",
    "    and test time.\n",
    "\n",
    "    This is useful to mitigate overfitting\n",
    "    (you could see it as a form of random data augmentation).\n",
    "    Gaussian Noise (GS) is a natural choice as corruption process\n",
    "    for real valued inputs.\n",
    "\n",
    "    Unlike the built in GaussianNoise regularisation layer it is \n",
    "    active at both training and test time. \n",
    "\n",
    "    # Arguments\n",
    "        stddev: float, standard deviation of the noise distribution.\n",
    "\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    \"\"\"\n",
    "\n",
    "    @interfaces.legacy_gaussiannoise_support\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super(GaussianNoise2, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        def noised():\n",
    "            return inputs + K.random_normal(shape=K.shape(inputs),\n",
    "                                            mean=0.,\n",
    "                                            stddev=self.stddev)\n",
    "        return K.in_train_phase(noised, noised, training=training)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'stddev': self.stddev}\n",
    "        base_config = super(GaussianNoise2, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RayleighBlockFading_fast(Layer):\n",
    "    \"\"\"\n",
    "    Applies Rayleigh Block (fast) Fading to the input data at both \n",
    "    training and test time.\n",
    "\n",
    "    # Arguments\n",
    "        sigma_m: float, standard deviation of the multiplicative \n",
    "        constant noise distribution.\n",
    "        sigma_a: float, standard deviation of the additive \n",
    "        constant noise distribution.\n",
    "\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma_m, sigma_a, **kwargs):\n",
    "        super(RayleighBlockFading_fast, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.sigma_m = sigma_m\n",
    "        self.sigma_a = sigma_a\n",
    "\n",
    "    def call(self, inputs, training=None):   \n",
    "        def custom_mult(in_,col_sel,h):\n",
    "            tmp = K.tf.multiply(col_sel,h)\n",
    "            tmp_expand = K.expand_dims(tmp,axis=1)\n",
    "            return K.tf.multiply(in_,tmp_expand)\n",
    "        def complex_mult(in_,h):\n",
    "            o1 = K.constant(np.array([1,0]))\n",
    "            o2 = K.constant(np.array([0,1]))\n",
    "            h_swap = K.tf.reverse(h,[1])\n",
    "            in_swap = K.tf.reverse(in_,[2])\n",
    "\n",
    "            t1 = custom_mult(in_,o1,h)          # real*real\n",
    "            t2 = custom_mult(in_swap,o1,h_swap) # imaginary*imaginary\n",
    "            t3 = custom_mult(in_swap,o2,h)      # real*imaginary\n",
    "            t4 = custom_mult(in_,o2,h_swap)     # imaginary*real\n",
    "\n",
    "            total1 = K.tf.math.subtract(t1,t2)  # r1*r2-i1*i2\n",
    "            total2 = K.tf.math.add(t3,t4)       # r1*i2 + i1*r2\n",
    "            return K.tf.math.add(total1,total2) # r1*r2-i1*i2 + r1*i2 + i1*r2\n",
    "        def RBF_fade():\n",
    "            hT = K.random_normal(shape=(K.shape(inputs)[0],K.shape(inputs)[2]),\n",
    "                                 mean=0.,\n",
    "                                 stddev=self.sigma_m)\n",
    "            nT = K.random_normal(shape=K.shape(inputs),\n",
    "                                 mean=0.,\n",
    "                                 stddev=self.sigma_a)\n",
    "            return K.tf.add(complex_mult(inputs,hT),nT)\n",
    "        return K.in_train_phase(RBF_fade, RBF_fade, training=training)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'sigma_m': self.sigma_m, 'sigma_a': self.sigma_a}\n",
    "        base_config = super(RayleighBlockFading_fast, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_shapes(start, end, num_steps):\n",
    "    shapes = [start]\n",
    "    diff = (end-start)/(num_steps-1)\n",
    "    # Always start with a full dense layer\n",
    "    for i in range(1,num_steps):\n",
    "        shapes.append(int(start + i*diff))\n",
    "    return shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_complex_n_layer_lr_tanh_tapering_model(M, R, sigma, \\\n",
    "                                                hl_activation_func, \\\n",
    "                                                ol_activation_func, \\\n",
    "                                                num_layers):\n",
    "    ### Initialising Parameters\n",
    "    k = np.log2(M) # Number of bits needed to represent M \n",
    "                   # messages\n",
    "    Nc = int(round(k/R)) # Number of bit being used to represent\n",
    "                        # channel symbols being used \n",
    "                        # Number of complex channel uses\n",
    "    Nr = Nc*2 # Number of real channel uses\n",
    "\n",
    "    ### Defining Layers\n",
    "    ## TRANSMITTER\n",
    "    tx_shapes = get_layer_shapes(M, Nr, num_layers)\n",
    "    input_message = Input(shape=(M,), name=\"input\")\n",
    "    # Hidden Tx layers\n",
    "    tx1 = Dense(tx_shapes[0],activation=hl_activation_func, \\\n",
    "                name=\"tx1\")(input_message)\n",
    "    for i in range(1,num_layers-1):\n",
    "        tx1 = Dense(tx_shapes[i],activation=hl_activation_func, \\\n",
    "                    name=(\"tx\"+str(i+1)))(tx1)\n",
    "    # Final layer with a different activation function to capture non\n",
    "    # linearity\n",
    "    tx_n = Dense(tx_shapes[-1],activation=ol_activation_func, \\\n",
    "                 name=(\"tx\"+str(num_layers)))(tx1)\n",
    "    # Reshape it to complex channel symbols\n",
    "    tx_complex = Lambda(lambda x : K.reshape(x, (-1,Nc,2)),\n",
    "                       output_shape=(Nc,2), \\\n",
    "                        name=\"tx_reshape\")(tx_n)\n",
    "\n",
    "    # Normalisation Layer\n",
    "    tx_norm = Lambda(lambda x : K.l2_normalize(x,axis=2),\n",
    "                     output_shape=(Nc,2), name=\"tx_norm\")\\\n",
    "                        (tx_complex)\n",
    "    tx_norm_scaled = Lambda(lambda x : K.tf.multiply(np.float32(np.sqrt(Nr)), x),\n",
    "                      output_shape=(Nr,), name=\"tx_norm_scaled\")\\\n",
    "                        (tx_norm)\n",
    "    \n",
    "    # Add Noise \n",
    "    noise = GaussianNoise2(sigma)(tx_norm_scaled)\n",
    "\n",
    "    ## RECIEVER\n",
    "    # Flatten the input\n",
    "    noise_flat = Lambda(lambda x : K.reshape(x, (-1,Nr)),\n",
    "                       output_shape=(Nr,),\\\n",
    "                        name=\"noise_flat\")(noise)\n",
    "    # First layer with the different activation function\n",
    "    # to capture non-linearity and for symmetry with the \n",
    "    # transmitter.\n",
    "    rx1 = Dense(tx_shapes[-2],activation=ol_activation_func, name=\"rx1\")\\\n",
    "                (noise_flat)\n",
    "    # Hidden Rx Layers\n",
    "    if(num_layers >= 3):\n",
    "        layer_ind = -3\n",
    "    else:\n",
    "        layer_ind = -2\n",
    "    rx_i = Dense(tx_shapes[layer_ind],activation=hl_activation_func, \\\n",
    "                name=\"rx2\")(rx1)\n",
    "    for i in range(2,num_layers):\n",
    "        ind = max(0,num_layers - 2 - i)\n",
    "        rx_i = Dense(tx_shapes[ind],activation=hl_activation_func, \\\n",
    "                    name=(\"rx\"+str(i+1)))(rx_i)\n",
    "    # Dense layer with softmax activation\n",
    "    rx_softmax = Dense(tx_shapes[0],activation='softmax', \\\n",
    "                       name=\"rx_softmax\")(rx_i)\n",
    "    \n",
    "    # Select the symbols with the maximum probabilities\n",
    "    ml_symbs = MostLikelySymbol()(rx_softmax)\n",
    "    \n",
    "    ###Defining the models\n",
    "    autoencoder = Model(input_message, rx_softmax)\n",
    "    ## Model the Tx and Rx seperately as well\n",
    "    # Model the Tx\n",
    "    transmitter = Model(input_message, tx_norm_scaled)\n",
    "    # Model the Tx plus the noise\n",
    "    channel_sym_with_noise = Model(input_message, noise)\n",
    "    channel_symbol = Input(shape=(Nr,))\n",
    "    # Take the last layer of the autoencoder model\n",
    "    reciever_layers = autoencoder.layers[-(num_layers+1)](channel_symbol)\n",
    "    for i in range(num_layers):\n",
    "        reciever_layers = autoencoder.layers[-(num_layers-i)](reciever_layers)\n",
    "\n",
    "    # Create a model of the reciever\n",
    "    reciever = Model(channel_symbol, reciever_layers)\n",
    "    autoencoder_symbs = Model(input_message,ml_symbs) \n",
    "    \n",
    "    # Compile the model\n",
    "    autoencoder.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")\n",
    "    return autoencoder, transmitter, reciever,\\\n",
    "            autoencoder_symbs, k, Nc, Nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mult(in_,col_sel,h):\n",
    "    tmp = K.tf.multiply(col_sel,h)\n",
    "    tmp_expand = K.expand_dims(tmp,axis=1)\n",
    "    return K.tf.multiply(in_,tmp_expand)\n",
    "\n",
    "def complex_div(in_lst):\n",
    "    in_,h = in_lst\n",
    "    o1 = K.constant(np.array([1,0]))\n",
    "    o2 = K.constant(np.array([0,1]))\n",
    "    h_swap = K.tf.reverse(h,[1])\n",
    "    in_swap = K.tf.reverse(in_,[2])\n",
    "\n",
    "    t1 = custom_mult(in_,o1,h)          # real*real\n",
    "    t2 = custom_mult(in_swap,o1,h_swap) # imaginary*imaginary\n",
    "    t3 = custom_mult(in_swap,o2,h)      # real*imaginary\n",
    "    t4 = custom_mult(in_,o2,h_swap)     # imaginary*real\n",
    "\n",
    "    total1 = K.tf.math.add(t1,t2)        # xr*hr-xi*hi\n",
    "    total2 = K.tf.math.subtract(t4,t3)   # xr*hi-xi*hr\n",
    "    total = K.tf.math.add(total1,total2) # xr*hr+xi*hi + xr*hi-xi*hr\n",
    "\n",
    "    scale_factor = K.sum(K.tf.math.square(h), axis=1) # hr^2 + hi^2\n",
    "    scale_factor_expanded_twice = K.expand_dims(K.expand_dims(scale_factor, axis=1), axis=2)\n",
    "    return K.tf.math.divide(total,scale_factor_expanded_twice) \n",
    "\n",
    "def complex_mult(in_,h):\n",
    "    o1 = K.constant(np.array([1,0]))\n",
    "    o2 = K.constant(np.array([0,1]))\n",
    "    h_swap = K.tf.reverse(h,[1])\n",
    "    in_swap = K.tf.reverse(in_,[2])\n",
    "\n",
    "    t1 = custom_mult(in_,o1,h)          # real*real\n",
    "    t2 = custom_mult(in_swap,o1,h_swap) # imaginary*imaginary\n",
    "    t3 = custom_mult(in_swap,o2,h)      # real*imaginary\n",
    "    t4 = custom_mult(in_,o2,h_swap)     # imaginary*real\n",
    "\n",
    "    total1 = K.tf.math.subtract(t1,t2)  # r1*r2-i1*i2\n",
    "    total2 = K.tf.math.add(t3,t4)       # r1*i2 + i1*r2\n",
    "    return K.tf.math.add(total1,total2) # r1*r2-i1*i2 + r1*i2 + i1*r2\n",
    "\n",
    "sigma_m = 0.33\n",
    "sigma_a = 1\n",
    "\n",
    "def RBF_fade(in_, sigma_m, sigma_a):\n",
    "    hT = K.random_normal(shape=(K.shape(in_)[0],K.shape(in_)[2]),\n",
    "                         mean=0.,\n",
    "                         stddev=sigma_m)\n",
    "    nT = K.random_normal(shape=K.shape(in_),\n",
    "                         mean=0.,\n",
    "                         stddev=sigma_a)\n",
    "    return K.tf.add(complex_mult(in_,hT),nT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_elements(tensor):\n",
    "    dim_lst = tensor.get_shape().as_list()\n",
    "    num_elements = 1\n",
    "    for dim in dim_lst:\n",
    "        if(dim != None):\n",
    "            num_elements *= dim\n",
    "    return num_elements\n",
    "\n",
    "def get_avg_power(signal):\n",
    "    # Gets the power according to the definition \n",
    "    # on the top of the fraction in Eq 8 the\n",
    "    # Aoudia and Hoydis paper.\n",
    "    assert(len(signal.shape) == 3)\n",
    "    num_elements = get_num_elements(signal)\n",
    "    print(\"num_elements = \", num_elements)\n",
    "    sq = K.tf.math.square(signal)\n",
    "    sq_sum_all = K.sum(K.sum(K.sum(sq,axis=2),axis=1),axis=0)\n",
    "    return K.tf.math.divide(sq_sum_all,K.tf.dtypes.cast(num_elements,tf.float32))\n",
    "\n",
    "def avg_power_normalise(signal):\n",
    "    avg_power = get_avg_power(signal)\n",
    "    return K.tf.math.divide(signal,K.tf.math.sqrt(avg_power))\n",
    "#     return K.tf.math.divide(signal,avg_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_elements =  8000000\n",
      "num_elements =  8000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average power constraint layer\n",
    "B = 1000000\n",
    "Nc = 4\n",
    "tx_T = K.constant(np.random.normal(0,2,(B,Nc,2)))\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    tx_T.eval(session=sess)\n",
    "# tx_T.eval(session=sess)\n",
    "get_avg_power(avg_power_normalise(tx_T)).eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_elements =  3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6666666"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_power(K.constant(np.array([[[0,1,2]]]))).eval(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWGN alternating model has a single dense ReLu layer, I'm going to use the topology I found to be best in the other notebook, two layers, tapered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rbf_autoencoder(k, Nc, sigma_m, sigma_a):\n",
    "#     k = np.log2(M) # Number of bits needed to represent M \n",
    "#                    # messages\n",
    "    M = 2**k\n",
    "    Nr = Nc*2 # Number of real channel uses\n",
    "    \n",
    "    ### Defining Layers\n",
    "    ## TRANSMITTER\n",
    "    tx_input_message = Input(shape=(M,), name=\"input\")\n",
    "    tx1 = keras.layers.Embedding(M, M, name=\"embedding\")(tx_input_message)\n",
    "    tx1 = keras.layers.ELU(alpha=1.0, name=\"embed_ELU\")(tx1)\n",
    "    # Reshape it to complex channel symbols\n",
    "    tx_flat = Reshape((M*M,), name=\"post_embed_flatten\")(tx1)  \n",
    "    tx2 = Dense(Nr,activation=\"linear\", name=\"tx2\")(tx_flat)\n",
    "    # Reshape it to complex channel symbols\n",
    "    tx_complex = Reshape((Nc,2), name=\"tx_reshape\")(tx2)\n",
    "    # Normalisation Layer\n",
    "    tx_norm = Lambda(lambda x : avg_power_normalise(x),\n",
    "                     output_shape=(Nc,2), name=\"tx_norm\")\\\n",
    "                    (tx_complex)\n",
    "    tx_norm_scaled = Lambda(lambda x : K.tf.multiply(np.float32(np.sqrt(32)), x),\n",
    "                      output_shape=(Nc,2), name=\"tx_norm_scaled\")\\\n",
    "                        (tx_norm)\n",
    "    # Add AWGN Noise \n",
    "    noise = RayleighBlockFading_fast(sigma_m,sigma_a)(tx_norm)\n",
    "\n",
    "    ## receiver\n",
    "    # Flatten the input\n",
    "    noise_flat = Reshape((Nr,), name=\"noise_flat\")(noise)\n",
    "    # First layer with the different activation function\n",
    "    # to capture non-linearity and for symmetry with the \n",
    "    # transmitter.\n",
    "    h1 = Dense(20,activation=\"tanh\", name=\"h1\")\\\n",
    "                (noise_flat)\n",
    "    h_hat = Dense(2,activation=\"linear\", name=\"h_hat\")\\\n",
    "                (h1)\n",
    "    y_over_h = Lambda(lambda x : complex_div(x),\n",
    "                      output_shape=(Nc,2),\\\n",
    "                      name=\"y_over_h\")([noise, h_hat])\n",
    "    y_over_h_flat = Reshape((Nr,), name=\"y_over_h_flat\")(y_over_h)\n",
    "    rx1 = Dense(M,activation=\"relu\", name=\"rx1\")\\\n",
    "                (y_over_h_flat)\n",
    "    # Dense layer with softmax activation\n",
    "    rx_softmax = Dense(M,activation='softmax', \\\n",
    "                       name=\"rx_softmax\")(rx1)\n",
    "    # Select the symbols with the maximum probabilities\n",
    "    ml_symbs = MostLikelySymbol()(rx_softmax)\n",
    "\n",
    "    ###Defining the models\n",
    "    # Model the Tx\n",
    "    autoencoder = Model(tx_input_message, rx_softmax)\n",
    "    autoencoder_symbs = Model(tx_input_message,ml_symbs) \n",
    "    transmitter = Model(tx_input_message, tx_norm_scaled)\n",
    "    # Model the Tx plus the noise\n",
    "    channel_sym_with_noise = Model(tx_input_message, noise)\n",
    "    # Model the Rx\n",
    "    channel_symbol = Input(shape=(Nc,2), name=\"rx_in\")\n",
    "    num_layers = 8\n",
    "    receiver_layers = autoencoder.layers[-num_layers](channel_symbol)\n",
    "    for i in range(3):\n",
    "        receiver_layers = autoencoder.layers[-(num_layers-1-i)](receiver_layers)\n",
    "    receiver_layers = autoencoder.layers[-(num_layers-4)]([channel_symbol,receiver_layers])\n",
    "    for i in range(4,num_layers-1):\n",
    "        receiver_layers = autoencoder.layers[-(num_layers-1-i)](receiver_layers)\n",
    "    # Create a model of the receiver\n",
    "    receiver = Model(channel_symbol, receiver_layers)\n",
    "    # Compile the model\n",
    "    autoencoder.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")\n",
    "\n",
    "    return autoencoder, autoencoder_symbs, transmitter, \\\n",
    "        channel_sym_with_noise, receiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_rbf_rx_and_tx_models(M, Nc, sigma_m, sigma_a):\n",
    "    k = np.log2(M) # Number of bits needed to represent M \n",
    "                   # messages\n",
    "    Nr = Nc*2 # Number of real channel uses\n",
    "\n",
    "    ### Defining Layers\n",
    "    ## TRANSMITTER\n",
    "    tx_input_message = Input(shape=(M,), name=\"input\")\n",
    "    tx1 = keras.layers.Embedding(M, M*M)(tx_input_message)\n",
    "    tx1 = keras.layers.ELU(alpha=1.0)(tx1)\n",
    "    tx2 = Dense(Nr,activation=\"linear\", name=\"tx2\")(tx1)\n",
    "\n",
    "    # Reshape it to complex channel symbols\n",
    "    tx_complex = Lambda(lambda x : K.reshape(x, (-1,Nc,2)),\n",
    "                       output_shape=(Nc,2), \\\n",
    "                        name=\"tx_reshape\")(tx2)\n",
    "\n",
    "    # Normalisation Layer\n",
    "    tx_norm = Lambda(lambda x : K.l2_normalize(x,axis=2),\n",
    "                     output_shape=(Nc,2), name=\"tx_norm\")\\\n",
    "                        (tx_complex)\n",
    "    tx_norm_scaled = Lambda(lambda x : K.tf.multiply(np.float32(np.sqrt(Nr)), x),\n",
    "                      output_shape=(Nc,2), name=\"tx_norm_scaled\")\\\n",
    "                        (tx_norm)\n",
    "    # Add AWGN Noise \n",
    "    noise = RayleighBlockFading_fast(sigma_m,sigma_a)(tx_norm_scaled)\n",
    "\n",
    "    ## RECIEVER\n",
    "    rx_input_message = Input(shape=(Nc,2), name=\"input\")\n",
    "    # Flatten the input\n",
    "    noise_flat = Lambda(lambda x : K.reshape(x, (-1,Nr)),\n",
    "                       output_shape=(Nr,),\\\n",
    "                        name=\"noise_flat\")(rx_input_message)\n",
    "    # First layer with the different activation function\n",
    "    # to capture non-linearity and for symmetry with the \n",
    "    # transmitter.\n",
    "    h1 = Dense(20,activation=\"tanh\", name=\"h1\")\\\n",
    "                (noise_flat)\n",
    "    h_hat = Dense(2,activation=\"linear\", name=\"h_hat\")\\\n",
    "                (h1)\n",
    "    h_complex = Lambda(lambda x : K.reshape(x, (-1,1,2)),\n",
    "                       output_shape=(Nc,2),\\\n",
    "                       name=\"h_complex\")(h_hat)\n",
    "    y_over_h = Lambda(lambda x : complex_div(x,h_complex),\n",
    "                      output_shape=(Nc,2),\\\n",
    "                      name=\"y_over_h\")(rx_input_message)\n",
    "    y_over_h_flat = Lambda(lambda x : K.reshape(x, (-1,Nr)),\n",
    "                       output_shape=(Nr,),\\\n",
    "                        name=\"noise_flat\")(y_over_h)\n",
    "    rx1 = Dense(M,activation=\"relu\", name=\"rx1\")\\\n",
    "                (y_over_h_flat)\n",
    "    # Dense layer with softmax activation\n",
    "    rx_softmax = Dense(M,activation='softmax', \\\n",
    "                       name=\"rx_softmax\")(rx1)\n",
    "    # Select the symbols with the maximum probabilities\n",
    "    ml_symbs = MostLikelySymbol()(rx_softmax)\n",
    "\n",
    "    ###Defining the models\n",
    "    # Model the Tx\n",
    "    transmitter = Model(tx_input_message, tx_norm_scaled)\n",
    "    # Model the Tx plus the noise\n",
    "    channel_sym_with_noise = Model(tx_input_message, noise)\n",
    "    # Model the Rx\n",
    "    receiver = Model(rx_input_message,rx_softmax)\n",
    "    receiver_symbs = Model(rx_input_message,ml_symbs)\n",
    "\n",
    "    # Compile the model\n",
    "    transmitter.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")\n",
    "    receiver.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")\n",
    "    return transmitter, channel_sym_with_noise, receiver,\\\n",
    "                receiver_symbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_awgn_rx_and_tx_models(M, Nc, sigma):\n",
    "    k = np.log2(M) # Number of bits needed to represent M \n",
    "                   # messages\n",
    "    Nr = Nc*2 # Number of real channel uses\n",
    "\n",
    "    ### Defining Layers\n",
    "    ## TRANSMITTER\n",
    "    tx_input_message = Input(shape=(M,), name=\"input\")\n",
    "    tx1 = keras.layers.Embedding(M, M*M)(tx_input_message)\n",
    "    tx1 = keras.layers.ELU(alpha=1.0)(tx1)\n",
    "    tx2 = Dense(Nr,activation=\"linear\", name=\"tx2\")(tx1)\n",
    "\n",
    "    # Reshape it to complex channel symbols\n",
    "    tx_complex = Lambda(lambda x : K.reshape(x, (-1,Nc,2)),\n",
    "                       output_shape=(Nc,2), \\\n",
    "                        name=\"tx_reshape\")(tx2)\n",
    "\n",
    "    # Normalisation Layer\n",
    "    tx_norm = Lambda(lambda x : K.l2_normalize(x,axis=2),\n",
    "                     output_shape=(Nc,2), name=\"tx_norm\")\\\n",
    "                        (tx_complex)\n",
    "    tx_norm_scaled = Lambda(lambda x : K.tf.multiply(np.float32(np.sqrt(Nr)), x),\n",
    "                      output_shape=(Nc,2), name=\"tx_norm_scaled\")\\\n",
    "                        (tx_norm)\n",
    "    # Add AWGN Noise \n",
    "    noise = GaussianNoise2(sigma)(tx_norm_scaled)\n",
    "\n",
    "    ## RECIEVER\n",
    "    rx_input_message = Input(shape=(M,), name=\"input\")\n",
    "    # Flatten the input\n",
    "    noise_flat = Lambda(lambda x : K.reshape(x, (-1,Nr)),\n",
    "                       output_shape=(Nr,),\\\n",
    "                        name=\"noise_flat\")(rx_input_message)\n",
    "    # First layer with the different activation function\n",
    "    # to capture non-linearity and for symmetry with the \n",
    "    # transmitter.\n",
    "    rx1 = Dense(M,activation=\"relu\", name=\"rx1\")\\\n",
    "                (noise_flat)\n",
    "    # Dense layer with softmax activation\n",
    "    rx_softmax = Dense(M,activation='softmax', \\\n",
    "                       name=\"rx_softmax\")(rx1)\n",
    "    # Select the symbols with the maximum probabilities\n",
    "    ml_symbs = MostLikelySymbol()(rx_softmax)\n",
    "\n",
    "    ###Defining the models\n",
    "    # Model the Tx\n",
    "    transmitter = Model(tx_input_message, tx_norm_scaled)\n",
    "    # Model the Tx plus the noise\n",
    "    channel_sym_with_noise = Model(tx_input_message, noise)\n",
    "    # Model the Rx\n",
    "    receiver = Model(rx_input_message,rx_softmax)\n",
    "    receiver_symbs = Model(rx_input_message,ml_symbs)\n",
    "\n",
    "    # Compile the model\n",
    "    transmitter.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")\n",
    "    receiver.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")    \n",
    "    return transmitter, channel_sym_with_noise, receiver,\\\n",
    "                receiver_symbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models, Data and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a set of data for a particular M\n",
    "# total_size = 1000000\n",
    "# all_one_hot_messages256 = np.diag(np.ones(256))\n",
    "\n",
    "\n",
    "# # Automatically saves the data for m to a filepath of\n",
    "# # './data/data${M}.npy'\n",
    "# t2 = time()\n",
    "# func_data256, file_path256, all_one_hot_messages256 = get_data_set(256, total_size)\n",
    "# print(f\"Finished 256 in {time()-t2}s\")\n",
    "\n",
    "# # Don't use this function unless it's for a new M, just\n",
    "# # load the data you have calculated other times.\n",
    "# # This makes results more comparible and saves time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data calculated from previous runs\n",
    "all_one_hot_messages256 = np.diag(np.ones(256))\n",
    "data256 = np.load('./data/data256.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data256.shape = (7200000, 256)\n",
      "valid_data256.shape = (800000, 256)\n",
      "test_data256.shape = (2000000, 256)\n"
     ]
    }
   ],
   "source": [
    "# Splitting into training, testing and validation sets\n",
    "train_data256, test_data256 = train_test_split(data256, \\\n",
    "                                         train_size=0.8)\n",
    "train_data256, valid_data256 = train_test_split(train_data256, \\\n",
    "                                         train_size=0.9)\n",
    "print(f\"train_data256.shape = {train_data256.shape}\")\n",
    "print(f\"valid_data256.shape = {valid_data256.shape}\")\n",
    "print(f\"test_data256.shape = {test_data256.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting a supervised model for AWGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    }
   ],
   "source": [
    "# (8,8)\n",
    "M = 2**8 # Number of one hot encoded messages\n",
    "R = 2 # R = k/n_r\n",
    "sigma = get_noise_sigma(7, Rc=R)\n",
    "hl_activation_func = keras.layers.advanced_activations.LeakyReLU()\n",
    "hl_activation_func.__name__ = 'leakyrelu'\n",
    "ol_activation_func = \"tanh\"\n",
    "num_layers = 2\n",
    "\n",
    "autoencoder8_8_tap_2l, transmitter8_8_tap_2l, reciever8_8_tap_2l, \\\n",
    "    autoencoder_symbs8_8_tap_2l, k8_8_tap_2l, Nc8_8_tap_2l, Nr8_8_tap_2l \\\n",
    "    = make_complex_n_layer_lr_tanh_tapering_model(M, R, sigma, \\\n",
    "                                                  hl_activation_func, \\\n",
    "                                                  ol_activation_func, \\\n",
    "                                                  num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Previous 0.1763\n",
    "# autoencoder8_8_tap_2l.fit(train_data256, train_data256,\n",
    "#                        epochs=1000,\n",
    "#                        batch_size=1000,\n",
    "#                        shuffle=True,\n",
    "#                        validation_data=(valid_data256,\n",
    "#                                         valid_data256),\n",
    "#                        callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder8_8_tap_2l.load_weights('./models/autoencoder8_8_tap_2l3.3856e-06.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting a supervised model for RBF\n",
    "\n",
    "The RBF model is trained at an SNR of 20db, because of the normalisation this is a sigma_a of 0.1. Could also set sigma_m to satisfy $2\\sigma_m^2=1$, ($\\sigma_m = \\frac{1}{\\sqrt{2}}$) so that the average fade gain is unity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_elements =  8\n"
     ]
    }
   ],
   "source": [
    "k = 8\n",
    "Nc = 4\n",
    "# sigma_m = 0.33\n",
    "sigma_m = np.sqrt(0.5)\n",
    "sigma_a = 0.1\n",
    "autoencoder_rbf_no_scale, autoencoder_symbs_rbf_no_scale, transmitter_rbf_no_scale, \\\n",
    "    channel_sym_with_noise_rbf_no_scale, receiver_rbf_no_scale \\\n",
    "    = get_rbf_autoencoder(k, Nc, sigma_m, sigma_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_elements =  8\n"
     ]
    }
   ],
   "source": [
    "k = 8\n",
    "Nc = 4\n",
    "# sigma_m = 0.33\n",
    "sigma_m = np.sqrt(0.5)\n",
    "sigma_a = 0.1\n",
    "autoencoder_rbf, autoencoder_symbs_rbf, transmitter_rbf, \\\n",
    "    channel_sym_with_noise_rbf, receiver_rbf \\\n",
    "    = get_rbf_autoencoder(k, Nc, sigma_m, sigma_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_elements =  8\n"
     ]
    }
   ],
   "source": [
    "k = 8\n",
    "Nc = 4\n",
    "# sigma_m = 0.33\n",
    "sigma_m = np.sqrt(0.5)\n",
    "sigma_a = 0.1\n",
    "autoencoder_rbf_ap, autoencoder_symbs_rbf_ap, transmitter_rbf_ap, \\\n",
    "    channel_sym_with_noise_rbf_ap, receiver_rbf_ap, \\\n",
    "    = get_rbf_autoencoder(k, Nc, sigma_m, sigma_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/1000\n",
      "7200000/7200000 [==============================] - 436s 61us/step - loss: 5.5466 - val_loss: 5.5323\n",
      "Epoch 2/1000\n",
      "7200000/7200000 [==============================] - 432s 60us/step - loss: 1.4284 - val_loss: 0.2523\n",
      "Epoch 3/1000\n",
      "7200000/7200000 [==============================] - 433s 60us/step - loss: 0.1950 - val_loss: 0.1531\n",
      "Epoch 4/1000\n",
      "7200000/7200000 [==============================] - 432s 60us/step - loss: 0.1595 - val_loss: 0.1589\n",
      "Epoch 5/1000\n",
      " 160000/7200000 [..............................] - ETA: 7:08 - loss: 0.1512"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-c7bcf93fc5f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                     validation_data=(valid_data256,\n\u001b[1;32m      6\u001b[0m                                      valid_data256),\n\u001b[0;32m----> 7\u001b[0;31m                     callbacks=[es])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder_rbf.fit(train_data256, train_data256,\n",
    "                    epochs=1000,\n",
    "                    batch_size=10000,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(valid_data256,\n",
    "                                     valid_data256),\n",
    "                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/apsw/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 720000 samples, validate on 80000 samples\n",
      "Epoch 1/1000\n",
      " 80000/720000 [==>...........................] - ETA: 10:16 - loss: 5.5763"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-c7bcf93fc5f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     validation_data=(valid_data256,\n\u001b[1;32m      6\u001b[0m                                      valid_data256),\n\u001b[0;32m----> 7\u001b[0;31m                     callbacks=[es])\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder_rbf.fit(train_data256, train_data256,\n",
    "                    epochs=1000,\n",
    "                    batch_size=10000,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(valid_data256,\n",
    "                                     valid_data256),\n",
    "                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/1000\n",
      "7200000/7200000 [==============================] - 429s 60us/step - loss: 5.5486 - val_loss: 5.5461\n",
      "Epoch 2/1000\n",
      " 990000/7200000 [===>..........................] - ETA: 5:59 - loss: 5.5459"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-216-047dcc4f4b47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                     validation_data=(valid_data256,\n\u001b[1;32m      6\u001b[0m                                      valid_data256),\n\u001b[0;32m----> 7\u001b[0;31m                     callbacks=[es])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder_rbf_ap.fit(train_data256, train_data256,\n",
    "                    epochs=1000,\n",
    "                    batch_size=10000,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(valid_data256,\n",
    "                                     valid_data256),\n",
    "                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving\n",
    "# Individual power constraint, sigma_m = 1/sqrt(2)\n",
    "# autoencoder_rbf.save('./models/autoencoder_rbf_0.1589.model')\n",
    "# autoencoder_rbf.save_weights('./models/autoencoder_rbf_0.1589.h5')\n",
    "\n",
    "# autoencoder_rbf_ap.save('./models/autoencoder_rbf_ap_5.5459.model')\n",
    "# autoencoder_rbf_ap.save_weights('./models/autoencoder_rbf_ap_5.5459.h5')\n",
    "\n",
    "\n",
    "\n",
    "### Loading\n",
    "autoencoder_rbf.load_weights('./models/autoencoder_rbf_0.1589.h5', by_name=True)\n",
    "autoencoder_rbf_ap.load_weights('./models/autoencoder_rbf_ap_5.5459.h5', by_name=True)\n",
    "autoencoder_rbf_no_scale.load_weights('./models/autoencoder_rbf_0.1589.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting an  alternating model for AWGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 8\n",
    "Nc = 4\n",
    "sigma = 0.22\n",
    "transmitter, channel_sym_with_noise, \\\n",
    "    receiver, receiver_symbs \\\n",
    "    = get_paper_awgn_rx_and_tx_models(M, Nc, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting an  alternating model for RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 8\n",
    "Nc = 4\n",
    "sigma_m = 0.33\n",
    "sigma_a = 1\n",
    "transmitter, channel_sym_with_noise, \\\n",
    "    receiver, receiver_symbs \\\n",
    "    = get_paper_rbf_rx_and_tx_models(M, Nc, sigma_m, sigma_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Supervised AWGN performance across the SNR range of [-4,0.5,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_to_sigma_a(db):\n",
    "    return 10**(-db/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_block_error_rate(test_data, pred_symbs):\n",
    "    errors = (test_data != pred_symbs)\n",
    "    block_errors = errors.any(axis=1)\n",
    "    return block_errors.sum()/block_errors.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complex_tapered_noise_bler_SNR(M, R, SNR, weights_file_path, \\\n",
    "                                   test_data, hl_activation_func, \\\n",
    "                                   ol_activation_func, num_layers):\n",
    "    ## Get noise std_dev \n",
    "    noise_std = db_to_sigma_a(SNR)   \n",
    "    ## Make new model with loaded weights\n",
    "    autoencoder8_8_tap_nl, _, _, autoencoder_symbs8_8_tap_nl, \\\n",
    "        _, _, _ \\\n",
    "        = make_complex_n_layer_lr_tanh_tapering_model(M, R, noise_std, \\\n",
    "                                                      hl_activation_func, \\\n",
    "                                                      ol_activation_func, \\\n",
    "                                                      num_layers)    \n",
    "    autoencoder8_8_tap_nl.load_weights(weights_file_path, by_name=True)    \n",
    "    ## Check Accuracy on test set\n",
    "    pred_symbs = autoencoder_symbs8_8_tap_nl.predict(test_data)\n",
    "    return get_block_error_rate(test_data, pred_symbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -4.0, i = 1/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0th loop took 51.436500549316406s\n",
      "ratio_db = -3.5, i = 2/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1th loop took 51.906299352645874s\n",
      "ratio_db = -3.0, i = 3/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2th loop took 51.930606842041016s\n",
      "ratio_db = -2.5, i = 4/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3th loop took 52.61579132080078s\n",
      "ratio_db = -2.0, i = 5/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4th loop took 52.799492597579956s\n",
      "ratio_db = -1.5, i = 6/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5th loop took 53.136507511138916s\n",
      "ratio_db = -1.0, i = 7/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6th loop took 53.56024432182312s\n",
      "ratio_db = -0.5, i = 8/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7th loop took 53.93984580039978s\n",
      "ratio_db = 0.0, i = 9/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8th loop took 53.98229122161865s\n",
      "ratio_db = 0.5, i = 10/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9th loop took 54.836410999298096s\n",
      "ratio_db = 1.0, i = 11/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10th loop took 55.04261589050293s\n",
      "ratio_db = 1.5, i = 12/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11th loop took 55.47947311401367s\n",
      "ratio_db = 2.0, i = 13/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12th loop took 55.967275857925415s\n",
      "ratio_db = 2.5, i = 14/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13th loop took 56.49920988082886s\n",
      "ratio_db = 3.0, i = 15/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14th loop took 57.087040424346924s\n",
      "ratio_db = 3.5, i = 16/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "15th loop took 57.46648573875427s\n",
      "ratio_db = 4.0, i = 17/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16th loop took 57.54560446739197s\n",
      "ratio_db = 4.5, i = 18/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "17th loop took 57.83512878417969s\n",
      "ratio_db = 5.0, i = 19/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "18th loop took 58.547157764434814s\n",
      "ratio_db = 5.5, i = 20/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "19th loop took 58.95889925956726s\n",
      "ratio_db = 6.0, i = 21/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20th loop took 59.41124773025513s\n",
      "ratio_db = 6.5, i = 22/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "21th loop took 60.30534219741821s\n",
      "ratio_db = 7.0, i = 23/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "22th loop took 60.44829869270325s\n",
      "ratio_db = 7.5, i = 24/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "23th loop took 60.9995174407959s\n",
      "ratio_db = 8.0, i = 25/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "24th loop took 60.82547616958618s\n",
      "ratio_db = 8.5, i = 26/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "25th loop took 62.30768156051636s\n",
      "ratio_db = 9.0, i = 27/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "26th loop took 61.84466576576233s\n",
      "ratio_db = 9.5, i = 28/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "27th loop took 63.05571937561035s\n",
      "ratio_db = 10.0, i = 29/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28th loop took 67.72161793708801s\n",
      "ratio_db = 10.5, i = 30/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "29th loop took 64.11016845703125s\n",
      "ratio_db = 11.0, i = 31/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "30th loop took 63.89042639732361s\n",
      "ratio_db = 11.5, i = 32/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "31th loop took 65.06354212760925s\n",
      "ratio_db = 12.0, i = 33/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "32th loop took 65.12028694152832s\n",
      "ratio_db = 12.5, i = 34/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "33th loop took 65.20026087760925s\n",
      "ratio_db = 13.0, i = 35/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "34th loop took 66.1613438129425s\n",
      "\n",
      "Took 2047.0448281764984\n"
     ]
    }
   ],
   "source": [
    "# (8,8) tapered, n layers\n",
    "t0 = time()\n",
    "Eb_N0_dbs = np.arange(-4,13.5,0.5)\n",
    "bler = np.empty(Eb_N0_dbs.size)\n",
    "\n",
    "hl_act_f = keras.layers.advanced_activations.LeakyReLU()\n",
    "hl_act_f.__name__ = 'leakyrelu'\n",
    "\n",
    "# for i, ratio_db in enumerate(tqdm_notebook(Eb_N0_dbs, desc=\"1st loop\")):\n",
    "for i, ratio_db in enumerate(Eb_N0_dbs):\n",
    "    print(f\"ratio_db = {ratio_db}, i = {i+1}/{Eb_N0_dbs.size}\", end=\"\\r\")\n",
    "    t1 = time()\n",
    "    bler[i] = get_complex_tapered_noise_bler_SNR(2**8, 2, ratio_db, \\\n",
    "                                                 './models/autoencoder8_8_tap_2l3.3856e-06.h5', \\\n",
    "                                                 test_data256, hl_act_f, \\\n",
    "                                                 \"tanh\", 2)\n",
    "    print(f\"\\n{i}th loop took {time() - t1}s\")\n",
    "print(f\"\\nTook {time() - t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.135625e-01, 5.675505e-01, 5.175005e-01, 4.668345e-01,\n",
       "       4.146005e-01, 3.611685e-01, 3.103660e-01, 2.603355e-01,\n",
       "       2.145900e-01, 1.720750e-01, 1.347600e-01, 1.032520e-01,\n",
       "       7.669500e-02, 5.474800e-02, 3.800300e-02, 2.544200e-02,\n",
       "       1.643800e-02, 1.002400e-02, 5.891500e-03, 3.354000e-03,\n",
       "       1.783500e-03, 8.850000e-04, 4.305000e-04, 1.800000e-04,\n",
       "       7.650000e-05, 2.200000e-05, 9.500000e-06, 4.000000e-06,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "       0.000000e+00, 0.000000e+00, 0.000000e+00])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder_awgn = bler\n",
    "# np.save('./key_results/paper2/autoencoder_awgn.npy', autoencoder_awgn)\n",
    "\n",
    "autoencoder_awgn = np.load('./key_results/paper2/autoencoder_awgn.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 6a - AWGN supervised vs unsupervised across a range of SNRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data for messing with the way the plot looks\n",
    "x = np.arange(-4,13.5,0.5)\n",
    "y = np.exp(-0.1*x**2)\n",
    "z = y*0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(x,autoencoder_awgn,'^-', color=\"orange\", markerfacecolor='none', markersize=4, linewidth=0.5)\n",
    "plt.semilogy(x,z,'D-', color=\"blue\", markerfacecolor='none', markersize=3, linewidth=0.5)\n",
    "plt.xlabel(\"SNR (db)\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.grid()\n",
    "plt.legend([\"Supervised\", \"Alternating\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 6b - RBF supervised vs unsupervised across a range of SNRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rician Fading\n",
    "\n",
    "https://en.wikipedia.org/wiki/Rician_fading\n",
    "<br>\n",
    "\n",
    "https://www.gaussianwaves.com/tag/rician/\n",
    "\n",
    "Also see the document in downloads about it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
