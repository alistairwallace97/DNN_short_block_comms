{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning - Aoudia and Hoydis\n",
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Lambda, ELU, Reshape\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from keras import backend as K\n",
    "from keras.layers import GaussianNoise, advanced_activations\n",
    "from keras.engine.topology import Layer\n",
    "from keras.legacy import interfaces\n",
    "from keras.initializers import Zeros as kZeros\n",
    "from keras.utils import multi_gpu_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from time import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import joblib\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # confirm TensorFlow sees the GPU\n",
    "# from tensorflow.python.client import device_lib\n",
    "# assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# # confirm Keras sees the GPU\n",
    "# from keras import backend\n",
    "# assert len(backend.tensorflow_backend._get_available_gpus()) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is in a seperate box because it isn't running on the \n",
    "# # AWS server. \n",
    "# from tqdm import tqdm_notebook, tnrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Useful guides\n",
    "https://hub.packtpub.com/build-reinforcement-learning-agent-in-keras-tutorial/\n",
    "<br>\n",
    "https://medium.com/ml-everything/policy-based-reinforcement-learning-with-keras-4996015a0b1\n",
    "<br>\n",
    "\n",
    "##### Notes from paper\n",
    "- Loss function = Cross Entropy\n",
    "- Normalisation = Average L2 power constraint = 1. $\\mathbb{E}\\left[\\frac{1}{N}\\left\\Vert x\\right\\Vert_2^2\\right]$ = 1.\n",
    "- Trained at an SNR of 10dB for AWGN and an SNR of 20dB for RBF channel. $\\sigma_{\\pi}^2$ = 0.02 at training time.\n",
    "    - During RL exploration $\\mathbf{x_p} = \\mathbf{x} + \\mathbf{w}$, where each element of $\\mathbf{w}$ is i.i.d ~ $\\mathcal{N}(0,\\sigma_{\\pi}^2)$\n",
    "- M= 256, N=4. N= the number of complex channel uses, so n = 8. Therefore n,k = (8,8)\n",
    "- AWGN -> 1 hidden layer, size M, ReLu activation function\n",
    "- Rayleigh -> see diagram.\n",
    "    - Two layers (Dense(20,tanh)->Dense(2,linear)) calculate an estimate for $\\hat{h}$, then we divide the received signal by $\\hat{h}$ then have two layers for finding the received signal. Dense(M,ReLu) then Dense(M,Softmax). Then have the select maximum likelihood symbol layer.\n",
    "- SNR $ = \\frac{\\mathbb{E}\\left[\\frac{1}{N}\\left\\Vert x\\right\\Vert_2^2\\right]}{\\sigma^2}$, but because of the normalisation this is $\\frac{1}{\\sigma^2}$.\n",
    "- The RBF seems to be slow fading, but I could check for fast fading as well. Rayleigh fading is of the form $\\mathbf{r} = \\mathbf{s}*\\mathbf{h} + \\mathbf{n}$ where $\\mathbf{h}$ ~ $\\mathcal{N}(0,\\sigma_m^2)$, $\\mathbf{n}$ ~ $\\mathcal{N}(0,\\sigma_a^2)$. \n",
    "    - Slow fading: h stays the same across the whole minibatch\n",
    "    - Fast fading: h changes every sample\n",
    "- I'm going to guess that $\\sigma_m ~ \\frac{1}{3}$. Because this means that 98% of the time it's less than 1. Which sounds alright.\n",
    "- I could also try Ricean fading and see how the model performs against that.\n",
    "- Ricean fading would actually be the most general, as then it could learn Rayleigh fading, and it could also learn AWGN just by learning to make $\\hat{h}$ every time.\n",
    "\n",
    "##### Work Log\n",
    "\n",
    "27/05/2019\n",
    "- Started researching RBF, think I can implement it in a custom layer.\n",
    "- Made lots of notes\n",
    "- Copied over functions from other notebook.\n",
    "\n",
    "28/05/2019\n",
    "- Made functions for making the Tx and Rx blocks for the unsupervised learning, however these will likely need to be edited.\n",
    "\n",
    "\n",
    "29/05/2019\n",
    "- Got food poisoning the night before and did no work.\n",
    "\n",
    "31/05/2019\n",
    "- Got the rbf supervised model fitting, but with no good results. \n",
    "- Copied the architecture from the paper exactly, however it was not effective. This involved:\n",
    "    - Adding layers for complex multiplication and division.\n",
    "    - Adding layers for seperately finding the $\\hat{h}$ as an expert feature.\n",
    "    - Adding a custom layer that added rbf fast fading \n",
    "- Initially I set the $sigma_m$ to 0.33, for the reasons laid out above, but the model wasn't making any progress training. So after some research I found it written online somewhere that it is often set to $\\frac{1}{\\sqrt{2}}$ to give unity fading gain on average. This seems wrong as you should expect to lose power in the channel. However, upon trying this the model trained vastly more successfully so I may stick with this, or train at a higher SNR if I don't use this.\n",
    "- Going to try debugging it for a while, then I may try using different numbers of layers and activation functions to try and get the two supervised lines in Figures 6a and 6b from the Aoudia and Hoydis paper. After this I can try and get the reinforcement learned lines giving the same results. \n",
    "\n",
    "03/06/2019\n",
    "- Converting the interim report into the introduction and background sections of the final report. Also writing the abstract.\n",
    "\n",
    "04/06/2019\n",
    "- Fixed the average power normalisation\n",
    "- Continued converting the interim report into the start of a final report.\n",
    "- Found that the model had very significant gains if I trained it with $sigma_a = 0$, investigating whether this could be an effective way of training the $\\hat{h}$ estimation section of the model more quickly so it could then go on to learning the additive noise section later. However this is not how it would be able to work in a normal channel.\n",
    "    - On this subject it was found that the model hardly learnt anything in an RBF channel at 20db of SNR. So 40db was trialled which was more successful, but still trained extremely slowly. This is how no noise was trielled, which gave extremely fast reduction of validation loss.\n",
    "    \n",
    "05/06/2019\n",
    "- Got supervised AWGN graph for a range of SNRs.\n",
    "- Converted interim report into the first two sections of the final report. Adapted Implementation Plan, Evaluation Plan sections and moved Safety, Ethical and Legal plan into the appendix.\n",
    "- Trained supervised_rbf_sa40 for 100 epochs locally.\n",
    "\n",
    "06/06/2019\n",
    "- blah, blah\n",
    "\n",
    "##### To do\n",
    "- Try running the best supervised_rbf_sa40 model over a range of SNRs to see what kind of curve it gives.\n",
    "- Investigate BCH codes, see how long I think it would take to implement one. \n",
    "- Look at a t-SNE reduced constellation diagram for the supervised_rbf_sa40 model to try and see if it has multiple symbols converging to single points.\n",
    "\n",
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_likely(posterior_probs):\n",
    "    max_vals = K.max(posterior_probs, axis=1, keepdims=True) \n",
    "    max_vals = K.cast(max_vals, 'float32')\n",
    "    geT = K.greater_equal(posterior_probs, max_vals)\n",
    "    return K.cast(geT, 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_sigma(Eb_N0_db, Rr=None, Rc=None):\n",
    "    assert(not((Rr == None)&(Rc == None)))\n",
    "    if(Rr == None):\n",
    "        Rr = Rc/2.\n",
    "    Eb_N0 = 10.**(Eb_N0_db/10.)\n",
    "    return np.sqrt(1./(2.*Rr*Eb_N0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_set(M, total_size):\n",
    "    t0 = time()\n",
    "    all_one_hot_messages = np.diag(np.ones(M))\n",
    "    perc_train = 0.75\n",
    "    perc_valid = 0.1\n",
    "\n",
    "    ## Making Data Set\n",
    "    multiple = total_size//M\n",
    "    diff = total_size - (multiple * M)\n",
    "\n",
    "    ## Get quotient \n",
    "    ## Converted the array into a list because it is significantly\n",
    "    ## faster\n",
    "    l = []\n",
    "    all_one_hot_messages_lst = all_one_hot_messages.tolist()\n",
    "    for mult in range(multiple):\n",
    "        for i in range(M):\n",
    "            l.append([all_one_hot_messages_lst[i]])\n",
    "    data = np.concatenate(l)\n",
    "\n",
    "    # Add remainder\n",
    "    random_inds = np.random.choice(np.arange(M),size=diff, replace=False)\n",
    "    extra_rows = all_one_hot_messages[random_inds,:]\n",
    "    data = np.concatenate((data, extra_rows), axis=0)\n",
    "    np.random.shuffle(data)\n",
    "    file_path = \"./data/data\"+str(M)+\".npy\"\n",
    "    np.save(file_path, data)\n",
    "    print(f\"Took {time() - t0}s\")\n",
    "    return data, file_path, all_one_hot_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MostLikelySymbol(Layer):\n",
    "    \"\"\"Return the most likely symbol from a softmax input in the\n",
    "    one hot encoded form.\n",
    "\n",
    "    This layer is only active at test time as otherwise it would\n",
    "    stop gradient propogation during training. Also it is useful\n",
    "    to train with a softmax output to encourage a decisive \n",
    "    decision and because it means you can assess confidence.\n",
    "\n",
    "    # Arguments\n",
    "        None\n",
    "\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    \"\"\"\n",
    "\n",
    "    @interfaces.legacy_gaussiannoise_support\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MostLikelySymbol, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        def most_likely():\n",
    "            max_vals = K.max(inputs, axis=1, keepdims=True) \n",
    "            max_vals = K.cast(max_vals, 'float32')\n",
    "            geT = K.greater_equal(inputs, max_vals)\n",
    "            return K.cast(geT, 'float32')            \n",
    "        return K.in_train_phase(inputs, most_likely, training=training)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        base_config = super(MostLikelySymbol, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise2(Layer):\n",
    "    \"\"\"Apply additive zero-centered Gaussian noise at both traning\n",
    "    and test time.\n",
    "\n",
    "    This is useful to mitigate overfitting\n",
    "    (you could see it as a form of random data augmentation).\n",
    "    Gaussian Noise (GS) is a natural choice as corruption process\n",
    "    for real valued inputs.\n",
    "\n",
    "    Unlike the built in GaussianNoise regularisation layer it is \n",
    "    active at both training and test time. \n",
    "\n",
    "    # Arguments\n",
    "        stddev: float, standard deviation of the noise distribution.\n",
    "\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    \"\"\"\n",
    "\n",
    "    @interfaces.legacy_gaussiannoise_support\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super(GaussianNoise2, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        def noised():\n",
    "            return inputs + K.random_normal(shape=K.shape(inputs),\n",
    "                                            mean=0.,\n",
    "                                            stddev=self.stddev)\n",
    "        return K.in_train_phase(noised, noised, training=training)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'stddev': self.stddev}\n",
    "        base_config = super(GaussianNoise2, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RayleighBlockFading_fast(Layer):\n",
    "    \"\"\"\n",
    "    Applies Rayleigh Block (fast) Fading to the input data at both \n",
    "    training and test time.\n",
    "\n",
    "    # Arguments\n",
    "        sigma_m: float, standard deviation of the multiplicative \n",
    "        constant noise distribution.\n",
    "        sigma_a: float, standard deviation of the additive \n",
    "        constant noise distribution.\n",
    "\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma_m, sigma_a, **kwargs):\n",
    "        super(RayleighBlockFading_fast, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.sigma_m = sigma_m\n",
    "        self.sigma_a = sigma_a\n",
    "\n",
    "    def call(self, inputs, training=None):   \n",
    "        def custom_mult(in_,col_sel,h):\n",
    "            tmp = K.tf.multiply(col_sel,h)\n",
    "            tmp_expand = K.expand_dims(tmp,axis=1)\n",
    "            return K.tf.multiply(in_,tmp_expand)\n",
    "        def complex_mult(in_,h):\n",
    "            o1 = K.constant(np.array([1,0]))\n",
    "            o2 = K.constant(np.array([0,1]))\n",
    "            h_swap = K.tf.reverse(h,[1])\n",
    "            in_swap = K.tf.reverse(in_,[2])\n",
    "\n",
    "            t1 = custom_mult(in_,o1,h)          # real*real\n",
    "            t2 = custom_mult(in_swap,o1,h_swap) # imaginary*imaginary\n",
    "            t3 = custom_mult(in_swap,o2,h)      # real*imaginary\n",
    "            t4 = custom_mult(in_,o2,h_swap)     # imaginary*real\n",
    "\n",
    "            total1 = K.tf.math.subtract(t1,t2)  # r1*r2-i1*i2\n",
    "            total2 = K.tf.math.add(t3,t4)       # r1*i2 + i1*r2\n",
    "            return K.tf.math.add(total1,total2) # r1*r2-i1*i2 + r1*i2 + i1*r2\n",
    "        def RBF_fade():\n",
    "            hT = K.random_normal(shape=(K.shape(inputs)[0],K.shape(inputs)[2]),\n",
    "                                 mean=0.,\n",
    "                                 stddev=self.sigma_m)\n",
    "            nT = K.random_normal(shape=K.shape(inputs),\n",
    "                                 mean=0.,\n",
    "                                 stddev=self.sigma_a)\n",
    "            return K.tf.add(complex_mult(inputs,hT),nT)\n",
    "        return K.in_train_phase(RBF_fade, RBF_fade, training=training)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'sigma_m': self.sigma_m, 'sigma_a': self.sigma_a}\n",
    "        base_config = super(RayleighBlockFading_fast, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_shapes(start, end, num_steps):\n",
    "    shapes = [start]\n",
    "    diff = (end-start)/(num_steps-1)\n",
    "    # Always start with a full dense layer\n",
    "    for i in range(1,num_steps):\n",
    "        shapes.append(int(start + i*diff))\n",
    "    return shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_complex_n_layer_lr_tanh_tapering_model(M, R, sigma, \\\n",
    "                                                hl_activation_func, \\\n",
    "                                                ol_activation_func, \\\n",
    "                                                num_layers):\n",
    "    ### Initialising Parameters\n",
    "    k = np.log2(M) # Number of bits needed to represent M \n",
    "                   # messages\n",
    "    Nc = int(round(k/R)) # Number of bit being used to represent\n",
    "                        # channel symbols being used \n",
    "                        # Number of complex channel uses\n",
    "    Nr = Nc*2 # Number of real channel uses\n",
    "\n",
    "    ### Defining Layers\n",
    "    ## TRANSMITTER\n",
    "    tx_shapes = get_layer_shapes(M, Nr, num_layers)\n",
    "    input_message = Input(shape=(M,), name=\"input\")\n",
    "    # Hidden Tx layers\n",
    "    tx1 = Dense(tx_shapes[0],activation=hl_activation_func, \\\n",
    "                name=\"tx1\")(input_message)\n",
    "    for i in range(1,num_layers-1):\n",
    "        tx1 = Dense(tx_shapes[i],activation=hl_activation_func, \\\n",
    "                    name=(\"tx\"+str(i+1)))(tx1)\n",
    "    # Final layer with a different activation function to capture non\n",
    "    # linearity\n",
    "    tx_n = Dense(tx_shapes[-1],activation=ol_activation_func, \\\n",
    "                 name=(\"tx\"+str(num_layers)))(tx1)\n",
    "    # Reshape it to complex channel symbols\n",
    "    tx_complex = Lambda(lambda x : K.reshape(x, (-1,Nc,2)),\n",
    "                       output_shape=(Nc,2), \\\n",
    "                        name=\"tx_reshape\")(tx_n)\n",
    "\n",
    "    # Normalisation Layer\n",
    "    tx_norm = Lambda(lambda x : K.l2_normalize(x,axis=2),\n",
    "                     output_shape=(Nc,2), name=\"tx_norm\")\\\n",
    "                        (tx_complex)\n",
    "    tx_norm_scaled = Lambda(lambda x : K.tf.multiply(np.float32(np.sqrt(Nr)), x),\n",
    "                      output_shape=(Nr,), name=\"tx_norm_scaled\")\\\n",
    "                        (tx_norm)\n",
    "    \n",
    "    # Add Noise \n",
    "    noise = GaussianNoise2(sigma)(tx_norm_scaled)\n",
    "\n",
    "    ## RECIEVER\n",
    "    # Flatten the input\n",
    "    noise_flat = Lambda(lambda x : K.reshape(x, (-1,Nr)),\n",
    "                       output_shape=(Nr,),\\\n",
    "                        name=\"noise_flat\")(noise)\n",
    "    # First layer with the different activation function\n",
    "    # to capture non-linearity and for symmetry with the \n",
    "    # transmitter.\n",
    "    rx1 = Dense(tx_shapes[-2],activation=ol_activation_func, name=\"rx1\")\\\n",
    "                (noise_flat)\n",
    "    # Hidden Rx Layers\n",
    "    if(num_layers >= 3):\n",
    "        layer_ind = -3\n",
    "    else:\n",
    "        layer_ind = -2\n",
    "    rx_i = Dense(tx_shapes[layer_ind],activation=hl_activation_func, \\\n",
    "                name=\"rx2\")(rx1)\n",
    "    for i in range(2,num_layers):\n",
    "        ind = max(0,num_layers - 2 - i)\n",
    "        rx_i = Dense(tx_shapes[ind],activation=hl_activation_func, \\\n",
    "                    name=(\"rx\"+str(i+1)))(rx_i)\n",
    "    # Dense layer with softmax activation\n",
    "    rx_softmax = Dense(tx_shapes[0],activation='softmax', \\\n",
    "                       name=\"rx_softmax\")(rx_i)\n",
    "    \n",
    "    # Select the symbols with the maximum probabilities\n",
    "    ml_symbs = MostLikelySymbol()(rx_softmax)\n",
    "    \n",
    "    ###Defining the models\n",
    "    autoencoder = Model(input_message, rx_softmax)\n",
    "    ## Model the Tx and Rx seperately as well\n",
    "    # Model the Tx\n",
    "    transmitter = Model(input_message, tx_norm_scaled)\n",
    "    # Model the Tx plus the noise\n",
    "    channel_sym_with_noise = Model(input_message, noise)\n",
    "    channel_symbol = Input(shape=(Nr,))\n",
    "    # Take the last layer of the autoencoder model\n",
    "    reciever_layers = autoencoder.layers[-(num_layers+1)](channel_symbol)\n",
    "    for i in range(num_layers):\n",
    "        reciever_layers = autoencoder.layers[-(num_layers-i)](reciever_layers)\n",
    "\n",
    "    # Create a model of the reciever\n",
    "    reciever = Model(channel_symbol, reciever_layers)\n",
    "    autoencoder_symbs = Model(input_message,ml_symbs) \n",
    "    \n",
    "    # Compile the model\n",
    "    autoencoder.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")\n",
    "    return autoencoder, transmitter, reciever,\\\n",
    "            autoencoder_symbs, k, Nc, Nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mult(in_,col_sel,h):\n",
    "    tmp = K.tf.multiply(col_sel,h)\n",
    "    tmp_expand = K.expand_dims(tmp,axis=1)\n",
    "    return K.tf.multiply(in_,tmp_expand)\n",
    "\n",
    "def complex_div(in_lst):\n",
    "    in_,h = in_lst\n",
    "    o1 = K.constant(np.array([1,0]))\n",
    "    o2 = K.constant(np.array([0,1]))\n",
    "    h_swap = K.tf.reverse(h,[1])\n",
    "    in_swap = K.tf.reverse(in_,[2])\n",
    "\n",
    "    t1 = custom_mult(in_,o1,h)          # real*real\n",
    "    t2 = custom_mult(in_swap,o1,h_swap) # imaginary*imaginary\n",
    "    t3 = custom_mult(in_swap,o2,h)      # real*imaginary\n",
    "    t4 = custom_mult(in_,o2,h_swap)     # imaginary*real\n",
    "\n",
    "    total1 = K.tf.math.add(t1,t2)        # xr*hr-xi*hi\n",
    "    total2 = K.tf.math.subtract(t4,t3)   # xr*hi-xi*hr\n",
    "    total = K.tf.math.add(total1,total2) # xr*hr+xi*hi + xr*hi-xi*hr\n",
    "\n",
    "    scale_factor = K.sum(K.tf.math.square(h), axis=1) # hr^2 + hi^2\n",
    "    scale_factor_expanded_twice = K.expand_dims(K.expand_dims(scale_factor, axis=1), axis=2)\n",
    "    return K.tf.math.divide(total,scale_factor_expanded_twice) \n",
    "\n",
    "def complex_mult(in_,h):\n",
    "    o1 = K.constant(np.array([1,0]))\n",
    "    o2 = K.constant(np.array([0,1]))\n",
    "    h_swap = K.tf.reverse(h,[1])\n",
    "    in_swap = K.tf.reverse(in_,[2])\n",
    "\n",
    "    t1 = custom_mult(in_,o1,h)          # real*real\n",
    "    t2 = custom_mult(in_swap,o1,h_swap) # imaginary*imaginary\n",
    "    t3 = custom_mult(in_swap,o2,h)      # real*imaginary\n",
    "    t4 = custom_mult(in_,o2,h_swap)     # imaginary*real\n",
    "\n",
    "    total1 = K.tf.math.subtract(t1,t2)  # r1*r2-i1*i2\n",
    "    total2 = K.tf.math.add(t3,t4)       # r1*i2 + i1*r2\n",
    "    return K.tf.math.add(total1,total2) # r1*r2-i1*i2 + r1*i2 + i1*r2\n",
    "\n",
    "sigma_m = 0.33\n",
    "sigma_a = 1\n",
    "\n",
    "def RBF_fade(in_, sigma_m, sigma_a):\n",
    "    hT = K.random_normal(shape=(K.shape(in_)[0],K.shape(in_)[2]),\n",
    "                         mean=0.,\n",
    "                         stddev=sigma_m)\n",
    "    nT = K.random_normal(shape=K.shape(in_),\n",
    "                         mean=0.,\n",
    "                         stddev=sigma_a)\n",
    "    return K.tf.add(complex_mult(in_,hT),nT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_elements(tensor):\n",
    "    dim_lst = tensor.get_shape().as_list()\n",
    "    num_elements = 1\n",
    "    for dim in dim_lst:\n",
    "        if(dim != None):\n",
    "            num_elements *= dim\n",
    "    return num_elements\n",
    "\n",
    "def get_avg_power(signal):\n",
    "    # Gets the power according to the definition \n",
    "    # on the top of the fraction in Eq 8 the\n",
    "    # Aoudia and Hoydis paper.\n",
    "    assert(len(signal.shape) == 3)\n",
    "    num_elements = get_num_elements(signal)\n",
    "    sq = K.tf.math.square(signal)\n",
    "    sq_sum_all = K.sum(K.sum(K.sum(sq,axis=2),axis=1),axis=0)\n",
    "    return K.tf.math.divide(sq_sum_all,K.tf.dtypes.cast(num_elements,tf.float32))\n",
    "\n",
    "def avg_power_normalise(signal):\n",
    "    avg_power = get_avg_power(signal)\n",
    "    return K.tf.math.divide(signal,K.tf.math.sqrt(avg_power))\n",
    "#     return K.tf.math.divide(signal,avg_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_elements =  8000000\n",
      "num_elements =  8000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0000002"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average power constraint layer\n",
    "B = 1000000\n",
    "Nc = 4\n",
    "tx_T = K.constant(np.random.normal(0,2,(B,Nc,2)))\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    tx_T.eval(session=sess)\n",
    "# tx_T.eval(session=sess)\n",
    "get_avg_power(avg_power_normalise(tx_T)).eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_elements =  3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6666666"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_power(K.constant(np.array([[[0,1,2]]]))).eval(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWGN alternating model has a single dense ReLu layer, I'm going to use the topology I found to be best in the other notebook, two layers, tapered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rbf_autoencoder(k, Nc, sigma_m, sigma_a):\n",
    "#     k = np.log2(M) # Number of bits needed to represent M \n",
    "#                    # messages\n",
    "    M = 2**k\n",
    "    Nr = Nc*2 # Number of real channel uses\n",
    "    \n",
    "    ### Defining Layers\n",
    "    ## TRANSMITTER\n",
    "    tx_input_message = Input(shape=(M,), name=\"input\")\n",
    "    tx1 = keras.layers.Embedding(M, M, name=\"embedding\")(tx_input_message)\n",
    "    tx1 = keras.layers.ELU(alpha=1.0, name=\"embed_ELU\")(tx1)\n",
    "    # Reshape it to complex channel symbols\n",
    "    tx_flat = Reshape((M*M,), name=\"post_embed_flatten\")(tx1)  \n",
    "    tx2 = Dense(Nr,activation=\"linear\", name=\"tx2\")(tx_flat)\n",
    "    # Reshape it to complex channel symbols\n",
    "    tx_complex = Reshape((Nc,2), name=\"tx_reshape\")(tx2)\n",
    "    # Normalisation Layer\n",
    "    tx_norm = Lambda(lambda x : avg_power_normalise(x),\n",
    "                     output_shape=(Nc,2), name=\"tx_norm\")\\\n",
    "                    (tx_complex)\n",
    "    tx_norm_scaled = Lambda(lambda x : K.tf.multiply(np.float32(np.sqrt(32)), x),\n",
    "                      output_shape=(Nc,2), name=\"tx_norm_scaled\")\\\n",
    "                        (tx_norm)\n",
    "    # Add AWGN Noise \n",
    "    noise = RayleighBlockFading_fast(sigma_m,sigma_a)(tx_norm)\n",
    "\n",
    "    ## receiver\n",
    "    # Flatten the input\n",
    "    noise_flat = Reshape((Nr,), name=\"noise_flat\")(noise)\n",
    "    # First layer with the different activation function\n",
    "    # to capture non-linearity and for symmetry with the \n",
    "    # transmitter.\n",
    "    h1 = Dense(20,activation=\"tanh\", name=\"h1\")\\\n",
    "                (noise_flat)\n",
    "    h_hat = Dense(2,activation=\"linear\", name=\"h_hat\")\\\n",
    "                (h1)\n",
    "    y_over_h = Lambda(lambda x : complex_div(x),\n",
    "                      output_shape=(Nc,2),\\\n",
    "                      name=\"y_over_h\")([noise, h_hat])\n",
    "    y_over_h_flat = Reshape((Nr,), name=\"y_over_h_flat\")(y_over_h)\n",
    "    rx1 = Dense(M,activation=\"relu\", name=\"rx1\")\\\n",
    "                (y_over_h_flat)\n",
    "    # Dense layer with softmax activation\n",
    "    rx_softmax = Dense(M,activation='softmax', \\\n",
    "                       name=\"rx_softmax\")(rx1)\n",
    "    # Select the symbols with the maximum probabilities\n",
    "    ml_symbs = MostLikelySymbol()(rx_softmax)\n",
    "\n",
    "    ###Defining the models\n",
    "    # Model the Tx\n",
    "    autoencoder = Model(tx_input_message, rx_softmax)\n",
    "    autoencoder_symbs = Model(tx_input_message,ml_symbs) \n",
    "    transmitter = Model(tx_input_message, tx_norm_scaled)\n",
    "    # Model the Tx plus the noise\n",
    "    channel_sym_with_noise = Model(tx_input_message, noise)\n",
    "    # Model the Rx\n",
    "    channel_symbol = Input(shape=(Nc,2), name=\"rx_in\")\n",
    "    num_layers = 8\n",
    "    receiver_layers = autoencoder.layers[-num_layers](channel_symbol)\n",
    "    for i in range(3):\n",
    "        receiver_layers = autoencoder.layers[-(num_layers-1-i)](receiver_layers)\n",
    "    receiver_layers = autoencoder.layers[-(num_layers-4)]([channel_symbol,receiver_layers])\n",
    "    for i in range(4,num_layers-1):\n",
    "        receiver_layers = autoencoder.layers[-(num_layers-1-i)](receiver_layers)\n",
    "    # Create a model of the receiver\n",
    "    receiver = Model(channel_symbol, receiver_layers)\n",
    "    # Compile the model\n",
    "    autoencoder.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")\n",
    "\n",
    "    return autoencoder, autoencoder_symbs, transmitter, \\\n",
    "        channel_sym_with_noise, receiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_rbf_rx_and_tx_models(M, Nc, sigma_m, sigma_a):\n",
    "    k = np.log2(M) # Number of bits needed to represent M \n",
    "                   # messages\n",
    "    Nr = Nc*2 # Number of real channel uses\n",
    "\n",
    "    ### Defining Layers\n",
    "    ## TRANSMITTER\n",
    "    tx_input_message = Input(shape=(M,), name=\"input\")\n",
    "    tx1 = keras.layers.Embedding(M, M*M)(tx_input_message)\n",
    "    tx1 = keras.layers.ELU(alpha=1.0)(tx1)\n",
    "    tx2 = Dense(Nr,activation=\"linear\", name=\"tx2\")(tx1)\n",
    "\n",
    "    # Reshape it to complex channel symbols\n",
    "    tx_complex = Lambda(lambda x : K.reshape(x, (-1,Nc,2)),\n",
    "                       output_shape=(Nc,2), \\\n",
    "                        name=\"tx_reshape\")(tx2)\n",
    "\n",
    "    # Normalisation Layer\n",
    "    tx_norm = Lambda(lambda x : K.l2_normalize(x,axis=2),\n",
    "                     output_shape=(Nc,2), name=\"tx_norm\")\\\n",
    "                        (tx_complex)\n",
    "    tx_norm_scaled = Lambda(lambda x : K.tf.multiply(np.float32(np.sqrt(Nr)), x),\n",
    "                      output_shape=(Nc,2), name=\"tx_norm_scaled\")\\\n",
    "                        (tx_norm)\n",
    "    # Add AWGN Noise \n",
    "    noise = RayleighBlockFading_fast(sigma_m,sigma_a)(tx_norm_scaled)\n",
    "\n",
    "    ## RECIEVER\n",
    "    rx_input_message = Input(shape=(Nc,2), name=\"input\")\n",
    "    # Flatten the input\n",
    "    noise_flat = Lambda(lambda x : K.reshape(x, (-1,Nr)),\n",
    "                       output_shape=(Nr,),\\\n",
    "                        name=\"noise_flat\")(rx_input_message)\n",
    "    # First layer with the different activation function\n",
    "    # to capture non-linearity and for symmetry with the \n",
    "    # transmitter.\n",
    "    h1 = Dense(20,activation=\"tanh\", name=\"h1\")\\\n",
    "                (noise_flat)\n",
    "    h_hat = Dense(2,activation=\"linear\", name=\"h_hat\")\\\n",
    "                (h1)\n",
    "    h_complex = Lambda(lambda x : K.reshape(x, (-1,1,2)),\n",
    "                       output_shape=(Nc,2),\\\n",
    "                       name=\"h_complex\")(h_hat)\n",
    "    y_over_h = Lambda(lambda x : complex_div(x,h_complex),\n",
    "                      output_shape=(Nc,2),\\\n",
    "                      name=\"y_over_h\")(rx_input_message)\n",
    "    y_over_h_flat = Lambda(lambda x : K.reshape(x, (-1,Nr)),\n",
    "                       output_shape=(Nr,),\\\n",
    "                        name=\"noise_flat\")(y_over_h)\n",
    "    rx1 = Dense(M,activation=\"relu\", name=\"rx1\")\\\n",
    "                (y_over_h_flat)\n",
    "    # Dense layer with softmax activation\n",
    "    rx_softmax = Dense(M,activation='softmax', \\\n",
    "                       name=\"rx_softmax\")(rx1)\n",
    "    # Select the symbols with the maximum probabilities\n",
    "    ml_symbs = MostLikelySymbol()(rx_softmax)\n",
    "\n",
    "    ###Defining the models\n",
    "    # Model the Tx\n",
    "    transmitter = Model(tx_input_message, tx_norm_scaled)\n",
    "    # Model the Tx plus the noise\n",
    "    channel_sym_with_noise = Model(tx_input_message, noise)\n",
    "    # Model the Rx\n",
    "    receiver = Model(rx_input_message,rx_softmax)\n",
    "    receiver_symbs = Model(rx_input_message,ml_symbs)\n",
    "\n",
    "    # Compile the model\n",
    "    transmitter.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")\n",
    "    receiver.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")\n",
    "    return transmitter, channel_sym_with_noise, receiver,\\\n",
    "                receiver_symbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_awgn_rx_and_tx_models(M, Nc, sigma):\n",
    "    k = np.log2(M) # Number of bits needed to represent M \n",
    "                   # messages\n",
    "    Nr = Nc*2 # Number of real channel uses\n",
    "\n",
    "    ### Defining Layers\n",
    "    ## TRANSMITTER\n",
    "    tx_input_message = Input(shape=(M,), name=\"input\")\n",
    "    tx1 = keras.layers.Embedding(M, M*M)(tx_input_message)\n",
    "    tx1 = keras.layers.ELU(alpha=1.0)(tx1)\n",
    "    tx2 = Dense(Nr,activation=\"linear\", name=\"tx2\")(tx1)\n",
    "\n",
    "    # Reshape it to complex channel symbols\n",
    "    tx_complex = Lambda(lambda x : K.reshape(x, (-1,Nc,2)),\n",
    "                       output_shape=(Nc,2), \\\n",
    "                        name=\"tx_reshape\")(tx2)\n",
    "\n",
    "    # Normalisation Layer\n",
    "    tx_norm = Lambda(lambda x : K.l2_normalize(x,axis=2),\n",
    "                     output_shape=(Nc,2), name=\"tx_norm\")\\\n",
    "                        (tx_complex)\n",
    "    tx_norm_scaled = Lambda(lambda x : K.tf.multiply(np.float32(np.sqrt(Nr)), x),\n",
    "                      output_shape=(Nc,2), name=\"tx_norm_scaled\")\\\n",
    "                        (tx_norm)\n",
    "    # Add AWGN Noise \n",
    "    noise = GaussianNoise2(sigma)(tx_norm_scaled)\n",
    "\n",
    "    ## RECIEVER\n",
    "    rx_input_message = Input(shape=(M,), name=\"input\")\n",
    "    # Flatten the input\n",
    "    noise_flat = Lambda(lambda x : K.reshape(x, (-1,Nr)),\n",
    "                       output_shape=(Nr,),\\\n",
    "                        name=\"noise_flat\")(rx_input_message)\n",
    "    # First layer with the different activation function\n",
    "    # to capture non-linearity and for symmetry with the \n",
    "    # transmitter.\n",
    "    rx1 = Dense(M,activation=\"relu\", name=\"rx1\")\\\n",
    "                (noise_flat)\n",
    "    # Dense layer with softmax activation\n",
    "    rx_softmax = Dense(M,activation='softmax', \\\n",
    "                       name=\"rx_softmax\")(rx1)\n",
    "    # Select the symbols with the maximum probabilities\n",
    "    ml_symbs = MostLikelySymbol()(rx_softmax)\n",
    "\n",
    "    ###Defining the models\n",
    "    # Model the Tx\n",
    "    transmitter = Model(tx_input_message, tx_norm_scaled)\n",
    "    # Model the Tx plus the noise\n",
    "    channel_sym_with_noise = Model(tx_input_message, noise)\n",
    "    # Model the Rx\n",
    "    receiver = Model(rx_input_message,rx_softmax)\n",
    "    receiver_symbs = Model(rx_input_message,ml_symbs)\n",
    "\n",
    "    # Compile the model\n",
    "    transmitter.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")\n",
    "    receiver.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")    \n",
    "    return transmitter, channel_sym_with_noise, receiver,\\\n",
    "                receiver_symbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_to_sigma_a(db):\n",
    "    return 10**(-db/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_block_error_rate(test_data, pred_symbs):\n",
    "    errors = (test_data != pred_symbs)\n",
    "    block_errors = errors.any(axis=1)\n",
    "    return block_errors.sum()/block_errors.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complex_tapered_noise_bler_SNR(M, R, SNR, weights_file_path, \\\n",
    "                                   test_data, hl_activation_func, \\\n",
    "                                   ol_activation_func, num_layers):\n",
    "    ## Get noise std_dev \n",
    "    noise_std = db_to_sigma_a(SNR)   \n",
    "    ## Make new model with loaded weights\n",
    "    autoencoder8_8_tap_nl, _, _, autoencoder_symbs8_8_tap_nl, \\\n",
    "        _, _, _ \\\n",
    "        = make_complex_n_layer_lr_tanh_tapering_model(M, R, noise_std, \\\n",
    "                                                      hl_activation_func, \\\n",
    "                                                      ol_activation_func, \\\n",
    "                                                      num_layers)    \n",
    "    autoencoder8_8_tap_nl.load_weights(weights_file_path, by_name=True)    \n",
    "    ## Check Accuracy on test set\n",
    "    pred_symbs = autoencoder_symbs8_8_tap_nl.predict(test_data)\n",
    "    return get_block_error_rate(test_data, pred_symbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_supervised_rbf_bler_SNR(k, Nc, sigma_m, SNR, \\\n",
    "                                weights_file_path, \\\n",
    "                                test_data):\n",
    "    ## Get noise std_dev \n",
    "    sigma_a = db_to_sigma_a(SNR)   \n",
    "    ## Make new model with loaded weights\n",
    "    autoencoder_rbf_tmp, autoencoder_symbs_rbf_tmp, transmitter_rbf_tmp, \\\n",
    "        channel_sym_with_noise_rbf_tmp, receiver_rbf_tmp \\\n",
    "        = get_rbf_autoencoder(k, Nc, sigma_m, sigma_a)\n",
    "    autoencoder_rbf_tmp.load_weights(weights_file_path, by_name=True)\n",
    "\n",
    "    ## Check Accuracy on test set\n",
    "    pred_symbs = autoencoder_symbs_rbf_tmp.predict(test_data)\n",
    "    return get_block_error_rate(test_data, pred_symbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models, Data and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a set of data for a particular M\n",
    "# total_size = 1000000\n",
    "# all_one_hot_messages256 = np.diag(np.ones(256))\n",
    "\n",
    "\n",
    "# # Automatically saves the data for m to a filepath of\n",
    "# # './data/data${M}.npy'\n",
    "# t2 = time()\n",
    "# func_data256, file_path256, all_one_hot_messages256 = get_data_set(256, total_size)\n",
    "# print(f\"Finished 256 in {time()-t2}s\")\n",
    "\n",
    "# # Don't use this function unless it's for a new M, just\n",
    "# # load the data you have calculated other times.\n",
    "# # This makes results more comparible and saves time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data calculated from previous runs\n",
    "all_one_hot_messages256 = np.diag(np.ones(256))\n",
    "data256 = np.load('./data/data256.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data256.shape = (720000, 256)\n",
      "valid_data256.shape = (80000, 256)\n",
      "test_data256.shape = (200000, 256)\n"
     ]
    }
   ],
   "source": [
    "# Splitting into training, testing and validation sets\n",
    "train_data256, test_data256 = train_test_split(data256, \\\n",
    "                                         train_size=0.8)\n",
    "train_data256, valid_data256 = train_test_split(train_data256, \\\n",
    "                                         train_size=0.9)\n",
    "print(f\"train_data256.shape = {train_data256.shape}\")\n",
    "print(f\"valid_data256.shape = {valid_data256.shape}\")\n",
    "print(f\"test_data256.shape = {test_data256.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting a supervised model for AWGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/apsw/.local/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    }
   ],
   "source": [
    "# (8,8)\n",
    "M = 2**8 # Number of one hot encoded messages\n",
    "R = 2 # R = k/n_r\n",
    "sigma = get_noise_sigma(7, Rc=R)\n",
    "hl_activation_func = keras.layers.advanced_activations.LeakyReLU()\n",
    "hl_activation_func.__name__ = 'leakyrelu'\n",
    "ol_activation_func = \"tanh\"\n",
    "num_layers = 2\n",
    "\n",
    "autoencoder8_8_tap_2l, transmitter8_8_tap_2l, reciever8_8_tap_2l, \\\n",
    "    autoencoder_symbs8_8_tap_2l, k8_8_tap_2l, Nc8_8_tap_2l, Nr8_8_tap_2l \\\n",
    "    = make_complex_n_layer_lr_tanh_tapering_model(M, R, sigma, \\\n",
    "                                                  hl_activation_func, \\\n",
    "                                                  ol_activation_func, \\\n",
    "                                                  num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Previous 0.1763\n",
    "# autoencoder8_8_tap_2l.fit(train_data256, train_data256,\n",
    "#                        epochs=1000,\n",
    "#                        batch_size=1000,\n",
    "#                        shuffle=True,\n",
    "#                        validation_data=(valid_data256,\n",
    "#                                         valid_data256),\n",
    "#                        callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder8_8_tap_2l.load_weights('./models/autoencoder8_8_tap_2l3.3856e-06.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting a supervised model for RBF\n",
    "\n",
    "The RBF model is trained at an SNR of 20db, because of the normalisation this is a sigma_a of 0.1. Could also set sigma_m to satisfy $2\\sigma_m^2=1$, ($\\sigma_m = \\frac{1}{\\sqrt{2}}$) so that the average fade gain is unity.\n",
    "\n",
    "SNR = $\\frac{1}{\\sigma_2}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_to_sigma_a(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_elements =  8\n"
     ]
    }
   ],
   "source": [
    "# Training at 20db\n",
    "k = 8\n",
    "Nc = 4\n",
    "# sigma_m = 0.33\n",
    "sigma_m = np.sqrt(0.5)\n",
    "sigma_a = db_to_sigma_a(20)\n",
    "autoencoder_rbf, autoencoder_symbs_rbf, transmitter_rbf, \\\n",
    "    channel_sym_with_noise_rbf, receiver_rbf \\\n",
    "    = get_rbf_autoencoder(k, Nc, sigma_m, sigma_a)\n",
    "history_rbf = History()\n",
    "checkpoints_rbf = ModelCheckpoint(\"./models/rbf_supervised/autoencoder_rbf_{val_loss}.h5\", save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/10\n",
      "2110000/7200000 [=======>......................] - ETA: 5:03 - loss: 5.5574"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-0334a702b6ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                     validation_data=(valid_data256,\n\u001b[1;32m      6\u001b[0m                                      valid_data256),\n\u001b[0;32m----> 7\u001b[0;31m                     callbacks=[es, history_rbf, checkpoints_rbf])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder_rbf.fit(train_data256, train_data256,\n",
    "                    epochs=10,\n",
    "                    batch_size=10000,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(valid_data256,\n",
    "                                     valid_data256),\n",
    "                    callbacks=[es, history_rbf, checkpoints_rbf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_elements =  8\n"
     ]
    }
   ],
   "source": [
    "# Training at 40db\n",
    "k = 8\n",
    "Nc = 4\n",
    "# sigma_m = 0.33\n",
    "sigma_m = np.sqrt(0.5)\n",
    "sigma_a = db_to_sigma_a(40)\n",
    "autoencoder_rbf_40, autoencoder_symbs_rbf_40, transmitter_rbf_40, \\\n",
    "    channel_sym_with_noise_rbf_40, receiver_rbf_40 \\\n",
    "    = get_rbf_autoencoder(k, Nc, sigma_m, sigma_a)\n",
    "history_rbf_40 = History()\n",
    "checkpoints_rbf_40 = ModelCheckpoint(\"./models/rbf_supervised/autoencoder_rbf_40_vl{val_loss}_e{epoch:02d}.h5\", save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720000 samples, validate on 80000 samples\n",
      "Epoch 1/70\n",
      "720000/720000 [==============================] - 464s 645us/step - loss: 4.4230 - val_loss: 4.4111\n",
      "Epoch 2/70\n",
      "720000/720000 [==============================] - 435s 604us/step - loss: 4.4206 - val_loss: 4.4164\n",
      "Epoch 3/70\n",
      "720000/720000 [==============================] - 447s 620us/step - loss: 4.4180 - val_loss: 4.4251\n",
      "Epoch 4/70\n",
      "720000/720000 [==============================] - 409s 568us/step - loss: 4.4157 - val_loss: 4.4277\n",
      "Epoch 5/70\n",
      "720000/720000 [==============================] - 393s 546us/step - loss: 4.4170 - val_loss: 4.4057\n",
      "Epoch 6/70\n",
      "720000/720000 [==============================] - 440s 611us/step - loss: 4.4176 - val_loss: 4.4052\n",
      "Epoch 7/70\n",
      "720000/720000 [==============================] - 405s 562us/step - loss: 4.4164 - val_loss: 4.4266\n",
      "Epoch 8/70\n",
      "720000/720000 [==============================] - 397s 552us/step - loss: 4.4113 - val_loss: 4.4128\n",
      "Epoch 9/70\n",
      "720000/720000 [==============================] - 397s 551us/step - loss: 4.4134 - val_loss: 4.4039\n",
      "Epoch 10/70\n",
      "720000/720000 [==============================] - 399s 554us/step - loss: 4.4129 - val_loss: 4.4115\n",
      "Epoch 11/70\n",
      "720000/720000 [==============================] - 397s 552us/step - loss: 4.4085 - val_loss: 4.4072\n",
      "Epoch 12/70\n",
      "720000/720000 [==============================] - 399s 554us/step - loss: 4.4075 - val_loss: 4.4130\n",
      "Epoch 13/70\n",
      "720000/720000 [==============================] - 397s 552us/step - loss: 4.4080 - val_loss: 4.4080\n",
      "Epoch 14/70\n",
      "720000/720000 [==============================] - 397s 551us/step - loss: 4.4117 - val_loss: 4.4079\n",
      "Epoch 15/70\n",
      "720000/720000 [==============================] - 397s 551us/step - loss: 4.4067 - val_loss: 4.4078\n",
      "Epoch 16/70\n",
      "720000/720000 [==============================] - 399s 554us/step - loss: 4.4071 - val_loss: 4.4025\n",
      "Epoch 17/70\n",
      "720000/720000 [==============================] - 397s 551us/step - loss: 4.4077 - val_loss: 4.3915\n",
      "Epoch 18/70\n",
      "720000/720000 [==============================] - 400s 556us/step - loss: 4.4062 - val_loss: 4.4152\n",
      "Epoch 19/70\n",
      "720000/720000 [==============================] - 399s 554us/step - loss: 4.4082 - val_loss: 4.4182\n",
      "Epoch 20/70\n",
      "720000/720000 [==============================] - 401s 557us/step - loss: 4.4036 - val_loss: 4.4048\n",
      "Epoch 21/70\n",
      "720000/720000 [==============================] - 397s 552us/step - loss: 4.4050 - val_loss: 4.4088\n",
      "Epoch 22/70\n",
      "720000/720000 [==============================] - 395s 549us/step - loss: 4.4056 - val_loss: 4.4002\n",
      "Epoch 23/70\n",
      "720000/720000 [==============================] - 396s 550us/step - loss: 4.4013 - val_loss: 4.4071\n",
      "Epoch 24/70\n",
      "720000/720000 [==============================] - 398s 552us/step - loss: 4.4027 - val_loss: 4.3958\n",
      "Epoch 25/70\n",
      "720000/720000 [==============================] - 396s 551us/step - loss: 4.3971 - val_loss: 4.3906\n",
      "Epoch 26/70\n",
      "720000/720000 [==============================] - 398s 553us/step - loss: 4.3992 - val_loss: 4.3968\n",
      "Epoch 27/70\n",
      "720000/720000 [==============================] - 397s 551us/step - loss: 4.4004 - val_loss: 4.3960\n",
      "Epoch 28/70\n",
      "720000/720000 [==============================] - 400s 556us/step - loss: 4.4015 - val_loss: 4.3998\n",
      "Epoch 29/70\n",
      "720000/720000 [==============================] - 396s 550us/step - loss: 4.4019 - val_loss: 4.3992\n",
      "Epoch 30/70\n",
      "720000/720000 [==============================] - 395s 549us/step - loss: 4.4003 - val_loss: 4.4020\n",
      "Epoch 31/70\n",
      "720000/720000 [==============================] - 399s 555us/step - loss: 4.3991 - val_loss: 4.3914\n",
      "Epoch 32/70\n",
      "720000/720000 [==============================] - 396s 550us/step - loss: 4.3997 - val_loss: 4.3966\n",
      "Epoch 33/70\n",
      "720000/720000 [==============================] - 400s 556us/step - loss: 4.3981 - val_loss: 4.3925\n",
      "Epoch 34/70\n",
      "720000/720000 [==============================] - 398s 553us/step - loss: 4.3964 - val_loss: 4.3930\n",
      "Epoch 35/70\n",
      "720000/720000 [==============================] - 397s 552us/step - loss: 4.3950 - val_loss: 4.4041\n",
      "Epoch 36/70\n",
      "720000/720000 [==============================] - 396s 550us/step - loss: 4.3977 - val_loss: 4.3893\n",
      "Epoch 37/70\n",
      "720000/720000 [==============================] - 398s 553us/step - loss: 4.3963 - val_loss: 4.3982\n",
      "Epoch 38/70\n",
      "720000/720000 [==============================] - 399s 555us/step - loss: 4.3958 - val_loss: 4.3957\n",
      "Epoch 39/70\n",
      "720000/720000 [==============================] - 399s 555us/step - loss: 4.3959 - val_loss: 4.3864\n",
      "Epoch 40/70\n",
      "720000/720000 [==============================] - 397s 551us/step - loss: 4.3979 - val_loss: 4.3943\n",
      "Epoch 41/70\n",
      "720000/720000 [==============================] - 397s 552us/step - loss: 4.3992 - val_loss: 4.3904\n",
      "Epoch 42/70\n",
      "720000/720000 [==============================] - 403s 560us/step - loss: 4.3958 - val_loss: 4.4000\n",
      "Epoch 43/70\n",
      "720000/720000 [==============================] - 430s 597us/step - loss: 4.3940 - val_loss: 4.3933\n",
      "Epoch 44/70\n",
      "720000/720000 [==============================] - 398s 552us/step - loss: 4.3940 - val_loss: 4.4049\n",
      "Epoch 45/70\n",
      "720000/720000 [==============================] - 399s 554us/step - loss: 4.3922 - val_loss: 4.3864\n",
      "Epoch 46/70\n",
      "720000/720000 [==============================] - 401s 558us/step - loss: 4.3965 - val_loss: 4.3942\n",
      "Epoch 47/70\n",
      "720000/720000 [==============================] - 402s 558us/step - loss: 4.3958 - val_loss: 4.3965\n",
      "Epoch 48/70\n",
      "720000/720000 [==============================] - 399s 554us/step - loss: 4.3948 - val_loss: 4.3951\n",
      "Epoch 49/70\n",
      "720000/720000 [==============================] - 399s 554us/step - loss: 4.3930 - val_loss: 4.3981\n",
      "Epoch 50/70\n",
      "720000/720000 [==============================] - 400s 555us/step - loss: 4.3946 - val_loss: 4.3873\n",
      "Epoch 51/70\n",
      "720000/720000 [==============================] - 403s 559us/step - loss: 4.3942 - val_loss: 4.3817\n",
      "Epoch 52/70\n",
      "720000/720000 [==============================] - 404s 561us/step - loss: 4.3942 - val_loss: 4.3892\n",
      "Epoch 53/70\n",
      "720000/720000 [==============================] - 398s 552us/step - loss: 4.3913 - val_loss: 4.3898\n",
      "Epoch 54/70\n",
      "720000/720000 [==============================] - 401s 557us/step - loss: 4.3909 - val_loss: 4.3938\n",
      "Epoch 55/70\n",
      "720000/720000 [==============================] - 400s 555us/step - loss: 4.3935 - val_loss: 4.3897\n",
      "Epoch 56/70\n",
      "720000/720000 [==============================] - 399s 554us/step - loss: 4.3902 - val_loss: 4.3968\n",
      "Epoch 57/70\n",
      "720000/720000 [==============================] - 397s 552us/step - loss: 4.3973 - val_loss: 4.3895\n",
      "Epoch 58/70\n",
      "720000/720000 [==============================] - 400s 556us/step - loss: 4.3903 - val_loss: 4.3823\n",
      "Epoch 59/70\n",
      "720000/720000 [==============================] - 399s 555us/step - loss: 4.3983 - val_loss: 4.3960\n",
      "Epoch 60/70\n",
      "720000/720000 [==============================] - 401s 557us/step - loss: 4.3918 - val_loss: 4.3950\n",
      "Epoch 61/70\n",
      "720000/720000 [==============================] - 398s 552us/step - loss: 4.3908 - val_loss: 4.3878\n",
      "Epoch 62/70\n",
      "720000/720000 [==============================] - 397s 552us/step - loss: 4.3935 - val_loss: 4.3869\n",
      "Epoch 63/70\n",
      "720000/720000 [==============================] - 398s 553us/step - loss: 4.3954 - val_loss: 4.3999\n",
      "Epoch 64/70\n",
      "720000/720000 [==============================] - 402s 558us/step - loss: 4.3942 - val_loss: 4.3945\n",
      "Epoch 65/70\n",
      "720000/720000 [==============================] - 397s 551us/step - loss: 4.3901 - val_loss: 4.3836\n",
      "Epoch 66/70\n",
      "720000/720000 [==============================] - 399s 555us/step - loss: 4.3934 - val_loss: 4.3831\n",
      "Epoch 67/70\n",
      "720000/720000 [==============================] - 399s 554us/step - loss: 4.3896 - val_loss: 4.3875\n",
      "Epoch 68/70\n",
      "720000/720000 [==============================] - 400s 556us/step - loss: 4.3857 - val_loss: 4.3896\n",
      "Epoch 69/70\n",
      "720000/720000 [==============================] - 402s 558us/step - loss: 4.3927 - val_loss: 4.3897\n",
      "Epoch 70/70\n",
      "720000/720000 [==============================] - 400s 555us/step - loss: 4.3897 - val_loss: 4.3870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f65d98bcf28>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder_rbf_40.fit(train_data256, train_data256,\n",
    "#                     epochs=10,\n",
    "                    epochs=70,\n",
    "                    batch_size=10000,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(valid_data256,\n",
    "                                     valid_data256),\n",
    "                    callbacks=[es, history_rbf_40, checkpoints_rbf_40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_rbf_40_local = history_rbf_40_local + history_rbf_40.history[\"val_loss\"]\n",
    "# np.save(\"./models/rbf_supervised/histories/history_rbf_40_local.npy\",history_rbf_40_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.386954426765442"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_rbf_40_local[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(history_rbf_40_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_elements =  8\n"
     ]
    }
   ],
   "source": [
    "# Training with no additive noise\n",
    "k = 8\n",
    "Nc = 4\n",
    "# sigma_m = 0.33\n",
    "sigma_m = np.sqrt(0.5)\n",
    "sigma_a = 0\n",
    "autoencoder_rbf_0, autoencoder_symbs_rbf_0, transmitter_rbf_0, \\\n",
    "    channel_sym_with_noise_rbf_0, receiver_rbf_0 \\\n",
    "    = get_rbf_autoencoder(k, Nc, sigma_m, sigma_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_rbf_40 = np.array([4.5165, 4.4356, 4.4190, 4.4086, 4.3993, \\\n",
    "                           4.3925, 4.3877, 4.3864, 4.3838, 4.3824])\n",
    "history_rbf_0 = np.array([5.5452, 5.5452, 2.1448, 1.6362e-04, 3.6967e-04,\\\n",
    "                           1.1467e-04, 5.1886e-04, 0.0019, 1.3443e-05, 8.5237e-04])\n",
    "# np.save(\"./models/rbf_supervised/histories/history_rbf_0.npy\",history_rbf_0)\n",
    "# np.save(\"./models/rbf_supervised/histories/history_rbf_40.npy\",history_rbf_40)\n",
    "\n",
    "history_rbf_40 = np.load(\"./models/rbf_supervised/histories/history_rbf_40.npy\")\n",
    "history_rbf_0 = np.load(\"./models/rbf_supervised/histories/history_rbf_0.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200000 samples, validate on 800000 samples\n",
      "Epoch 1/1000\n",
      "7200000/7200000 [==============================] - 436s 61us/step - loss: 5.5455 - val_loss: 5.5452\n",
      "Epoch 2/1000\n",
      "7200000/7200000 [==============================] - 391s 54us/step - loss: 5.5452 - val_loss: 5.5452\n",
      "Epoch 3/1000\n",
      "7200000/7200000 [==============================] - 390s 54us/step - loss: 5.4183 - val_loss: 2.1448\n",
      "Epoch 4/1000\n",
      "7200000/7200000 [==============================] - 392s 54us/step - loss: 0.0308 - val_loss: 1.6362e-04\n",
      "Epoch 5/1000\n",
      "7200000/7200000 [==============================] - 390s 54us/step - loss: 8.9093e-04 - val_loss: 3.6967e-04\n",
      "Epoch 6/1000\n",
      "7200000/7200000 [==============================] - 389s 54us/step - loss: 0.0013 - val_loss: 1.1467e-04\n",
      "Epoch 7/1000\n",
      "7200000/7200000 [==============================] - 390s 54us/step - loss: 0.0011 - val_loss: 5.1886e-04\n",
      "Epoch 8/1000\n",
      "7200000/7200000 [==============================] - 389s 54us/step - loss: 0.0011 - val_loss: 0.0019\n",
      "Epoch 9/1000\n",
      "7200000/7200000 [==============================] - 392s 54us/step - loss: 9.8993e-04 - val_loss: 1.3443e-05\n",
      "Epoch 10/1000\n",
      "7200000/7200000 [==============================] - 392s 55us/step - loss: 8.6543e-04 - val_loss: 8.5237e-04\n",
      "Epoch 11/1000\n",
      "  10000/7200000 [..............................] - ETA: 9:41 - loss: 5.1837e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-90524b7fdaf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                        valid_data256),\n\u001b[1;32m      6\u001b[0m                                       \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                       verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history_rbf_0 = autoencoder_rbf_0.fit(train_data256, train_data256,\n",
    "                                      epochs=10, batch_size=10000,\n",
    "                                      shuffle=True,\n",
    "                                      validation_data=(valid_data256,\n",
    "                                                       valid_data256),\n",
    "                                      callbacks=[es, history_rbf_0, checkpoints_rbf_0],\n",
    "                                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving\n",
    "# Individual power constraint, sigma_m = 1/sqrt(2)\n",
    "# autoencoder_rbf.save('./models/autoencoder_rbf_0.1589.model')\n",
    "# autoencoder_rbf.save_weights('./models/autoencoder_rbf_0.1589.h5')\n",
    "\n",
    "# autoencoder_rbf_ap.save('./models/autoencoder_rbf_ap_5.5459.model')\n",
    "# autoencoder_rbf_ap.save_weights('./models/autoencoder_rbf_ap_5.5459.h5')\n",
    "\n",
    "# autoencoder_rbf_40.save('./models/autoencoder_rbf_sa40_4.3824.model')\n",
    "# autoencoder_rbf_40.save_weights('./models/autoencoder_rbf_sa40_4.3824.h5')\n",
    "\n",
    "# autoencoder_rbf_40.save('./models/autoencoder_rbf_sa40_4.3870.model')\n",
    "# autoencoder_rbf_40.save_weights('./models/autoencoder_rbf_sa40_4.3870.h5')\n",
    "\n",
    "# autoencoder_rbf_0.save('./models/autoencoder_rbf_sa0_8.5237e_04.model')\n",
    "# autoencoder_rbf_0.save_weights('./models/autoencoder_rbf_sa0_8.5237e_04.h5')\n",
    "\n",
    "### Loading\n",
    "autoencoder_rbf.load_weights('./models/autoencoder_rbf_0.1589.h5', by_name=True)\n",
    "autoencoder_rbf_40.load_weights('./models/autoencoder_rbf_sa40_4.3824.h5', by_name=True)\n",
    "# autoencoder_rbf_40.load_weights('./models/autoencoder_rbf_sa0_8.5237e_04.h5', by_name=True)\n",
    "autoencoder_rbf_0.load_weights('./models/autoencoder_rbf_sa0_8.5237e_04.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting an  alternating model for AWGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 8\n",
    "Nc = 4\n",
    "sigma = 0.22\n",
    "transmitter, channel_sym_with_noise, \\\n",
    "    receiver, receiver_symbs \\\n",
    "    = get_paper_awgn_rx_and_tx_models(M, Nc, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting an  alternating model for RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 8\n",
    "Nc = 4\n",
    "sigma_m = 0.33\n",
    "sigma_a = 1\n",
    "transmitter, channel_sym_with_noise, \\\n",
    "    receiver, receiver_symbs \\\n",
    "    = get_paper_rbf_rx_and_tx_models(M, Nc, sigma_m, sigma_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Supervised AWGN performance across the SNR range of [-4,0.5,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -4.0, i = 1/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0th loop took 51.436500549316406s\n",
      "ratio_db = -3.5, i = 2/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1th loop took 51.906299352645874s\n",
      "ratio_db = -3.0, i = 3/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2th loop took 51.930606842041016s\n",
      "ratio_db = -2.5, i = 4/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3th loop took 52.61579132080078s\n",
      "ratio_db = -2.0, i = 5/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4th loop took 52.799492597579956s\n",
      "ratio_db = -1.5, i = 6/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5th loop took 53.136507511138916s\n",
      "ratio_db = -1.0, i = 7/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6th loop took 53.56024432182312s\n",
      "ratio_db = -0.5, i = 8/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7th loop took 53.93984580039978s\n",
      "ratio_db = 0.0, i = 9/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8th loop took 53.98229122161865s\n",
      "ratio_db = 0.5, i = 10/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9th loop took 54.836410999298096s\n",
      "ratio_db = 1.0, i = 11/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10th loop took 55.04261589050293s\n",
      "ratio_db = 1.5, i = 12/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11th loop took 55.47947311401367s\n",
      "ratio_db = 2.0, i = 13/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12th loop took 55.967275857925415s\n",
      "ratio_db = 2.5, i = 14/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13th loop took 56.49920988082886s\n",
      "ratio_db = 3.0, i = 15/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14th loop took 57.087040424346924s\n",
      "ratio_db = 3.5, i = 16/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "15th loop took 57.46648573875427s\n",
      "ratio_db = 4.0, i = 17/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16th loop took 57.54560446739197s\n",
      "ratio_db = 4.5, i = 18/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "17th loop took 57.83512878417969s\n",
      "ratio_db = 5.0, i = 19/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "18th loop took 58.547157764434814s\n",
      "ratio_db = 5.5, i = 20/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "19th loop took 58.95889925956726s\n",
      "ratio_db = 6.0, i = 21/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20th loop took 59.41124773025513s\n",
      "ratio_db = 6.5, i = 22/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "21th loop took 60.30534219741821s\n",
      "ratio_db = 7.0, i = 23/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "22th loop took 60.44829869270325s\n",
      "ratio_db = 7.5, i = 24/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "23th loop took 60.9995174407959s\n",
      "ratio_db = 8.0, i = 25/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "24th loop took 60.82547616958618s\n",
      "ratio_db = 8.5, i = 26/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "25th loop took 62.30768156051636s\n",
      "ratio_db = 9.0, i = 27/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "26th loop took 61.84466576576233s\n",
      "ratio_db = 9.5, i = 28/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "27th loop took 63.05571937561035s\n",
      "ratio_db = 10.0, i = 29/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28th loop took 67.72161793708801s\n",
      "ratio_db = 10.5, i = 30/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "29th loop took 64.11016845703125s\n",
      "ratio_db = 11.0, i = 31/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "30th loop took 63.89042639732361s\n",
      "ratio_db = 11.5, i = 32/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "31th loop took 65.06354212760925s\n",
      "ratio_db = 12.0, i = 33/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "32th loop took 65.12028694152832s\n",
      "ratio_db = 12.5, i = 34/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "33th loop took 65.20026087760925s\n",
      "ratio_db = 13.0, i = 35/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "34th loop took 66.1613438129425s\n",
      "\n",
      "Took 2047.0448281764984\n"
     ]
    }
   ],
   "source": [
    "# (8,8) tapered, n layers\n",
    "t0 = time()\n",
    "SNRs_db = np.arange(-4,13.5,0.5)\n",
    "bler = np.empty(SNRs_db.size)\n",
    "\n",
    "hl_act_f = keras.layers.advanced_activations.LeakyReLU()\n",
    "hl_act_f.__name__ = 'leakyrelu'\n",
    "\n",
    "# for i, ratio_db in enumerate(tqdm_notebook(SNRs_db, desc=\"1st loop\")):\n",
    "for i, ratio_db in enumerate(SNRs_db):\n",
    "    print(f\"ratio_db = {ratio_db}, i = {i+1}/{SNRs_db.size}\", end=\"\\r\")\n",
    "    t1 = time()\n",
    "    bler[i] = get_complex_tapered_noise_bler_SNR(2**8, 2, ratio_db, \\\n",
    "                                                 './models/autoencoder8_8_tap_2l3.3856e-06.h5', \\\n",
    "                                                 test_data256, hl_act_f, \\\n",
    "                                                 \"tanh\", 2)\n",
    "    print(f\"\\n{i}th loop took {time() - t1}s\")\n",
    "print(f\"\\nTook {time() - t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder_awgn = bler\n",
    "# np.save('./key_results/paper2/autoencoder_awgn.npy', autoencoder_awgn)\n",
    "\n",
    "autoencoder_awgn = np.load('./key_results/paper2/autoencoder_awgn.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Supervised RBF performance across the SNR range of [-4,0.5,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c06449fb56f45848ae672a11384ab0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='1st loop', max=35, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = 13.0, i = 35/35\n",
      "\n",
      "Took 1973.473479270935\n"
     ]
    }
   ],
   "source": [
    "# autoencoder with rbf\n",
    "# - trained at an SNR of 40\n",
    "# - exactly the same architecture as the paper\n",
    "# - training val_loss of 4.3824\n",
    "t0 = time()\n",
    "SNRs_db = np.arange(-4,13.5,0.5)\n",
    "bler = np.empty(SNRs_db.size)\n",
    "\n",
    "for i, ratio_db in enumerate(tqdm_notebook(SNRs_db, desc=\"1st loop\")):\n",
    "# for i, ratio_db in enumerate(SNRs_db):\n",
    "    print(f\"ratio_db = {ratio_db}, i = {i+1}/{SNRs_db.size}\", end=\"\\r\")\n",
    "    t1 = time()\n",
    "    bler[i] = get_supervised_rbf_bler_SNR(8, 4, np.sqrt(0.5), ratio_db, \\\n",
    "                                          './models/autoencoder_rbf_sa40_4.3824.h5', \\\n",
    "                                          test_data256)\n",
    "\n",
    "    \n",
    "#     print(f\"\\n{i}th loop took {time() - t1}s\")\n",
    "print(f\"\\nTook {time() - t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_cores =  4\n",
      "Took 136.46156430244446s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print(\"num_cores = \", num_cores)\n",
    "\n",
    "def tmp(SNR):\n",
    "    return get_supervised_rbf_bler_SNR(8, 4, np.sqrt(0.5), SNR, \\\n",
    "                                          './models/autoencoder_rbf_sa40_4.3824.h5', \\\n",
    "                                          test_data256)\n",
    "\n",
    "results = Parallel(n_jobs=num_cores)(delayed(tmp)(SNR) for SNR in SNRs_db)\n",
    "print(f\"Took {time()-t0}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.995525, 0.994975, 0.99555, 0.99547]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.995455, 0.995345, 0.9953  , 0.99551 , 0.994965, 0.994895,\n",
       "       0.99479 , 0.9947  , 0.99411 , 0.99397 , 0.994045, 0.993545,\n",
       "       0.99333 , 0.992605, 0.99238 , 0.991515, 0.990945, 0.990435,\n",
       "       0.989335, 0.98868 , 0.987745, 0.98669 , 0.984195, 0.982355,\n",
       "       0.98044 , 0.97794 , 0.97541 , 0.970705, 0.967605, 0.96307 ,\n",
       "       0.95757 , 0.950065, 0.942965, 0.935495, 0.9258  ])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From second run to check constistency\n",
    "bler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.995415, 0.99536 , 0.995215, 0.995095])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From first run\n",
    "bler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder_rbf_sa40_4_3824_bler = bler\n",
    "# np.save('./key_results/paper2/autoencoder_rbf_sa40_4_3824_bler.npy', autoencoder_rbf_sa40_4_3824_bler)\n",
    "\n",
    "autoencoder_rbf_sa40_4_3824_bler = np.load('./key_results/paper2/autoencoder_rbf_sa40_4_3824_bler.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 6a - AWGN supervised vs unsupervised across a range of SNRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data for messing with the way the plot looks\n",
    "x = np.arange(-4,13.5,0.5)\n",
    "y = np.exp(-0.1*x**2)\n",
    "z = y*0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(x,autoencoder_awgn,'^-', color=\"orange\", markerfacecolor='none', markersize=4, linewidth=0.5)\n",
    "plt.semilogy(x,z,'D-', color=\"blue\", markerfacecolor='none', markersize=3, linewidth=0.5)\n",
    "plt.xlabel(\"SNR (db)\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.grid()\n",
    "plt.legend([\"Supervised\", \"Alternating\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 6b - RBF supervised vs unsupervised across a range of SNRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_rbf_sa40_4_3824_bler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(x,autoencoder_rbf_sa40_4_3824_bler,'^-', color=\"orange\", markerfacecolor='none', markersize=4, linewidth=0.5)\n",
    "# plt.semilogy(x,z,'D-', color=\"blue\", markerfacecolor='none', markersize=3, linewidth=0.5)\n",
    "plt.xlabel(\"SNR (db)\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.grid()\n",
    "plt.legend([\"Supervised\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rician Fading\n",
    "\n",
    "https://en.wikipedia.org/wiki/Rician_fading\n",
    "<br>\n",
    "\n",
    "https://www.gaussianwaves.com/tag/rician/\n",
    "\n",
    "Also see the document in downloads about it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
