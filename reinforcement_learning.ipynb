{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning - Aoudia and Hoydis\n",
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Lambda, ELU, Reshape, Flatten, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "from keras.layers import advanced_activations\n",
    "from keras.engine.topology import Layer\n",
    "from keras.legacy import interfaces\n",
    "from keras.initializers import Zeros as kZeros\n",
    "from keras.utils import multi_gpu_model, plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from time import time\n",
    "import pickle\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # confirm TensorFlow sees the GPU\n",
    "# from tensorflow.python.client import device_lib\n",
    "# assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# # confirm Keras sees the GPU\n",
    "# from keras import backend\n",
    "# assert len(backend.tensorflow_backend._get_available_gpus()) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is in a seperate box because it isn't running on the \n",
    "# # AWS server. \n",
    "# from tqdm import tqdm_notebook, tnrange\n",
    "# from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Useful guides\n",
    "https://hub.packtpub.com/build-reinforcement-learning-agent-in-keras-tutorial/\n",
    "<br>\n",
    "https://medium.com/ml-everything/policy-based-reinforcement-learning-with-keras-4996015a0b1\n",
    "<br>\n",
    "\n",
    "##### Notes from paper\n",
    "- Loss function = Cross Entropy\n",
    "- Normalisation = Average L2 power constraint = 1. $\\mathbb{E}\\left[\\frac{1}{N}\\left\\Vert x\\right\\Vert_2^2\\right]$ = 1.\n",
    "- Trained at an SNR of 10dB for AWGN and an SNR of 20dB for RBF channel. $\\sigma_{\\pi}^2$ = 0.02 at training time.\n",
    "    - During RL exploration $\\mathbf{x_p} = \\mathbf{x} + \\mathbf{w}$, where each element of $\\mathbf{w}$ is i.i.d ~ $\\mathcal{N}(0,\\sigma_{\\pi}^2)$\n",
    "- M= 256, N=4. N= the number of complex channel uses, so n = 8. Therefore n,k = (8,8)\n",
    "- AWGN -> 1 hidden layer, size M, ReLu activation function\n",
    "- Rayleigh -> see diagram.\n",
    "    - Two layers (Dense(20,tanh)->Dense(2,linear)) calculate an estimate for $\\hat{h}$, then we divide the received signal by $\\hat{h}$ then have two layers for finding the received signal. Dense(M,ReLu) then Dense(M,Softmax). Then have the select maximum likelihood symbol layer.\n",
    "- SNR $ = \\frac{\\mathbb{E}\\left[\\frac{1}{N}\\left\\Vert x\\right\\Vert_2^2\\right]}{\\sigma^2}$, but because of the normalisation this is $\\frac{1}{\\sigma^2}$.\n",
    "- The RBF seems to be slow fading, but I could check for fast fading as well. Rayleigh fading is of the form $\\mathbf{r} = \\mathbf{s}*\\mathbf{h} + \\mathbf{n}$ where $\\mathbf{h}$ ~ $\\mathcal{N}(0,\\sigma_m^2)$, $\\mathbf{n}$ ~ $\\mathcal{N}(0,\\sigma_a^2)$. \n",
    "    - Slow fading: h stays the same across the whole minibatch\n",
    "    - Fast fading: h changes every sample\n",
    "- I'm going to guess that $\\sigma_m ~ \\frac{1}{3}$. Because this means that 98% of the time it's less than 1. Which sounds alright.\n",
    "- I could also try Ricean fading and see how the model performs against that.\n",
    "- Ricean fading would actually be the most general, as then it could learn Rayleigh fading, and it could also learn AWGN just by learning to make $\\hat{h}$ every time.\n",
    "\n",
    "##### Work Log\n",
    "\n",
    "27/05/2019\n",
    "- Started researching RBF, think I can implement it in a custom layer.\n",
    "- Made lots of notes\n",
    "- Copied over functions from other notebook.\n",
    "\n",
    "28/05/2019\n",
    "- Made functions for making the Tx and Rx blocks for the unsupervised learning, however these will likely need to be edited.\n",
    "\n",
    "\n",
    "29/05/2019\n",
    "- Got food poisoning the night before and did no work.\n",
    "\n",
    "31/05/2019\n",
    "- Got the rbf supervised model fitting, but with no good results. \n",
    "- Copied the architecture from the paper exactly, however it was not effective. This involved:\n",
    "    - Adding layers for complex multiplication and division.\n",
    "    - Adding layers for seperately finding the $\\hat{h}$ as an expert feature.\n",
    "    - Adding a custom layer that added rbf fast fading \n",
    "- Initially I set the $sigma_m$ to 0.33, for the reasons laid out above, but the model wasn't making any progress training. So after some research I found it written online somewhere that it is often set to $\\frac{1}{\\sqrt{2}}$ to give unity fading gain on average. This seems wrong as you should expect to lose power in the channel. However, upon trying this the model trained vastly more successfully so I may stick with this, or train at a higher SNR if I don't use this.\n",
    "- Going to try debugging it for a while, then I may try using different numbers of layers and activation functions to try and get the two supervised lines in Figures 6a and 6b from the Aoudia and Hoydis paper. After this I can try and get the reinforcement learned lines giving the same results. \n",
    "\n",
    "03/06/2019\n",
    "- Converting the interim report into the introduction and background sections of the final report. Also writing the abstract.\n",
    "\n",
    "04/06/2019\n",
    "- Fixed the average power normalisation\n",
    "- Continued converting the interim report into the start of a final report.\n",
    "- Found that the model had very significant gains if I trained it with $sigma_a = 0$, investigating whether this could be an effective way of training the $\\hat{h}$ estimation section of the model more quickly so it could then go on to learning the additive noise section later. However this is not how it would be able to work in a normal channel.\n",
    "    - On this subject it was found that the model hardly learnt anything in an RBF channel at 20db of SNR. So 40db was trialled which was more successful, but still trained extremely slowly. This is how no noise was trielled, which gave extremely fast reduction of validation loss.\n",
    "    \n",
    "05/06/2019\n",
    "- Got supervised AWGN graph for a range of SNRs.\n",
    "- Converted interim report into the first two sections of the final report. Adapted Implementation Plan, Evaluation Plan sections and moved Safety, Ethical and Legal plan into the appendix.\n",
    "- Trained supervised_rbf_sa40 for 100 epochs locally.\n",
    "\n",
    "06/06/2019\n",
    "- Added parallel for using joblib for the sweeping across SNRs\n",
    "- Tried running the best supervised_rbf_sa40 model over a range of SNRs, it gave an alright shaped curve but with error rates varying from 0.99 to 0.92, so absurdly high.\n",
    "\n",
    "07/06/2019\n",
    "- Tried out a few different combinations of activation functions\n",
    "- Wrote a function for making a general model with variable numbers of layers and activation functions.\n",
    "- Found out that rayleigh block fading has the same $\\hat{h}$ for the whole block, what I have been doing so far is \"*slow rayleigh fading*\"\n",
    "\n",
    "8-13/06/2019\n",
    "- Wrote most of the report\n",
    "\n",
    "15/06/2019\n",
    "- Tried to make a RBF autoencoder. Got it having non-zero gradients, but it couldn't see the weights of the individual transmitters and receivers. Also it took prohibitively long for block sizes of ~1000.\n",
    "\n",
    "\n",
    "##### To do\n",
    "- Investigate BCH codes, see how long I think it would take to implement one. \n",
    "- Look at a t-SNE reduced constellation diagram for the supervised_rbf_sa40 model to try and see if it has multiple symbols converging to single points.\n",
    "\n",
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_likely(posterior_probs):\n",
    "    max_vals = K.max(posterior_probs, axis=1, keepdims=True) \n",
    "    max_vals = K.cast(max_vals, 'float32')\n",
    "    geT = K.greater_equal(posterior_probs, max_vals)\n",
    "    return K.cast(geT, 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_sigma(Eb_N0_db, Rr=None, Rc=None):\n",
    "    assert(not((Rr == None)&(Rc == None)))\n",
    "    if(Rr == None):\n",
    "        Rr = Rc/2.\n",
    "    Eb_N0 = 10.**(Eb_N0_db/10.)\n",
    "    return np.sqrt(1./(2.*Rr*Eb_N0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_set(M, total_size):\n",
    "    t0 = time()\n",
    "    all_one_hot_messages = np.diag(np.ones(M))\n",
    "    perc_train = 0.75\n",
    "    perc_valid = 0.1\n",
    "\n",
    "    ## Making Data Set\n",
    "    multiple = total_size//M\n",
    "    diff = total_size - (multiple * M)\n",
    "\n",
    "    ## Get quotient \n",
    "    ## Converted the array into a list because it is significantly\n",
    "    ## faster\n",
    "    l = []\n",
    "    all_one_hot_messages_lst = all_one_hot_messages.tolist()\n",
    "    for mult in range(multiple):\n",
    "        for i in range(M):\n",
    "            l.append([all_one_hot_messages_lst[i]])\n",
    "    data = np.concatenate(l)\n",
    "\n",
    "    # Add remainder\n",
    "    random_inds = np.random.choice(np.arange(M),size=diff, replace=False)\n",
    "    extra_rows = all_one_hot_messages[random_inds,:]\n",
    "    data = np.concatenate((data, extra_rows), axis=0)\n",
    "    np.random.shuffle(data)\n",
    "    file_path = \"./data/data\"+str(M)+\".npy\"\n",
    "    np.save(file_path, data)\n",
    "    print(f\"Took {time() - t0}s\")\n",
    "    return data, file_path, all_one_hot_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MostLikelySymbol(Layer):\n",
    "    \"\"\"Return the most likely symbol from a softmax input in the\n",
    "    one hot encoded form.\n",
    "\n",
    "    This layer is only active at test time as otherwise it would\n",
    "    stop gradient propogation during training. Also it is useful\n",
    "    to train with a softmax output to encourage a decisive \n",
    "    decision and because it means you can assess confidence.\n",
    "\n",
    "    # Arguments\n",
    "        None\n",
    "\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    \"\"\"\n",
    "\n",
    "    @interfaces.legacy_gaussiannoise_support\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MostLikelySymbol, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        def most_likely():\n",
    "            max_vals = K.max(inputs, axis=1, keepdims=True) \n",
    "            max_vals = K.cast(max_vals, 'float32')\n",
    "            geT = K.greater_equal(inputs, max_vals)\n",
    "            return K.cast(geT, 'float32')            \n",
    "        return K.in_train_phase(inputs, most_likely, training=training)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        base_config = super(MostLikelySymbol, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise2(Layer):\n",
    "    \"\"\"Apply additive zero-centered Gaussian noise at both traning\n",
    "    and test time.\n",
    "\n",
    "    This is useful to mitigate overfitting\n",
    "    (you could see it as a form of random data augmentation).\n",
    "    Gaussian Noise (GS) is a natural choice as corruption process\n",
    "    for real valued inputs.\n",
    "\n",
    "    Unlike the built in GaussianNoise regularisation layer it is \n",
    "    active at both training and test time. \n",
    "\n",
    "    # Arguments\n",
    "        stddev: float, standard deviation of the noise distribution.\n",
    "\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    \"\"\"\n",
    "\n",
    "    @interfaces.legacy_gaussiannoise_support\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super(GaussianNoise2, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        def noised():\n",
    "            return inputs + K.random_normal(shape=K.shape(inputs),\n",
    "                                            mean=0.,\n",
    "                                            stddev=self.stddev)\n",
    "        return K.in_train_phase(noised, noised, training=training)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'stddev': self.stddev}\n",
    "        base_config = super(GaussianNoise2, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RayleighSlowFading(Layer):\n",
    "    \"\"\"\n",
    "    Applies Rayleigh Block (fast) Fading to the input data at both \n",
    "    training and test time.\n",
    "\n",
    "    # Arguments\n",
    "        sigma_m: float, standard deviation of the multiplicative \n",
    "        constant noise distribution.\n",
    "        sigma_a: float, standard deviation of the additive \n",
    "        constant noise distribution.\n",
    "\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma_m, sigma_a, **kwargs):\n",
    "        super(RayleighSlowFading, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.sigma_m = sigma_m\n",
    "        self.sigma_a = sigma_a\n",
    "\n",
    "    def call(self, inputs, training=None):   \n",
    "        def custom_mult(in_,col_sel,h):\n",
    "            tmp = K.tf.multiply(col_sel,h)\n",
    "            tmp_expand = K.expand_dims(tmp,axis=1)\n",
    "            return K.tf.multiply(in_,tmp_expand)\n",
    "        def complex_mult(in_,h):\n",
    "            o1 = K.constant(np.array([1,0]))\n",
    "            o2 = K.constant(np.array([0,1]))\n",
    "            h_swap = K.tf.reverse(h,[1])\n",
    "            in_swap = K.tf.reverse(in_,[2])\n",
    "\n",
    "            t1 = custom_mult(in_,o1,h)          # real*real\n",
    "            t2 = custom_mult(in_swap,o1,h_swap) # imaginary*imaginary\n",
    "            t3 = custom_mult(in_swap,o2,h)      # real*imaginary\n",
    "            t4 = custom_mult(in_,o2,h_swap)     # imaginary*real\n",
    "\n",
    "            total1 = K.tf.math.subtract(t1,t2)  # r1*r2-i1*i2\n",
    "            total2 = K.tf.math.add(t3,t4)       # r1*i2 + i1*r2\n",
    "            return K.tf.math.add(total1,total2) # r1*r2-i1*i2 + r1*i2 + i1*r2\n",
    "        def rayleigh_slow_fade():\n",
    "            hT = K.random_normal(shape=(K.shape(inputs)[0],K.shape(inputs)[2]),\n",
    "                                 mean=0.,\n",
    "                                 stddev=self.sigma_m)\n",
    "            nT = K.random_normal(shape=K.shape(inputs),\n",
    "                                 mean=0.,\n",
    "                                 stddev=self.sigma_a)\n",
    "            return K.tf.add(complex_mult(inputs,hT),nT)\n",
    "        return K.in_train_phase(rayleigh_slow_fade, rayleigh_slow_fade, training=training)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'sigma_m': self.sigma_m, 'sigma_a': self.sigma_a}\n",
    "        base_config = super(RayleighSlowFading, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RayleighBlockFading(Layer):\n",
    "    \"\"\"\n",
    "    Applies Rayleigh Block (fast) Fading to the input data at both \n",
    "    training and test time.\n",
    "\n",
    "    # Arguments\n",
    "        sigma_m: float, standard deviation of the multiplicative \n",
    "        constant noise distribution.\n",
    "        sigma_a: float, standard deviation of the additive \n",
    "        constant noise distribution.\n",
    "\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma_m, sigma_a, **kwargs):\n",
    "        super(RayleighBlockFading, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.sigma_m = sigma_m\n",
    "        self.sigma_a = sigma_a\n",
    "        self.name = \"RayleighBlockFading\"\n",
    "\n",
    "    def call(self, inputs, training=None):   \n",
    "        def custom_mult_block(in_,col_sel,h):\n",
    "            tmp = K.tf.multiply(col_sel,h)\n",
    "            expanded_twice = K.expand_dims(K.expand_dims(tmp,axis=1),axis=1)\n",
    "            return K.tf.multiply(in_,expanded_twice)\n",
    "        def complex_mult_block(in_,h):\n",
    "            o1 = K.constant(np.array([1,0]))\n",
    "            o2 = K.constant(np.array([0,1]))\n",
    "            h_swap = K.tf.reverse(h,[1])\n",
    "            in_swap = K.tf.reverse(in_,[3])\n",
    "\n",
    "            t1 = custom_mult_block(in_,o1,h)          # real*real\n",
    "            t2 = custom_mult_block(in_swap,o1,h_swap) # imaginary*imaginary\n",
    "            t3 = custom_mult_block(in_swap,o2,h)      # real*imaginary\n",
    "            t4 = custom_mult_block(in_,o2,h_swap)     # imaginary*real\n",
    "\n",
    "            total1 = K.tf.math.subtract(t1,t2)  # r1*r2-i1*i2\n",
    "            total2 = K.tf.math.add(t3,t4)       # r1*i2 + i1*r2\n",
    "            return K.tf.math.add(total1,total2) # r1*r2-i1*i2 + r1*i2 + i1*r2\n",
    "        def rayleigh_block_fade():\n",
    "            # in_.shape = (Batch_size, Block_size, Nc, 2)\n",
    "            hT = K.random_normal(shape=(K.shape(inputs)[0],K.shape(inputs)[3]),\n",
    "                                 mean=0.,\n",
    "                                 stddev=self.sigma_m)\n",
    "            nT = K.random_normal(shape=K.shape(inputs),\n",
    "                                 mean=0.,\n",
    "                                 stddev=self.sigma_a)\n",
    "            return K.tf.add(complex_mult_block(inputs,hT),nT)\n",
    "        return K.in_train_phase(rayleigh_block_fade, rayleigh_block_fade, training=training)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'sigma_m': self.sigma_m, 'sigma_a': self.sigma_a}\n",
    "        base_config = super(RayleighBlockFading, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_shapes(start, end, num_steps):\n",
    "    shapes = [start]\n",
    "    diff = (end-start)/(num_steps-1)\n",
    "    # Always start with a full dense layer\n",
    "    for i in range(1,num_steps):\n",
    "        shapes.append(int(start + i*diff))\n",
    "    return shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_complex_n_layer_lr_tanh_tapering_model(M, R, sigma, \\\n",
    "                                                hl_activation_func, \\\n",
    "                                                ol_activation_func, \\\n",
    "                                                num_layers):\n",
    "    ### Initialising Parameters\n",
    "    k = np.log2(M) # Number of bits needed to represent M \n",
    "                   # messages\n",
    "    Nc = int(round(k/R)) # Number of bit being used to represent\n",
    "                        # channel symbols being used \n",
    "                        # Number of complex channel uses\n",
    "    Nr = Nc*2 # Number of real channel uses\n",
    "\n",
    "    ### Defining Layers\n",
    "    ## TRANSMITTER\n",
    "    tx_shapes = get_layer_shapes(M, Nr, num_layers)\n",
    "    input_message = Input(shape=(M,), name=\"input\")\n",
    "    # Hidden Tx layers\n",
    "    tx1 = Dense(tx_shapes[0],activation=hl_activation_func, \\\n",
    "                name=\"tx1\")(input_message)\n",
    "    for i in range(1,num_layers-1):\n",
    "        tx1 = Dense(tx_shapes[i],activation=hl_activation_func, \\\n",
    "                    name=(\"tx\"+str(i+1)))(tx1)\n",
    "    # Final layer with a different activation function to capture non\n",
    "    # linearity\n",
    "    tx_n = Dense(tx_shapes[-1],activation=ol_activation_func, \\\n",
    "                 name=(\"tx\"+str(num_layers)))(tx1)\n",
    "    # Reshape it to complex channel symbols\n",
    "    tx_complex = Lambda(lambda x : K.reshape(x, (-1,Nc,2)),\n",
    "                       output_shape=(Nc,2), \\\n",
    "                        name=\"tx_reshape\")(tx_n)\n",
    "\n",
    "    # Normalisation Layer\n",
    "    tx_norm = Lambda(lambda x : K.l2_normalize(x,axis=2),\n",
    "                     output_shape=(Nc,2), name=\"tx_norm\")\\\n",
    "                        (tx_complex)\n",
    "    tx_norm_scaled = Lambda(lambda x : K.tf.multiply(np.float32(np.sqrt(2)), x),\n",
    "                      output_shape=(Nc,2), name=\"tx_norm_scaled\")\\\n",
    "                        (tx_norm)\n",
    "    \n",
    "    # Add Noise \n",
    "    noise = GaussianNoise2(sigma)(tx_norm_scaled)\n",
    "\n",
    "    ## RECIEVER\n",
    "    # Flatten the input\n",
    "    noise_flat = Lambda(lambda x : K.reshape(x, (-1,Nr)),\n",
    "                       output_shape=(Nr,),\\\n",
    "                        name=\"noise_flat\")(noise)\n",
    "    # First layer with the different activation function\n",
    "    # to capture non-linearity and for symmetry with the \n",
    "    # transmitter.\n",
    "    rx1 = Dense(tx_shapes[-2],activation=ol_activation_func, name=\"rx1\")\\\n",
    "                (noise_flat)\n",
    "    # Hidden Rx Layers\n",
    "    if(num_layers >= 3):\n",
    "        layer_ind = -3\n",
    "    else:\n",
    "        layer_ind = -2\n",
    "    rx_i = Dense(tx_shapes[layer_ind],activation=hl_activation_func, \\\n",
    "                name=\"rx2\")(rx1)\n",
    "    for i in range(2,num_layers):\n",
    "        ind = max(0,num_layers - 2 - i)\n",
    "        rx_i = Dense(tx_shapes[ind],activation=hl_activation_func, \\\n",
    "                    name=(\"rx\"+str(i+1)))(rx_i)\n",
    "    # Dense layer with softmax activation\n",
    "    rx_softmax = Dense(tx_shapes[0],activation='softmax', \\\n",
    "                       name=\"rx_softmax\")(rx_i)\n",
    "    \n",
    "    # Select the symbols with the maximum probabilities\n",
    "    ml_symbs = MostLikelySymbol()(rx_softmax)\n",
    "    \n",
    "    ###Defining the models\n",
    "    autoencoder = Model(input_message, rx_softmax)\n",
    "    ## Model the Tx and Rx seperately as well\n",
    "    # Model the Tx\n",
    "    transmitter = Model(input_message, tx_norm_scaled)\n",
    "    # Model the Tx plus the noise\n",
    "    channel_sym_with_noise = Model(input_message, noise)\n",
    "    channel_symbol = Input(shape=(Nr,))\n",
    "    # Take the last layer of the autoencoder model\n",
    "    reciever_layers = autoencoder.layers[-(num_layers+1)](channel_symbol)\n",
    "    for i in range(num_layers):\n",
    "        reciever_layers = autoencoder.layers[-(num_layers-i)](reciever_layers)\n",
    "\n",
    "    # Create a model of the reciever\n",
    "    reciever = Model(channel_symbol, reciever_layers)\n",
    "    autoencoder_symbs = Model(input_message,ml_symbs) \n",
    "    \n",
    "    # Compile the model\n",
    "    autoencoder.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")\n",
    "    return autoencoder, transmitter, reciever,\\\n",
    "            autoencoder_symbs, k, Nc, Nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mult(in_,col_sel,h):\n",
    "    tmp = K.tf.multiply(col_sel,h)\n",
    "    tmp_expand = K.expand_dims(tmp,axis=1)\n",
    "    return K.tf.multiply(in_,tmp_expand)\n",
    "\n",
    "def complex_div(in_lst):\n",
    "    in_,h = in_lst\n",
    "    o1 = K.constant(np.array([1,0]))\n",
    "    o2 = K.constant(np.array([0,1]))\n",
    "    h_swap = K.tf.reverse(h,[1])\n",
    "    in_swap = K.tf.reverse(in_,[2])\n",
    "\n",
    "    t1 = custom_mult(in_,o1,h)          # real*real\n",
    "    t2 = custom_mult(in_swap,o1,h_swap) # imaginary*imaginary\n",
    "    t3 = custom_mult(in_swap,o2,h)      # real*imaginary\n",
    "    t4 = custom_mult(in_,o2,h_swap)     # imaginary*real\n",
    "\n",
    "    total1 = K.tf.math.add(t1,t2)        # xr*hr-xi*hi\n",
    "    total2 = K.tf.math.subtract(t4,t3)   # xr*hi-xi*hr\n",
    "    total = K.tf.math.add(total1,total2) # xr*hr+xi*hi + xr*hi-xi*hr\n",
    "\n",
    "    scale_factor = K.sum(K.tf.math.square(h), axis=1) # hr^2 + hi^2\n",
    "    scale_factor_expanded_twice = K.expand_dims(K.expand_dims(scale_factor, axis=1), axis=2)\n",
    "    return K.tf.math.divide(total,scale_factor_expanded_twice) \n",
    "\n",
    "def complex_mult(in_,h):\n",
    "    o1 = K.constant(np.array([1,0]))\n",
    "    o2 = K.constant(np.array([0,1]))\n",
    "    h_swap = K.tf.reverse(h,[1])\n",
    "    in_swap = K.tf.reverse(in_,[2])\n",
    "\n",
    "    t1 = custom_mult(in_,o1,h)          # real*real\n",
    "    t2 = custom_mult(in_swap,o1,h_swap) # imaginary*imaginary\n",
    "    t3 = custom_mult(in_swap,o2,h)      # real*imaginary\n",
    "    t4 = custom_mult(in_,o2,h_swap)     # imaginary*real\n",
    "\n",
    "    total1 = K.tf.math.subtract(t1,t2)  # r1*r2-i1*i2\n",
    "    total2 = K.tf.math.add(t3,t4)       # r1*i2 + i1*r2\n",
    "    return K.tf.math.add(total1,total2) # r1*r2-i1*i2 + r1*i2 + i1*r2\n",
    "\n",
    "sigma_m = 0.33\n",
    "sigma_a = 1\n",
    "\n",
    "def rayleigh_slow_fade(in_, sigma_m, sigma_a):\n",
    "    # in_.shape = (Batch_size, Nc, 2)\n",
    "    hT = K.random_normal(shape=(K.shape(in_)[0],K.shape(in_)[2]),\n",
    "                         mean=0.,\n",
    "                         stddev=sigma_m)\n",
    "    nT = K.random_normal(shape=K.shape(in_),\n",
    "                         mean=0.,\n",
    "                         stddev=sigma_a)\n",
    "    return K.tf.add(complex_mult(in_,hT),nT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mult_block(in_,col_sel,h):\n",
    "    tmp = K.tf.multiply(col_sel,h)\n",
    "    expanded_twice = K.expand_dims(K.expand_dims(tmp,axis=1),axis=1)\n",
    "    return K.tf.multiply(in_,expanded_twice)\n",
    "\n",
    "def complex_div_block(in_lst):\n",
    "    in_,h = in_lst\n",
    "    o1 = K.constant(np.array([1,0]))\n",
    "    o2 = K.constant(np.array([0,1]))\n",
    "    h_swap = K.tf.reverse(h,[1])\n",
    "    in_swap = K.tf.reverse(in_,[3])\n",
    "\n",
    "    t1 = custom_mult_block(in_,o1,h)          # real*real\n",
    "    t2 = custom_mult_block(in_swap,o1,h_swap) # imaginary*imaginary\n",
    "    t3 = custom_mult_block(in_swap,o2,h)      # real*imaginary\n",
    "    t4 = custom_mult_block(in_,o2,h_swap)     # imaginary*real\n",
    "\n",
    "    total1 = K.tf.math.add(t1,t2)        # xr*hr-xi*hi\n",
    "    total2 = K.tf.math.subtract(t4,t3)   # xr*hi-xi*hr\n",
    "    total = K.tf.math.add(total1,total2) # xr*hr+xi*hi + xr*hi-xi*hr\n",
    "\n",
    "    scale_factor = K.sum(K.tf.math.square(h), axis=1) # hr^2 + hi^2\n",
    "    # Will probably need updating to expand three times\n",
    "    scale_factor_expanded_thrice = K.expand_dims(\n",
    "                                    K.expand_dims(\n",
    "                                     K.expand_dims(scale_factor, axis=1), axis=2), axis=3)\n",
    "    return K.tf.math.divide(total,scale_factor_expanded_thrice) \n",
    "\n",
    "def complex_mult_block(in_,h):\n",
    "    o1 = K.constant(np.array([1,0]))\n",
    "    o2 = K.constant(np.array([0,1]))\n",
    "    h_swap = K.tf.reverse(h,[1])\n",
    "    in_swap = K.tf.reverse(in_,[3])\n",
    "\n",
    "    t1 = custom_mult_block(in_,o1,h)          # real*real\n",
    "    t2 = custom_mult_block(in_swap,o1,h_swap) # imaginary*imaginary\n",
    "    t3 = custom_mult_block(in_swap,o2,h)      # real*imaginary\n",
    "    t4 = custom_mult_block(in_,o2,h_swap)     # imaginary*real\n",
    "\n",
    "    total1 = K.tf.math.subtract(t1,t2)  # r1*r2-i1*i2\n",
    "    total2 = K.tf.math.add(t3,t4)       # r1*i2 + i1*r2\n",
    "    return K.tf.math.add(total1,total2) # r1*r2-i1*i2 + r1*i2 + i1*r2\n",
    "\n",
    "def rayleigh_block_fade(in_, sigma_m, sigma_a):\n",
    "    # in_.shape = (Batch_size, Block_size, Nc, 2)\n",
    "    # Gives the same multiplicative factor h to all\n",
    "    # the messages in the block.\n",
    "    # Adds different additive noise for each bit.\n",
    "    hT = K.random_normal(shape=(K.shape(in_)[0],K.shape(in_)[3]),\n",
    "                         mean=0.,\n",
    "                         stddev=sigma_m)\n",
    "    nT = K.random_normal(shape=K.shape(in_),\n",
    "                         mean=0.,\n",
    "                         stddev=sigma_a)\n",
    "    return K.tf.add(complex_mult_block(in_,hT),nT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_elements(tensor):\n",
    "    dim_lst = tensor.get_shape().as_list()\n",
    "    num_elements = 1\n",
    "    for dim in dim_lst:\n",
    "        if(dim != None):\n",
    "            num_elements *= dim\n",
    "    return num_elements\n",
    "\n",
    "def get_avg_power(signal):\n",
    "    # Gets the power according to the definition \n",
    "    # on the top of the fraction in Eq 8 the\n",
    "    # Aoudia and Hoydis paper.\n",
    "    assert(len(signal.shape) == 3)\n",
    "    num_elements = get_num_elements(signal)\n",
    "    sq = K.tf.math.square(signal)\n",
    "    sq_sum_all = K.sum(K.sum(K.sum(sq,axis=2),axis=1),axis=0)\n",
    "    return K.tf.math.divide(sq_sum_all,K.tf.dtypes.cast(num_elements,tf.float32))\n",
    "\n",
    "def avg_power_normalise(signal):\n",
    "    avg_power = get_avg_power(signal)\n",
    "    return K.tf.math.divide(signal,K.tf.math.sqrt(avg_power))\n",
    "#     return K.tf.math.divide(signal,avg_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average power constraint layer\n",
    "B = 1000000\n",
    "Nc = 4\n",
    "tx_T = K.constant(np.random.normal(0,2,(B,Nc,2)))\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    tx_T.eval(session=sess)\n",
    "# tx_T.eval(session=sess)\n",
    "get_avg_power(avg_power_normalise(tx_T)).eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6666666"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_power(K.constant(np.array([[[0,1,2]]]))).eval(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWGN alternating model has a single dense ReLu layer, I'm going to use the topology I found to be best in the other notebook, two layers, tapered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rsf_autoencoder(k, Nc, sigma_m, sigma_a):\n",
    "#     k = np.log2(M) # Number of bits needed to represent M \n",
    "#                    # messages\n",
    "    M = 2**k\n",
    "    Nr = Nc*2 # Number of real channel uses\n",
    "    \n",
    "    ### Defining Layers\n",
    "    ## TRANSMITTER\n",
    "    tx_input_message = Input(shape=(M,), name=\"input\")\n",
    "    tx1 = keras.layers.Embedding(M, M, name=\"embedding\")(tx_input_message)\n",
    "    tx1 = keras.layers.ELU(alpha=1.0, name=\"embed_ELU\")(tx1)\n",
    "    # Reshape it to complex channel symbols\n",
    "    tx_flat = Reshape((M*M,), name=\"post_embed_flatten\")(tx1)  \n",
    "    tx2 = Dense(Nr,activation=\"linear\", name=\"tx2\")(tx_flat)\n",
    "    # Reshape it to complex channel symbols\n",
    "    tx_complex = Reshape((Nc,2), name=\"tx_reshape\")(tx2)\n",
    "    # Normalisation Layer\n",
    "    tx_norm = Lambda(lambda x : avg_power_normalise(x),\n",
    "                     output_shape=(Nc,2), name=\"tx_norm\")\\\n",
    "                    (tx_complex)\n",
    "    tx_norm_scaled = Lambda(lambda x : K.tf.multiply(np.float32(np.sqrt(2)), x),\n",
    "                      output_shape=(Nc,2), name=\"tx_norm_scaled\")\\\n",
    "                        (tx_norm)\n",
    "    # Add AWGN Noise \n",
    "    noise = RayleighSlowFading(sigma_m,sigma_a)(tx_norm)\n",
    "\n",
    "    ## receiver\n",
    "    # Flatten the input\n",
    "    noise_flat = Reshape((Nr,), name=\"noise_flat\")(noise)\n",
    "    # First layer with the different activation function\n",
    "    # to capture non-linearity and for symmetry with the \n",
    "    # transmitter.\n",
    "    h1 = Dense(20,activation=\"tanh\", name=\"h1\")\\\n",
    "                (noise_flat)\n",
    "    h_hat = Dense(2,activation=\"linear\", name=\"h_hat\")\\\n",
    "                (h1)\n",
    "    y_over_h = Lambda(lambda x : complex_div(x),\n",
    "                      output_shape=(Nc,2),\\\n",
    "                      name=\"y_over_h\")([noise, h_hat])\n",
    "    y_over_h_flat = Reshape((Nr,), name=\"y_over_h_flat\")(y_over_h)\n",
    "    rx1 = Dense(M,activation=\"relu\", name=\"rx1\")\\\n",
    "                (y_over_h_flat)\n",
    "    # Dense layer with softmax activation\n",
    "    rx_softmax = Dense(M,activation='softmax', \\\n",
    "                       name=\"rx_softmax\")(rx1)\n",
    "    # Select the symbols with the maximum probabilities\n",
    "    ml_symbs = MostLikelySymbol()(rx_softmax)\n",
    "\n",
    "    ###Defining the models\n",
    "    # Model the Tx\n",
    "    autoencoder = Model(tx_input_message, rx_softmax)\n",
    "    autoencoder_symbs = Model(tx_input_message,ml_symbs) \n",
    "    transmitter = Model(tx_input_message, tx_norm_scaled)\n",
    "    # Model the Tx plus the noise\n",
    "    channel_sym_with_noise = Model(tx_input_message, noise)\n",
    "    # Model the whole receiver (including h_est)\n",
    "    channel_symbol = Input(shape=(Nc,2), name=\"channel_symbols\")\n",
    "    num_layers = 7\n",
    "    receiver_layers = autoencoder.layers[-num_layers](channel_symbol)\n",
    "    for i in range(2):\n",
    "        receiver_layers = autoencoder.layers[-(num_layers-1-i)](receiver_layers)\n",
    "    h_estimator = Model(channel_symbol, receiver_layers)\n",
    "    receiver_layers = autoencoder.layers[-(num_layers-3)]([channel_symbol,receiver_layers])\n",
    "    for i in range(3,num_layers-1):\n",
    "        receiver_layers = autoencoder.layers[-(num_layers-1-i)](receiver_layers)\n",
    "    whole_receiver = Model(channel_symbol, receiver_layers)\n",
    "    # Create a model of only the receiver\n",
    "    rx_in = Input(shape=(Nc,2), name=\"rx_in\")\n",
    "    rx_layers = autoencoder.layers[-3](rx_in)\n",
    "    rx_layers = autoencoder.layers[-2](rx_layers)\n",
    "    rx_layers = autoencoder.layers[-1](rx_layers)\n",
    "    receiver = Model(rx_in, rx_layers)\n",
    "    # Compile the model\n",
    "    autoencoder.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")\n",
    "\n",
    "    return autoencoder, autoencoder_symbs, transmitter, \\\n",
    "        channel_sym_with_noise, receiver, whole_receiver,\\\n",
    "        h_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rsf_autoencoder_model(k, Nc, sigma_m, sigma_a):\n",
    "#     k = np.log2(M) # Number of bits needed to represent M \n",
    "#                    # messages\n",
    "    M = 2**k\n",
    "    Nr = Nc*2 # Number of real channel uses\n",
    "    \n",
    "    ### Defining Layers\n",
    "    ## TRANSMITTER\n",
    "    def Transmitter():\n",
    "        tx_input_message = Input(shape=(M,), name=\"tx_input_message\")\n",
    "        tx1 = keras.layers.Embedding(M, M, name=\"embedding\")(tx_input_message)\n",
    "        tx1 = keras.layers.ELU(alpha=1.0, name=\"embed_ELU\")(tx1)\n",
    "        # Reshape it to complex channel symbols\n",
    "        tx_flat = Reshape((M*M,), name=\"post_embed_flatten\")(tx1)  \n",
    "        tx2 = Dense(Nr,activation=\"linear\", name=\"tx2\")(tx_flat)\n",
    "        # Reshape it to complex channel symbols\n",
    "        tx_complex = Reshape((Nc,2), name=\"tx_reshape\")(tx2)\n",
    "        # Normalisation Layer\n",
    "        tx_norm = Lambda(lambda x : avg_power_normalise(x),\n",
    "                         output_shape=(Nc,2), name=\"tx_norm\")\\\n",
    "                        (tx_complex)\n",
    "        tx_norm_scaled = Lambda(lambda x : K.tf.multiply(np.float32(np.sqrt(2)), x),\n",
    "                          output_shape=(Nc,2), name=\"tx_norm_scaled\")\\\n",
    "                            (tx_norm)\n",
    "        return Model(tx_input_message, tx_norm_scaled, name=\"Transmitter\")\n",
    "    \n",
    "    def HEstimator():\n",
    "        h_est_input = Input(shape=(Nc,2), name=\"h_est_input\")\n",
    "        noise_flat = Flatten(name=\"noise_flat\")(h_est_input)\n",
    "        # First layer with the different activation function\n",
    "        # to capture non-linearity and for symmetry with the \n",
    "        # transmitter.\n",
    "        h1 = Dense(20,activation=\"tanh\", name=\"h1\")\\\n",
    "                    (noise_flat)\n",
    "        h_hat = Dense(2,activation=\"linear\", name=\"h_hat\")\\\n",
    "                    (h1)\n",
    "        return Model(h_est_input,h_hat, name=\"HEstimator\")\n",
    "    \n",
    "    def Receiver():\n",
    "        receiver_input = Input(shape=(Nc,2), name=\"receiver_input\")\n",
    "        y_over_h_flat = Flatten(name=\"y_over_h_flat\")(receiver_input)\n",
    "        rx1 = Dense(M,activation=\"relu\", name=\"rx1\")\\\n",
    "                    (y_over_h_flat)\n",
    "        # Dense layer with softmax activation\n",
    "        rx_softmax = Dense(M,activation='softmax', \\\n",
    "                           name=\"rx_softmax\")(rx1)\n",
    "        return Model(receiver_input,rx_softmax, name=\"Receiver\")\n",
    "    \n",
    "    # Overall Autoencoder\n",
    "    ae_input_message = Input(shape=(M,), name=\"ae_input_message\")\n",
    "    channel_symbols = Transmitter()(ae_input_message)\n",
    "    noised = RayleighSlowFading(sigma_m,sigma_a)(channel_symbols)\n",
    "    h_hat = HEstimator()(noised)\n",
    "    y_over_h = Lambda(lambda x : complex_div(x),\n",
    "                      output_shape=(Nc,2),\\\n",
    "                      name=\"y_over_h\")([noised, h_hat])\n",
    "    rx_softmax = Receiver()(y_over_h)\n",
    "    ml_symbs = MostLikelySymbol()(rx_softmax)\n",
    "   \n",
    "    ###Defining the models\n",
    "    autoencoder = Model(ae_input_message, rx_softmax)\n",
    "    autoencoder_symbs = Model(ae_input_message,ml_symbs) \n",
    "    transmitter = Model(ae_input_message, channel_symbols)\n",
    "    channel_sym_with_noise = Model(ae_input_message, noised)\n",
    "    ## Submodel layers for rx and h_est model\n",
    "    channel_symbol = Input(shape=(Nc,2), name=\"channel_symbol\")\n",
    "    h_hat2 = autoencoder.layers[-3](channel_symbol)\n",
    "    rx_input = Input(shape=(Nc,2), name=\"rx_input\")\n",
    "    received_pdf2 = autoencoder.layers[-1](rx_input)\n",
    "    h_est = Model(channel_symbol,h_hat2)\n",
    "    receiver = Model(rx_input,received_pdf2)\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    autoencoder.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")\n",
    "\n",
    "    return autoencoder, autoencoder_symbs, transmitter, \\\n",
    "        channel_sym_with_noise, receiver, h_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rsf_autoencoder_custom(k, Nc, sigma_m, sigma_a, \\\n",
    "                               num_layers, tx_activation_func, \\\n",
    "                               rx_activation_func, rx_end_activation_func):\n",
    "#     k = np.log2(M) # Number of bits needed to represent M \n",
    "#                    # messages\n",
    "    M = 2**k\n",
    "    Nr = Nc*2 # Number of real channel uses\n",
    "    \n",
    "    ### Defining Layers\n",
    "    ## TRANSMITTER\n",
    "    # ELU layer captures non-linearity\n",
    "    tx_input_message = Input(shape=(M,), name=\"input\")\n",
    "    tx1 = keras.layers.Embedding(M, M, name=\"embedding\")(tx_input_message)\n",
    "    tx1 = keras.layers.ELU(alpha=1.0, name=\"embed_ELU\")(tx1)\n",
    "    # Reshape it to complex channel symbols\n",
    "    tx_flat = Reshape((M*M,), name=\"post_embed_flatten\")(tx1)  \n",
    "    # Tx encoding layers\n",
    "    for i in range(num_layers-2):\n",
    "        width = Nr if i == 0 else M\n",
    "        tx_flat = Dense(width,activation=tx_activation_func, name=\"tx_\"+str(i+2))(tx_flat)\n",
    "    tx_n = Dense(Nr,activation=\"tanh\", name=\"tx_\"+str(num_layers))(tx_flat)\n",
    "    # Reshape it to complex channel symbols\n",
    "    tx_complex = Reshape((Nc,2), name=\"tx_reshape\")(tx_n)\n",
    "    # Normalisation Layer\n",
    "    tx_norm = Lambda(lambda x : avg_power_normalise(x),\n",
    "                     output_shape=(Nc,2), name=\"tx_norm\")\\\n",
    "                    (tx_complex)\n",
    "    tx_norm_scaled = Lambda(lambda x : K.tf.multiply(np.float32(np.sqrt(2)), x),\n",
    "                      output_shape=(Nc,2), name=\"tx_norm_scaled\")\\\n",
    "                        (tx_norm)\n",
    "    # Add AWGN Noise \n",
    "    noise = RayleighSlowFading(sigma_m,sigma_a)(tx_norm)\n",
    "\n",
    "    ## receiver\n",
    "    # Flatten the input\n",
    "    noise_flat = Reshape((Nr,), name=\"noise_flat\")(noise)\n",
    "    # First layer with the different activation function\n",
    "    # to capture non-linearity and for symmetry with the \n",
    "    # transmitter.\n",
    "    h1 = Dense(20,activation=\"tanh\", name=\"h1\")\\\n",
    "                (noise_flat)\n",
    "    h_hat = Dense(2,activation=rx_activation_func, name=\"h_hat\")\\\n",
    "                (h1)\n",
    "    y_over_h = Lambda(lambda x : complex_div(x),\n",
    "                      output_shape=(Nc,2),\\\n",
    "                      name=\"y_over_h\")([noise, h_hat])\n",
    "    y_over_h_flat = Reshape((Nr,), name=\"y_over_h_flat\")(y_over_h)\n",
    "    rx_1 = Dense(M,activation=rx_activation_func, name=\"rx_1\")\\\n",
    "                (y_over_h_flat)\n",
    "    for i in range(num_layers-3):\n",
    "        rx_1 = Dense(M,activation=rx_activation_func, name=\"rx_\"+str(i+2))(rx_1)\n",
    "    if(num_layers > 2):\n",
    "        rx_1 = Dense(M,activation=rx_end_activation_func, name=\"rx_\"+str(num_layers-1))(rx_1)\n",
    "    # Dense layer with softmax activation\n",
    "    rx_softmax = Dense(M,activation='softmax', \\\n",
    "                       name=\"rx_softmax\")(rx_1)\n",
    "    # Select the symbols with the maximum probabilities\n",
    "    ml_symbs = MostLikelySymbol()(rx_softmax)\n",
    "\n",
    "    ###Defining the models\n",
    "    # Model the Tx\n",
    "    autoencoder = Model(tx_input_message, rx_softmax)\n",
    "    autoencoder_symbs = Model(tx_input_message,ml_symbs) \n",
    "    transmitter = Model(tx_input_message, tx_norm_scaled)\n",
    "    # Model the Tx plus the noise\n",
    "    channel_sym_with_noise = Model(tx_input_message, noise)\n",
    "    # Model the Rx\n",
    "    channel_symbol = Input(shape=(Nc,2), name=\"rx_in\")\n",
    "    total_layers = 6+num_layers\n",
    "    # First four layers getting h_hat and reshaping\n",
    "    receiver_layers = autoencoder.layers[-total_layers](channel_symbol)\n",
    "    for i in range(3):\n",
    "        receiver_layers = autoencoder.layers[-(total_layers-1-i)](receiver_layers)\n",
    "    # Needs different format for the y_over_h layer\n",
    "    receiver_layers = autoencoder.layers[-(num_layers+2)]([channel_symbol,receiver_layers])\n",
    "    for i in range(0,num_layers+1):\n",
    "        receiver_layers = autoencoder.layers[i-num_layers-1](receiver_layers)\n",
    "    # Create a model of the receiver\n",
    "    receiver = Model(channel_symbol, receiver_layers)\n",
    "    # Compile the model\n",
    "    autoencoder.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")\n",
    "\n",
    "    return autoencoder, autoencoder_symbs, transmitter, \\\n",
    "        channel_sym_with_noise, receiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_rbf_rx_and_tx_models(M, Nc, sigma_m, sigma_a):\n",
    "    k = np.log2(M) # Number of bits needed to represent M \n",
    "                   # messages\n",
    "    Nr = Nc*2 # Number of real channel uses\n",
    "\n",
    "    ### Defining Layers\n",
    "    ## TRANSMITTER\n",
    "    tx_input_message = Input(shape=(M,), name=\"input\")\n",
    "    tx1 = keras.layers.Embedding(M, M*M)(tx_input_message)\n",
    "    tx1 = keras.layers.ELU(alpha=1.0)(tx1)\n",
    "    tx2 = Dense(Nr,activation=\"linear\", name=\"tx2\")(tx1)\n",
    "\n",
    "    # Reshape it to complex channel symbols\n",
    "    tx_complex = Lambda(lambda x : K.reshape(x, (-1,Nc,2)),\n",
    "                       output_shape=(Nc,2), \\\n",
    "                        name=\"tx_reshape\")(tx2)\n",
    "\n",
    "    # Normalisation Layer\n",
    "    tx_norm = Lambda(lambda x : K.l2_normalize(x,axis=2),\n",
    "                     output_shape=(Nc,2), name=\"tx_norm\")\\\n",
    "                        (tx_complex)\n",
    "    tx_norm_scaled = Lambda(lambda x : K.tf.multiply(np.float32(np.sqrt(Nr)), x),\n",
    "                      output_shape=(Nc,2), name=\"tx_norm_scaled\")\\\n",
    "                        (tx_norm)\n",
    "    # Add AWGN Noise \n",
    "    noise = RayleighSlowFading(sigma_m,sigma_a)(tx_norm_scaled)\n",
    "\n",
    "    ## RECIEVER\n",
    "    rx_input_message = Input(shape=(Nc,2), name=\"input\")\n",
    "    # Flatten the input\n",
    "    noise_flat = Lambda(lambda x : K.reshape(x, (-1,Nr)),\n",
    "                       output_shape=(Nr,),\\\n",
    "                        name=\"noise_flat\")(rx_input_message)\n",
    "    # First layer with the different activation function\n",
    "    # to capture non-linearity and for symmetry with the \n",
    "    # transmitter.\n",
    "    h1 = Dense(20,activation=\"tanh\", name=\"h1\")\\\n",
    "                (noise_flat)\n",
    "    h_hat = Dense(2,activation=\"linear\", name=\"h_hat\")\\\n",
    "                (h1)\n",
    "    h_complex = Lambda(lambda x : K.reshape(x, (-1,1,2)),\n",
    "                       output_shape=(Nc,2),\\\n",
    "                       name=\"h_complex\")(h_hat)\n",
    "    y_over_h = Lambda(lambda x : complex_div(x,h_complex),\n",
    "                      output_shape=(Nc,2),\\\n",
    "                      name=\"y_over_h\")(rx_input_message)\n",
    "    y_over_h_flat = Lambda(lambda x : K.reshape(x, (-1,Nr)),\n",
    "                       output_shape=(Nr,),\\\n",
    "                        name=\"noise_flat\")(y_over_h)\n",
    "    rx1 = Dense(M,activation=\"relu\", name=\"rx1\")\\\n",
    "                (y_over_h_flat)\n",
    "    # Dense layer with softmax activation\n",
    "    rx_softmax = Dense(M,activation='softmax', \\\n",
    "                       name=\"rx_softmax\")(rx1)\n",
    "    # Select the symbols with the maximum probabilities\n",
    "    ml_symbs = MostLikelySymbol()(rx_softmax)\n",
    "\n",
    "    ###Defining the models\n",
    "    # Model the Tx\n",
    "    transmitter = Model(tx_input_message, tx_norm_scaled)\n",
    "    # Model the Tx plus the noise\n",
    "    channel_sym_with_noise = Model(tx_input_message, noise)\n",
    "    # Model the Rx\n",
    "    receiver = Model(rx_input_message,rx_softmax)\n",
    "    receiver_symbs = Model(rx_input_message,ml_symbs)\n",
    "\n",
    "    # Compile the model\n",
    "    transmitter.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")\n",
    "    receiver.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")\n",
    "    return transmitter, channel_sym_with_noise, receiver,\\\n",
    "                receiver_symbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_awgn_rx_and_tx_models(M, Nc, sigma):\n",
    "    k = np.log2(M) # Number of bits needed to represent M \n",
    "                   # messages\n",
    "    Nr = Nc*2 # Number of real channel uses\n",
    "\n",
    "    ### Defining Layers\n",
    "    ## TRANSMITTER\n",
    "    tx_input_message = Input(shape=(M,), name=\"input\")\n",
    "    tx1 = keras.layers.Embedding(M, M*M)(tx_input_message)\n",
    "    tx1 = keras.layers.ELU(alpha=1.0)(tx1)\n",
    "    tx2 = Dense(Nr,activation=\"linear\", name=\"tx2\")(tx1)\n",
    "\n",
    "    # Reshape it to complex channel symbols\n",
    "    tx_complex = Lambda(lambda x : K.reshape(x, (-1,Nc,2)),\n",
    "                       output_shape=(Nc,2), \\\n",
    "                        name=\"tx_reshape\")(tx2)\n",
    "\n",
    "    # Normalisation Layer\n",
    "    tx_norm = Lambda(lambda x : K.l2_normalize(x,axis=2),\n",
    "                     output_shape=(Nc,2), name=\"tx_norm\")\\\n",
    "                        (tx_complex)\n",
    "    tx_norm_scaled = Lambda(lambda x : K.tf.multiply(np.float32(np.sqrt(Nr)), x),\n",
    "                      output_shape=(Nc,2), name=\"tx_norm_scaled\")\\\n",
    "                        (tx_norm)\n",
    "    # Add AWGN Noise \n",
    "    noise = GaussianNoise2(sigma)(tx_norm_scaled)\n",
    "\n",
    "    ## RECIEVER\n",
    "    rx_input_message = Input(shape=(M,), name=\"input\")\n",
    "    # Flatten the input\n",
    "    noise_flat = Lambda(lambda x : K.reshape(x, (-1,Nr)),\n",
    "                       output_shape=(Nr,),\\\n",
    "                        name=\"noise_flat\")(rx_input_message)\n",
    "    # First layer with the different activation function\n",
    "    # to capture non-linearity and for symmetry with the \n",
    "    # transmitter.\n",
    "    rx1 = Dense(M,activation=\"relu\", name=\"rx1\")\\\n",
    "                (noise_flat)\n",
    "    # Dense layer with softmax activation\n",
    "    rx_softmax = Dense(M,activation='softmax', \\\n",
    "                       name=\"rx_softmax\")(rx1)\n",
    "    # Select the symbols with the maximum probabilities\n",
    "    ml_symbs = MostLikelySymbol()(rx_softmax)\n",
    "\n",
    "    ###Defining the models\n",
    "    # Model the Tx\n",
    "    transmitter = Model(tx_input_message, tx_norm_scaled)\n",
    "    # Model the Tx plus the noise\n",
    "    channel_sym_with_noise = Model(tx_input_message, noise)\n",
    "    # Model the Rx\n",
    "    receiver = Model(rx_input_message,rx_softmax)\n",
    "    receiver_symbs = Model(rx_input_message,ml_symbs)\n",
    "\n",
    "    # Compile the model\n",
    "    transmitter.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")\n",
    "    receiver.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")    \n",
    "    return transmitter, channel_sym_with_noise, receiver,\\\n",
    "                receiver_symbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_to_sigma_a(db):\n",
    "#     return 10**(-db/20)\n",
    "    return 10**(-db/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_block_error_rate(test_data, pred_symbs):\n",
    "    errors = (test_data != pred_symbs)\n",
    "    block_errors = errors.any(axis=1)\n",
    "    return block_errors.sum()/block_errors.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complex_tapered_noise_bler_SNR(M, R, SNR, weights_file_path, \\\n",
    "                                   test_data, hl_activation_func, \\\n",
    "                                   ol_activation_func, num_layers):\n",
    "    ## Get noise std_dev \n",
    "    noise_std = db_to_sigma_a(SNR)   \n",
    "    ## Make new model with loaded weights\n",
    "    autoencoder8_8_tap_nl, _, _, autoencoder_symbs8_8_tap_nl, \\\n",
    "        _, _, _ \\\n",
    "        = make_complex_n_layer_lr_tanh_tapering_model(M, R, noise_std, \\\n",
    "                                                      hl_activation_func, \\\n",
    "                                                      ol_activation_func, \\\n",
    "                                                      num_layers)    \n",
    "    autoencoder8_8_tap_nl.load_weights(weights_file_path, by_name=True)    \n",
    "    ## Check Accuracy on test set\n",
    "    pred_symbs = autoencoder_symbs8_8_tap_nl.predict(test_data)\n",
    "    return get_block_error_rate(test_data, pred_symbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_supervised_rsf_bler_SNR(k, Nc, sigma_m, SNR, \\\n",
    "                                weights_file_path, \\\n",
    "                                test_data):\n",
    "    ## Get noise std_dev \n",
    "    sigma_a = db_to_sigma_a(SNR)   \n",
    "    ## Make new model with loaded weights\n",
    "    autoencoder_rsf_tmp, autoencoder_symbs_rsf_tmp, \\\n",
    "        _, _, _, _,_ \\\n",
    "        = get_rsf_autoencoder(k, Nc, sigma_m, sigma_a)\n",
    "    autoencoder_rsf_tmp.load_weights(weights_file_path, by_name=True)\n",
    "\n",
    "    ## Check Accuracy on test set\n",
    "    pred_symbs = autoencoder_symbs_rsf_tmp.predict(test_data)\n",
    "    return get_block_error_rate(test_data, pred_symbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rbf_autoencoder(k, Nc, block_size, sigma_m, sigma_a):\n",
    "    # Internal Parameters\n",
    "    M = 2**k\n",
    "    Nr = Nc*2 # Number of real channel uses\n",
    "\n",
    "    ## Transmitter\n",
    "    def Transmitter():\n",
    "        # (?,M) -> (?,Nc,2)   \n",
    "        tx_input_message = Input(shape=(M,), name=\"tx_input\")\n",
    "        tx1 = keras.layers.Embedding(M, M, name=\"embedding\")(tx_input_message)\n",
    "        tx1 = keras.layers.ELU(alpha=1.0, name=\"embed_ELU\")(tx1)\n",
    "        tx_flat = Flatten(name=\"post_embed_flatten\")(tx1)  \n",
    "        tx2 = Dense(Nr,activation=\"linear\", name=\"tx2\")(tx_flat)\n",
    "        tx_complex = Reshape((Nc,2), name=\"tx_reshape\")(tx2)\n",
    "        tx_norm = Lambda(lambda x : avg_power_normalise(x),\n",
    "                         output_shape=(Nc,2), name=\"tx_norm\")\\\n",
    "                        (tx_complex)\n",
    "        tx_norm_scaled = Lambda(lambda x : K.tf.multiply(np.float32(np.sqrt(2)), x),\n",
    "                          output_shape=(Nc,2), name=\"tx_norm_scaled\")\\\n",
    "                            (tx_norm)\n",
    "        return Model(tx_input_message, tx_norm_scaled, name=\"Transmitter\")\n",
    "    transmitter = Transmitter()\n",
    "\n",
    "    ## Block Transmitter\n",
    "    def tx_block_lambda(block_in):\n",
    "        # (?,block_size,M) -> (?,block_size,Nc,2)   \n",
    "#         test_lst = [0]*block_size\n",
    "        test_lst = []\n",
    "        for i in range(block_size):\n",
    "            tmp = transmitter(block_in[:,i,:])\n",
    "            test_lst.append(Lambda(lambda x : K.expand_dims(x,axis=1))(tmp))\n",
    "        return Concatenate(axis=1)(test_lst)\n",
    "    \n",
    "    def BlockTransmitter():\n",
    "        # (?,block_size,M) -> (?,block_size,Nc,2)\n",
    "        block_in = Input(shape=(block_size,M), name=\"tx_block_input_message\")\n",
    "        tx_output = Lambda(lambda x : tx_block_lambda(x),\n",
    "                           output_shape=(block_size,Nc,2),\\\n",
    "                           name=\"tx_blk_out\")(block_in)\n",
    "        return Model(block_in, tx_output, name=\"BlockTransmitter\")  \n",
    "    \n",
    "    ## H Parameter Estimator\n",
    "    def HEstimator():    \n",
    "        h_est_input = Input(shape=(block_size,Nc,2), name=\"h_est_input\")\n",
    "        noise_flat = Flatten(name=\"noise_flat\")(h_est_input)\n",
    "        h1 = Dense(20,activation=\"tanh\", name=\"h1\")\\\n",
    "                    (noise_flat)\n",
    "        h_hat = Dense(2,activation=\"linear\", name=\"h_hat\")\\\n",
    "                    (h1)\n",
    "        return Model(h_est_input, h_hat, name=\"HEstimator\")\n",
    "\n",
    "    ## Receiver\n",
    "    def Receiver():\n",
    "        # (?,Nc,2) -> (?,M)   \n",
    "        rx_input_message = Input(shape=(Nc,2), name=\"rx_input\")\n",
    "        y_over_h_flat = Flatten(name=\"y_over_h_flat\")(rx_input_message)\n",
    "        rx1 = Dense(M,activation=\"relu\", name=\"rx1\")\\\n",
    "                    (y_over_h_flat)\n",
    "        # Dense layer with softmax activation\n",
    "        rx_softmax = Dense(M,activation='softmax', \\\n",
    "                           name=\"rx_softmax\")(rx1)\n",
    "        return Model(rx_input_message, rx_softmax, name=\"Receiver\")\n",
    "    receiver = Receiver()\n",
    "    \n",
    "    ## Block Receiver\n",
    "    def rx_block_lambda(block_in):\n",
    "        # (?,block_size,Nc,2) -> (?,block_size,M)   \n",
    "#         test_lst = [0]*block_size\n",
    "        test_lst = []\n",
    "        for i in range(block_size):\n",
    "            tmp = receiver(block_in[:,i,:,:])\n",
    "            test_lst.append(Lambda(lambda x : K.expand_dims(x,axis=1))(tmp))\n",
    "        return Concatenate(axis=1)(test_lst)\n",
    "    \n",
    "    def BlockReceiver():\n",
    "        # (?,block_size,Nc,2) -> (?,block_size,M)   \n",
    "        block_in = Input(shape=(block_size,Nc,2), name=\"rx_block_input_message\")\n",
    "        rx_output = Lambda(lambda x : rx_block_lambda(x),\n",
    "                           output_shape=(block_size,M),\\\n",
    "                           name=\"rx_blk_out\")(block_in)\n",
    "        return Model(block_in, rx_output, name=\"BlockReceiver\")  \n",
    "    \n",
    "    ## Overall Autoencoder\n",
    "    ae_block_input = Input(shape=(block_size,M), name=\"ae_block_input\")\n",
    "    channel_symbs = BlockTransmitter()(ae_block_input)\n",
    "    faded = RayleighBlockFading(sigma_m,sigma_a)(channel_symbs)\n",
    "    h_est = HEstimator()(faded)\n",
    "    y_over_h = Lambda(lambda x : complex_div_block(x),\n",
    "                      output_shape=(block_size,Nc,2),\\\n",
    "                      name=\"y_over_h_block\")([faded, h_est])\n",
    "    received_pdf = BlockReceiver()(y_over_h)\n",
    "    ml_symbs = MostLikelySymbol()(received_pdf)\n",
    "        \n",
    "    ## Overall Autoencoder Models\n",
    "    autoencoder = Model(ae_block_input,received_pdf)\n",
    "    autoencoder_symbs = Model(ae_block_input,ml_symbs) \n",
    "    autoencoder.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=\"adam\")\n",
    "    \n",
    "    ## Submodel layers  \n",
    "    rx_blc_input = Input(shape=(block_size,Nc,2), name=\"rx_blc_input_\")\n",
    "    rx_blc_input2 = Input(shape=(block_size,Nc,2), name=\"rx_blc_input_\")\n",
    "    h_est2 = autoencoder.layers[-3](rx_blc_input)\n",
    "    y_over_h2 = autoencoder.layers[-2]([rx_blc_input, h_est2])\n",
    "    received_pdf2 = autoencoder.layers[-1](y_over_h2)\n",
    "    \n",
    "    ## Submodels\n",
    "    transmitter_block = Model(ae_block_input,channel_symbs)\n",
    "    channel_sym_with_noise = Model(ae_block_input,faded)\n",
    "    h_estimator = Model(rx_blc_input,h_est2)\n",
    "    receiver_block = Model(rx_blc_input2,autoencoder.layers[-1](rx_blc_input2))\n",
    "    whole_rx = Model(rx_blc_input,received_pdf2)\n",
    "    transmitter_block.compile(loss='categorical_crossentropy',\n",
    "                              optimizer=\"adam\")\n",
    "    channel_sym_with_noise.compile(loss='categorical_crossentropy',\n",
    "                                   optimizer=\"adam\")    \n",
    "    h_estimator.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=\"adam\")\n",
    "    receiver_block.compile(loss='categorical_crossentropy',\n",
    "                           optimizer=\"adam\")    \n",
    "    whole_rx.compile(loss='categorical_crossentropy',\n",
    "                     optimizer=\"adam\")   \n",
    "       \n",
    "    return autoencoder, autoencoder_symbs, transmitter_block, \\\n",
    "        channel_sym_with_noise, h_estimator, receiver_block, whole_rx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models, Data and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a set of data for a particular M\n",
    "# total_size = 1000000\n",
    "# all_one_hot_messages256 = np.diag(np.ones(256))\n",
    "\n",
    "\n",
    "# # Automatically saves the data for m to a filepath of\n",
    "# # './data/data${M}.npy'\n",
    "# t2 = time()\n",
    "# func_data256, file_path256, all_one_hot_messages256 = get_data_set(256, total_size)\n",
    "# print(f\"Finished 256 in {time()-t2}s\")\n",
    "\n",
    "# # Don't use this function unless it's for a new M, just\n",
    "# # load the data you have calculated other times.\n",
    "# # This makes results more comparible and saves time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data calculated from previous runs\n",
    "all_one_hot_messages256 = np.diag(np.ones(256))\n",
    "data256 = np.load('./data/data256.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data256.shape = (7200000, 256)\n",
      "valid_data256.shape = (800000, 256)\n",
      "test_data256.shape = (2000000, 256)\n"
     ]
    }
   ],
   "source": [
    "# Splitting into training, testing and validation sets\n",
    "train_data256, test_data256 = train_test_split(data256, \\\n",
    "                                         train_size=0.8)\n",
    "train_data256, valid_data256 = train_test_split(train_data256, \\\n",
    "                                         train_size=0.9)\n",
    "print(f\"train_data256.shape = {train_data256.shape}\")\n",
    "print(f\"valid_data256.shape = {valid_data256.shape}\")\n",
    "print(f\"test_data256.shape = {test_data256.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting a supervised model for AWGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    }
   ],
   "source": [
    "# (8,8)\n",
    "M = 2**8 # Number of one hot encoded messages\n",
    "R = 2 # R = k/n_r\n",
    "sigma = get_noise_sigma(7, Rc=R)\n",
    "hl_activation_func = keras.layers.advanced_activations.LeakyReLU()\n",
    "hl_activation_func.__name__ = 'leakyrelu'\n",
    "ol_activation_func = \"tanh\"\n",
    "num_layers = 2\n",
    "\n",
    "autoencoder8_8_tap_2l, transmitter8_8_tap_2l, reciever8_8_tap_2l, \\\n",
    "    autoencoder_symbs8_8_tap_2l, k8_8_tap_2l, Nc8_8_tap_2l, Nr8_8_tap_2l \\\n",
    "    = make_complex_n_layer_lr_tanh_tapering_model(M, R, sigma, \\\n",
    "                                                  hl_activation_func, \\\n",
    "                                                  ol_activation_func, \\\n",
    "                                                  num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Previous 0.1763\n",
    "# autoencoder8_8_tap_2l.fit(train_data256, train_data256,\n",
    "#                        epochs=1000,\n",
    "#                        batch_size=1000,\n",
    "#                        shuffle=True,\n",
    "#                        validation_data=(valid_data256,\n",
    "#                                         valid_data256),\n",
    "#                        callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder8_8_tap_2l.load_weights('./models/autoencoder8_8_tap_2l3.3856e-06.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting a supervised model for RBF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RBF autoencoder notes\n",
    "I need:\n",
    "- tx to be M -> Nc,2\n",
    "- h_est to be block,M -> 2,\n",
    "- rx to be Nc,2 -> M\n",
    "\n",
    "Could use tf.stack maybe?\n",
    "\n",
    "https://stackoverflow.com/questions/43151775/how-to-have-parallel-convolutional-layers-in-keras\n",
    "This seem encouraging, model transmitter and receiver as functions, then make new funcitions, transmitter_block and receiver_block which are:\n",
    "- tx_block: B,M -> B,Nc,2\n",
    "- rx_block: B,Nc,2 -> B,M\n",
    "\n",
    "Then make a new model called h_est which is:\n",
    "- h_est: block,M -> 2.\n",
    "\n",
    "Connect these three layers together to make one model called autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes on Debugging the None gradient bug\n",
    "- First it I had them in seperate models\n",
    "    - It worked up until the receiver_block model was added. However the receiver_block model could be fit, all the individual blocks could be fit. Just not the autoencoder.\n",
    "- Changed to putting the two problem blocks together not in models. Still didn't work.\n",
    "- Switched to a more functional way of assembling the model.\n",
    "    - Isolated it to it being a problem of the multiple input to the receiver block.\n",
    "    - Moved it so the y_over_h layer was seperate to the ReceiverBlock model and moved back to having the receiver block call all the functionality from inside a lambda.\n",
    "    - It worked!!!\n",
    "    - Added back in all the transmitter blocks, took a while.\n",
    "    - Tidied up code to get rid of all random commented out code.\n",
    "    - One last check, it didn't work any more. No idea why. \n",
    "- Back to square one\n",
    "    - Removing the names from the models doesn't help.\n",
    "    - Changing the name of the y_over_h didn't help.\n",
    "    - It works up to after the y_over_h layer.\n",
    "    - It also works with the h_est and y_over_h layers skipped.\n",
    "    - Can't be the y_over_h block as exactly this is used in the make_rsf_autoencoder function.\n",
    "    - Tried swapping the contents of the BlockReceiver function for a simple two layers that would definitely not have a none gradient. Still doesn't work.\n",
    "    - Must be the combination of the y_over_h and the receiver block together, but why???\n",
    "    - Found a seperate error from adding back in the name's incorrectly, also took out the initialisations of M and Nc inside tx_block_lambda. Then it worked again with the dummy BlockReceiver function. \n",
    "    - Doesn't work if I put the normal BlockReceiver function back in. \n",
    "    - Turned the flattening Reshapes into Flattens, still works with the dummy BlockReceiver function.\n",
    "    - Changed the internals of tx_block_lambda and rx_block_lambda to use the concatenate method instead. This gets rid of all the K.variables and the .assigns. Makes everything in these functions a Keras layer/model.\n",
    "        - Works with the dummy BlockReceiver function.\n",
    "        - Works with the lambdas. Going to save, then tidy up my code. If it stops working I'll git reset --hard\n",
    "- Added back in the transmitter and channel_sym_with_noise blocks, still works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data256_10 = np.reshape(train_data256, [-1,10,256])\n",
    "valid_data256_10 = np.reshape(valid_data256, [-1,10,256])\n",
    "test_data256_10 = np.reshape(test_data256, [-1,10,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data256_100 = np.reshape(train_data256, [-1,100,256])\n",
    "valid_data256_100 = np.reshape(valid_data256, [-1,100,256])\n",
    "test_data256_100 = np.reshape(test_data256, [-1,100,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data256_1000 = np.reshape(train_data256, [-1,1000,256])\n",
    "valid_data256_1000 = np.reshape(valid_data256, [-1,1000,256])\n",
    "test_data256_1000 = np.reshape(test_data256, [-1,1000,256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time take to compile\n",
    "\n",
    "| Block Size | Compile Time (s) | h_est parameters | training s/step |\n",
    "|---|---|---|---|\n",
    "| 10  | 4.75 | 1662 | 173$\\mu s$ |\n",
    "| 100 | 33.45 | 16062 |  |\n",
    "| 1000 | ~15 mins | ? |  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 2.58s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "k = 8\n",
    "Nc = 4 \n",
    "block_size = 10\n",
    "sigma_m = 1./np.sqrt(2.)\n",
    "sigma_a = db_to_sigma_a(40)\n",
    "autoencoder_rbf_10, autoencoder_symbs_rbf_10, transmitter_block_rbf_10, \\\n",
    "    channel_sym_with_noise_rbf_10, h_estimator, receiver_block_rbf_10, \\\n",
    "    whole_rx_rbf_10 \\\n",
    "    = get_rbf_autoencoder(k, Nc, block_size, sigma_m, sigma_a)\n",
    "history_rbf_10 = History()\n",
    "checkpoints_rbf_10 = ModelCheckpoint(\"./models/rbf_supervised/autoencoder_rbf_10_e{epoch:02d}_vl{val_loss}.h5\", save_best_only=True, save_weights_only=True)\n",
    "plot_model(autoencoder_rbf_10,to_file='./figures/autoencoder_rbf_10.png',show_shapes=True)\n",
    "print(f\"Took {round(time()-t0,2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 18.88s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "k = 8\n",
    "Nc = 4 \n",
    "block_size = 100\n",
    "sigma_m = 1./np.sqrt(2.)\n",
    "sigma_a = db_to_sigma_a(40)\n",
    "autoencoder_rbf_10e2, autoencoder_symbs_rbf_10e2, transmitter_block_rbf_10e2, \\\n",
    "    channel_sym_with_noise_rbf_10e2, h_estimator, receiver_block_rbf_10e2, \\\n",
    "    whole_rx_rbf_10e2 \\\n",
    "    = get_rbf_autoencoder(k, Nc, block_size, sigma_m, sigma_a)\n",
    "history_rbf_10e2 = History()\n",
    "checkpoints_rbf_10e2 = ModelCheckpoint(\"./models/rbf_supervised/autoencoder_rbf_10e2_e{epoch:02d}_vl{val_loss}.h5\", save_best_only=True, save_weights_only=True)\n",
    "# plot_model(autoencoder_rbf_10e2,to_file='./figures/aoudia_paper/autoencoder_rbf_10e2.png',show_shapes=True)\n",
    "# plot_model(autoencoder_rbf_10e2,to_file='./final_report/figures/aoudia_paper/autoencoder_rbf_10e2.png',show_shapes=True)\n",
    "print(f\"Took {round(time()-t0,2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 4.61s\n"
     ]
    }
   ],
   "source": [
    "# t0 = time()\n",
    "# k = 8\n",
    "# Nc = 4 \n",
    "# block_size = 10\n",
    "# sigma_m = 1./np.sqrt(2.)\n",
    "# sigma_a = db_to_sigma_a(40)\n",
    "# autoencoder_rbf_10e3, autoencoder_symbs_rbf_10e3, transmitter_block_rbf_10e3, \\\n",
    "#     channel_sym_with_noise_rbf_10e3, h_estimator, receiver_block_rbf_10e3, \\\n",
    "#     whole_rx_rbf_10e3 \\\n",
    "#     = get_rbf_autoencoder(k, Nc, block_size, sigma_m, sigma_a)\n",
    "# history_rbf_10e3 = History()\n",
    "# checkpoints_rbf_10e3 = ModelCheckpoint(\"./models/rbf_supervised/autoencoder_rbf_10e3_e{epoch:02d}_vl{val_loss}.h5\", save_best_only=True, save_weights_only=True)\n",
    "# # plot_model(autoencoder_rbf_10e3,to_file='./figures/autoencoder_rbf_10e3.png',show_shapes=True)\n",
    "# print(f\"Took {round(time()-t0,2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ae_block_input (InputLayer)     (None, 10, 256)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "BlockTransmitter (Model)        (None, 10, 4, 2)     0           ae_block_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "RayleighBlockFading (RayleighBl (None, 10, 4, 2)     0           BlockTransmitter[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "HEstimator (Model)              (None, 2)            1662        RayleighBlockFading[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "y_over_h_block (Lambda)         (None, 10, 4, 2)     0           RayleighBlockFading[0][0]        \n",
      "                                                                 HEstimator[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "BlockReceiver (Model)           (None, 10, 256)      0           y_over_h_block[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 1,662\n",
      "Trainable params: 1,662\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder_rbf_10.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720000 samples, validate on 80000 samples\n",
      "Epoch 1/10\n",
      "720000/720000 [==============================] - 125s 173us/step - loss: 5.5467 - val_loss: 5.5456\n",
      "Epoch 2/10\n",
      "720000/720000 [==============================] - 115s 160us/step - loss: 5.5456 - val_loss: 5.5455\n",
      "Epoch 3/10\n",
      "720000/720000 [==============================] - 115s 160us/step - loss: 5.5455 - val_loss: 5.5455\n",
      "Epoch 4/10\n",
      "720000/720000 [==============================] - 115s 160us/step - loss: 5.5455 - val_loss: 5.5454\n",
      "Epoch 5/10\n",
      "719000/720000 [============================>.] - ETA: 0s - loss: 5.5454"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-41017382a631>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                          validation_data=(valid_data256_10,\n\u001b[1;32m      6\u001b[0m                                           valid_data256_10),\n\u001b[0;32m----> 7\u001b[0;31m                          callbacks=[es, history_rbf_10, checkpoints_rbf_10])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[1;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                                              verbose=0)\n\u001b[0m\u001b[1;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder_rbf_10.fit(train_data256_10, train_data256_10,\n",
    "                         epochs=10,\n",
    "                         batch_size=1000,\n",
    "                         shuffle=True,\n",
    "                         validation_data=(valid_data256_10,\n",
    "                                          valid_data256_10),\n",
    "                         callbacks=[es, history_rbf_10, checkpoints_rbf_10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder_rbf_10.save('./models/rbf_supervised/autoencoder_rbf40_bs10_first.model')\n",
    "# autoencoder_rbf_10.save_weights('./models/rbf_supervised/autoencoder_rbf40_bs10_first.h5')\n",
    "\n",
    "### Loading\n",
    "# autoencoder_rbf_10.load_weights('./models/rbf_supervised/autoencoder_rbf40_bs10_first.h5', by_name=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720 samples, validate on 80 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "autoencoder_rbf_10e2.fit(train_data256_1000, train_data256_1000,\n",
    "                         epochs=10,\n",
    "                         batch_size=10000,\n",
    "                         shuffle=True,\n",
    "                         validation_data=(valid_data256_1000,\n",
    "                                          valid_data256_1000),\n",
    "                         callbacks=[es, history_rbf, checkpoints_rbf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720 samples, validate on 80 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "# autoencoder_rbf_10e3.fit(train_data256_1000, train_data256_1000,\n",
    "#                          epochs=10,\n",
    "#                          batch_size=10000,\n",
    "#                          shuffle=True,\n",
    "#                          validation_data=(valid_data256_1000,\n",
    "#                                           valid_data256_1000),\n",
    "#                          callbacks=[es, history_rbf, checkpoints_rbf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting a supervised model for RSF\n",
    "\n",
    "The RSF model is trained at an SNR of 20db, because of the normalisation this is a sigma_a of 0.1. Could also set sigma_m to satisfy $2\\sigma_m^2=1$, ($\\sigma_m = \\frac{1}{\\sqrt{2}}$) so that the average fade gain is unity.\n",
    "\n",
    "SNR = $\\frac{1}{\\sigma_2}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_to_sigma_a(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training at 20db\n",
    "k = 8\n",
    "Nc = 4\n",
    "# sigma_m = 0.33\n",
    "sigma_m = np.sqrt(0.5)\n",
    "sigma_a = db_to_sigma_a(20)\n",
    "autoencoder_rsf, autoencoder_symbs_rsf, transmitter_rsf, \\\n",
    "    channel_sym_with_noise_rsf, receiver_rsf, _, _ \\\n",
    "    = get_rsf_autoencoder(k, Nc, sigma_m, sigma_a)\n",
    "history_rsf = History()\n",
    "checkpoints_rsf = ModelCheckpoint(\"./models/rsf_supervised/autoencoder_rsf_{val_loss}.h5\", save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# autoencoder_rsf.fit(train_data256, train_data256,\n",
    "#                     epochs=10,\n",
    "#                     batch_size=10000,\n",
    "#                     shuffle=True,\n",
    "#                     validation_data=(valid_data256,\n",
    "#                                      valid_data256),\n",
    "#                     callbacks=[es, history_rsf, checkpoints_rsf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training at 40db\n",
    "k = 8\n",
    "Nc = 4\n",
    "# sigma_m = 0.33\n",
    "sigma_m = np.sqrt(0.5)\n",
    "sigma_a = db_to_sigma_a(40)\n",
    "autoencoder_rsf_40, autoencoder_symbs_rsf_40, transmitter_rsf_40, \\\n",
    "    channel_sym_with_noise_rsf_40, receiver_rsf_40, whole_receiver_rsf_40, \\\n",
    "    h_estimator_rsf_40 \\\n",
    "    = get_rsf_autoencoder(k, Nc, sigma_m, sigma_a)\n",
    "history_rsf_40 = History()\n",
    "checkpoints_rsf_40 = ModelCheckpoint(\"./models/rsf_supervised/autoencoder_rsf_40_vl{val_loss}_e{epoch:02d}.h5\", save_best_only=True, save_weights_only=True)\n",
    "# plot_model(autoencoder_rsf_40,to_file='./figures/aoudia_paper/autoencoder_rsf_arch.png',show_shapes=True)\n",
    "# plot_model(autoencoder_rsf_40,to_file='./final_report/figures/aoudia_paper/autoencoder_rsf_arch.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(transmitter_rsf_40,to_file='./figures/aoudia_paper/transmitter_rsf_arch.png',show_shapes=True)\n",
    "# plot_model(transmitter_rsf_40,to_file='./final_report/figures/aoudia_paper/transmitter_rsf_arch.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(receiver_rsf_40,to_file='./figures/aoudia_paper/receiver_rsf_arch.png',show_shapes=True)\n",
    "# plot_model(receiver_rsf_40,to_file='./final_report/figures/aoudia_paper/receiver_rsf_arch.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(h_estimator_rsf_40,to_file='./figures/aoudia_paper/h_estimator_rsf_arch.png',show_shapes=True)\n",
    "# plot_model(h_estimator_rsf_40,to_file='./final_report/figures/aoudia_paper/h_estimator_rsf_arch.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training at 40db\n",
    "k = 8\n",
    "Nc = 4\n",
    "# sigma_m = 0.33\n",
    "sigma_m = np.sqrt(0.5)\n",
    "sigma_a = db_to_sigma_a(40)\n",
    "autoencoder_rsf_40, autoencoder_symbs_rsf_40, transmitter_rsf_40, \\\n",
    "    channel_sym_with_noise_rsf_40, receiver_rsf_40, h_est_rsf_40 \\\n",
    "    = get_rsf_autoencoder_model(k, Nc, sigma_m, sigma_a)\n",
    "history_rsf_40 = History()\n",
    "checkpoints_rsf_40 = ModelCheckpoint(\"./models/rsf_supervised/autoencoder_rsf_40_vl{val_loss}_e{epoch:02d}.h5\", save_best_only=True, save_weights_only=True)\n",
    "# plot_model(autoencoder_rsf_40,to_file='./figures/aoudia_paper/autoencoder_rsf_arch.png',show_shapes=True)\n",
    "# plot_model(autoencoder_rsf_40,to_file='./final_report/figures/aoudia_paper/autoencoder_rsf_arch.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(transmitter_rsf_40,to_file='./figures/aoudia_paper/transmitter_rsf_arch.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# autoencoder_rsf_40.fit(train_data256, train_data256,\n",
    "# #                     epochs=10,\n",
    "#                     epochs=70,\n",
    "#                     batch_size=10000,\n",
    "#                     shuffle=True,\n",
    "#                     validation_data=(valid_data256,\n",
    "#                                      valid_data256),\n",
    "#                     callbacks=[es, history_rsf_40, checkpoints_rsf_40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_rsf_40_local = history_rsf_40_local + history_rsf_40.history[\"val_loss\"]\n",
    "# np.save(\"./models/rsf_supervised/histories/history_rsf_40_local.npy\",history_rsf_40_local)\n",
    "\n",
    "history_rsf_40_local = np.load(\"./models/rsf_supervised/histories/history_rsf_40_local.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with no additive noise\n",
    "k = 8\n",
    "Nc = 4\n",
    "# sigma_m = 0.33\n",
    "sigma_m = np.sqrt(0.5)\n",
    "sigma_a = 0\n",
    "autoencoder_rsf_0, autoencoder_symbs_rsf_0, transmitter_rsf_0, \\\n",
    "    channel_sym_with_noise_rsf_0, receiver_rsf_0, _, _ \\\n",
    "    = get_rsf_autoencoder(k, Nc, sigma_m, sigma_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_rsf_40 = np.array([4.5165, 4.4356, 4.4190, 4.4086, 4.3993, \\\n",
    "#                            4.3925, 4.3877, 4.3864, 4.3838, 4.3824])\n",
    "# history_rsf_0 = np.array([5.5452, 5.5452, 2.1448, 1.6362e-04, 3.6967e-04,\\\n",
    "#                            1.1467e-04, 5.1886e-04, 0.0019, 1.3443e-05, 8.5237e-04])\n",
    "# np.save(\"./models/rsf_supervised/histories/history_rsf_0.npy\",history_rsf_0)\n",
    "# np.save(\"./models/rsf_supervised/histories/history_rsf_40.npy\",history_rsf_40)\n",
    "\n",
    "history_rsf_40 = np.load(\"./models/rsf_supervised/histories/history_rsf_40.npy\")\n",
    "history_rsf_0 = np.load(\"./models/rsf_supervised/histories/history_rsf_0.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# history_rsf_0 = autoencoder_rsf_0.fit(train_data256, train_data256,\n",
    "#                                       epochs=10, batch_size=10000,\n",
    "#                                       shuffle=True,\n",
    "#                                       validation_data=(valid_data256,\n",
    "#                                                        valid_data256),\n",
    "#                                       callbacks=[es, history_rsf_0, checkpoints_rsf_0],\n",
    "#                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving\n",
    "# Individual power constraint, sigma_m = 1/sqrt(2)\n",
    "# autoencoder_rsf.save('./models/autoencoder_rsf_0.1589.model')\n",
    "# autoencoder_rsf.save_weights('./models/autoencoder_rsf_0.1589.h5')\n",
    "\n",
    "# autoencoder_rsf_ap.save('./models/autoencoder_rsf_ap_5.5459.model')\n",
    "# autoencoder_rsf_ap.save_weights('./models/autoencoder_rsf_ap_5.5459.h5')\n",
    "\n",
    "# autoencoder_rsf_40.save('./models/autoencoder_rsf_sa40_4.3824.model')\n",
    "# autoencoder_rsf_40.save_weights('./models/autoencoder_rsf_sa40_4.3824.h5')\n",
    "\n",
    "# autoencoder_rsf_40.save('./models/autoencoder_rsf_sa40_4.3870.model')\n",
    "# autoencoder_rsf_40.save_weights('./models/autoencoder_rsf_sa40_4.3870.h5')\n",
    "\n",
    "# autoencoder_rsf_0.save('./models/autoencoder_rsf_sa0_8.5237e_04.model')\n",
    "# autoencoder_rsf_0.save_weights('./models/autoencoder_rsf_sa0_8.5237e_04.h5')\n",
    "\n",
    "### Loading\n",
    "autoencoder_rsf.load_weights('./models/rsf_supervised/autoencoder_rsf_0.1589.h5', by_name=True)\n",
    "autoencoder_rsf_40.load_weights('./models/rsf_supervised/autoencoder_rsf_sa40_4.3824.h5', by_name=True)\n",
    "# autoencoder_rsf_40.load_weights('./models/autoencoder_rsf_sa0_8.5237e_04.h5', by_name=True)\n",
    "autoencoder_rsf_0.load_weights('./models/rsf_supervised/autoencoder_rsf_sa0_8.5237e_04.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Supervised RSF with leaky-relu instead of relu and more tanh layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local train_data256.shape = (720,000; 256) <br>\n",
    "AWS train_data256.shape = (7,200,000; 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    }
   ],
   "source": [
    "# Training at 40db with custom activation functions and layers\n",
    "k = 8\n",
    "Nc = 4\n",
    "# sigma_m = 0.33\n",
    "sigma_m = np.sqrt(0.5)\n",
    "sigma_a = db_to_sigma_a(40)\n",
    "num_layers = 2\n",
    "\n",
    "act_f = keras.layers.advanced_activations.LeakyReLU()\n",
    "act_f.__name__ = 'leakyrelu'\n",
    "\n",
    "tx_activation_func = act_f\n",
    "rx_activation_func = act_f\n",
    "rx_end_activation_func = \"tanh\"\n",
    "\n",
    "autoencoder_rsf_2l_lr_lr_tanh_40, autoencoder_symbs_rsf_2l_lr_lr_tanh_40, transmitter_rsf_2l_lr_lr_tanh_40, \\\n",
    "    channel_sym_with_noise_rsf_2l_lr_lr_tanh_40, receiver_rsf_2l_lr_lr_tanh_40 \\\n",
    "    = get_rsf_autoencoder_custom(k, Nc, sigma_m, sigma_a, \\\n",
    "                                 num_layers, tx_activation_func, \\\n",
    "                                 rx_activation_func, rx_end_activation_func)\n",
    "\n",
    "history_rsf_2l_lr_lr_tanh_40 = History()\n",
    "checkpoints_rsf_2l_lr_lr_tanh_40 = ModelCheckpoint(\"./models/rsf_supervised/autoencoder_rsf_2l_lr_lr_tanh_40_vl{val_loss}_e{epoch:02d}.h5\", save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 720000 samples, validate on 800000 samples\n",
      "Epoch 1/10\n",
      "720000/720000 [==============================] - 52s 73us/step - loss: 5.5549 - val_loss: 5.5457\n",
      "Epoch 2/10\n",
      "720000/720000 [==============================] - 52s 72us/step - loss: 5.5456 - val_loss: 5.5457\n",
      "Epoch 3/10\n",
      "720000/720000 [==============================] - 53s 73us/step - loss: 5.5455 - val_loss: 5.5456\n",
      "Epoch 4/10\n",
      "720000/720000 [==============================] - 53s 73us/step - loss: 5.5454 - val_loss: 5.5457\n",
      "Epoch 5/10\n",
      "720000/720000 [==============================] - 52s 73us/step - loss: 5.5454 - val_loss: 5.5456\n",
      "Epoch 6/10\n",
      "720000/720000 [==============================] - 53s 73us/step - loss: 5.5454 - val_loss: 5.5457\n",
      "Epoch 7/10\n",
      "720000/720000 [==============================] - 53s 74us/step - loss: 5.5453 - val_loss: 5.5456\n",
      "Epoch 8/10\n",
      "720000/720000 [==============================] - 52s 73us/step - loss: 5.5454 - val_loss: 5.5456\n",
      "Epoch 9/10\n",
      "720000/720000 [==============================] - 52s 73us/step - loss: 5.5453 - val_loss: 5.5456\n",
      "Epoch 10/10\n",
      "720000/720000 [==============================] - 53s 74us/step - loss: 5.5453 - val_loss: 5.5455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6c603ea0f0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Set to local train_data size to train faster over the first 10 epochs\n",
    "# autoencoder_rsf_2l_lr_lr_tanh_40.fit(train_data256[:720000,:], train_data256[:720000,:],\n",
    "#                     epochs=10,\n",
    "#                     batch_size=10000,\n",
    "#                     shuffle=True,\n",
    "#                     validation_data=(valid_data256,\n",
    "#                                      valid_data256),\n",
    "#                     callbacks=[es, history_rsf_2l_lr_lr_tanh_40, checkpoints_rsf_2l_lr_lr_tanh_40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training at 40db with custom activation functions and layers\n",
    "k = 8\n",
    "Nc = 4\n",
    "# sigma_m = 0.33\n",
    "sigma_m = np.sqrt(0.5)\n",
    "sigma_a = db_to_sigma_a(40)\n",
    "num_layers = 2\n",
    "\n",
    "tx_activation_func = \"tanh\"\n",
    "rx_activation_func = \"tanh\"\n",
    "rx_end_activation_func = \"tanh\"\n",
    "\n",
    "autoencoder_rsf_2l_th_th_th_40, autoencoder_symbs_rsf_2l_th_th_th_40, transmitter_rsf_2l_th_th_th_40, \\\n",
    "    channel_sym_with_noise_rsf_2l_th_th_th_40, receiver_rsf_2l_th_th_th_40 \\\n",
    "    = get_rsf_autoencoder_custom(k, Nc, sigma_m, sigma_a, \\\n",
    "                                 num_layers, tx_activation_func, \\\n",
    "                                 rx_activation_func, rx_end_activation_func)\n",
    "\n",
    "history_rsf_2l_th_th_th_40 = History()\n",
    "checkpoints_rsf_2l_th_th_th_40 = ModelCheckpoint(\"./models/rsf_supervised/autoencoder_rsf_2l_th_th_th_40_vl{val_loss}_e{epoch:02d}.h5\", save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720000 samples, validate on 800000 samples\n",
      "Epoch 1/10\n",
      "720000/720000 [==============================] - 55s 76us/step - loss: 5.5473 - val_loss: 5.5456\n",
      "Epoch 2/10\n",
      "720000/720000 [==============================] - 49s 68us/step - loss: 5.5454 - val_loss: 5.5457\n",
      "Epoch 3/10\n",
      "720000/720000 [==============================] - 49s 68us/step - loss: 5.5454 - val_loss: 5.5456\n",
      "Epoch 4/10\n",
      "720000/720000 [==============================] - 49s 69us/step - loss: 5.5453 - val_loss: 5.5455\n",
      "Epoch 5/10\n",
      "720000/720000 [==============================] - 49s 68us/step - loss: 5.5454 - val_loss: 5.5456\n",
      "Epoch 6/10\n",
      "720000/720000 [==============================] - 49s 68us/step - loss: 5.5453 - val_loss: 5.5455\n",
      "Epoch 7/10\n",
      "720000/720000 [==============================] - 49s 68us/step - loss: 5.5453 - val_loss: 5.5455\n",
      "Epoch 8/10\n",
      "720000/720000 [==============================] - 49s 68us/step - loss: 5.5453 - val_loss: 5.5455\n",
      "Epoch 9/10\n",
      "720000/720000 [==============================] - 48s 67us/step - loss: 5.5452 - val_loss: 5.5455\n",
      "Epoch 10/10\n",
      "720000/720000 [==============================] - 50s 69us/step - loss: 5.5452 - val_loss: 5.5455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6c400969b0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Set to local train_data size to train faster over the first 10 epochs\n",
    "# autoencoder_rsf_2l_th_th_th_40.fit(train_data256[:720000,:], train_data256[:720000,:],\n",
    "#                     epochs=10,\n",
    "#                     batch_size=10000,\n",
    "#                     shuffle=True,\n",
    "#                     validation_data=(valid_data256,\n",
    "#                                      valid_data256),\n",
    "#                     callbacks=[es, history_rsf_2l_th_th_th_40, checkpoints_rsf_2l_th_th_th_40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training at 40db with custom activation functions and layers\n",
    "k = 8\n",
    "Nc = 4\n",
    "# sigma_m = 0.33\n",
    "sigma_m = np.sqrt(0.5)\n",
    "sigma_a = db_to_sigma_a(40)\n",
    "num_layers = 2\n",
    "\n",
    "tx_activation_func = \"linear\"\n",
    "rx_activation_func = \"linear\"\n",
    "rx_end_activation_func = \"relu\"\n",
    "\n",
    "autoencoder_rsf_2l_lin_lin_rl_40, autoencoder_symbs_rsf_2l_lin_lin_rl_40, transmitter_rsf_2l_lin_lin_rl_40, \\\n",
    "    channel_sym_with_noise_rsf_2l_lin_lin_rl_40, receiver_rsf_2l_lin_lin_rl_40 \\\n",
    "    = get_rsf_autoencoder_custom(k, Nc, sigma_m, sigma_a, \\\n",
    "                                 num_layers, tx_activation_func, \\\n",
    "                                 rx_activation_func, rx_end_activation_func)\n",
    "\n",
    "history_rsf_2l_lin_lin_rl_40 = History()\n",
    "checkpoints_rsf_2l_lin_lin_rl_40 = ModelCheckpoint(\"./models/rsf_supervised/autoencoder_rsf_2l_lin_lin_rl_40_vl{val_loss}_e{epoch:02d}.h5\", save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720000 samples, validate on 800000 samples\n",
      "Epoch 1/10\n",
      "720000/720000 [==============================] - 55s 77us/step - loss: 5.5510 - val_loss: 5.5456\n",
      "Epoch 2/10\n",
      "720000/720000 [==============================] - 54s 75us/step - loss: 5.5455 - val_loss: 5.5456\n",
      "Epoch 3/10\n",
      "720000/720000 [==============================] - 54s 75us/step - loss: 5.5454 - val_loss: 5.5455\n",
      "Epoch 4/10\n",
      "720000/720000 [==============================] - 54s 74us/step - loss: 5.5453 - val_loss: 5.5456\n",
      "Epoch 5/10\n",
      "720000/720000 [==============================] - 55s 76us/step - loss: 5.5453 - val_loss: 5.5456\n",
      "Epoch 6/10\n",
      "720000/720000 [==============================] - 53s 74us/step - loss: 5.5453 - val_loss: 5.5456\n",
      "Epoch 7/10\n",
      "720000/720000 [==============================] - 54s 75us/step - loss: 5.5453 - val_loss: 5.5456\n",
      "Epoch 8/10\n",
      "720000/720000 [==============================] - 54s 76us/step - loss: 5.5453 - val_loss: 5.5455\n",
      "Epoch 9/10\n",
      "720000/720000 [==============================] - 54s 75us/step - loss: 5.5453 - val_loss: 5.5456\n",
      "Epoch 10/10\n",
      "720000/720000 [==============================] - 53s 74us/step - loss: 5.5453 - val_loss: 5.5455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6bb4190e10>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Set to local train_data size to train faster over the first 10 epochs\n",
    "# autoencoder_rsf_2l_lin_lin_rl_40.fit(train_data256[:720000,:], train_data256[:720000,:],\n",
    "#                     epochs=10,\n",
    "#                     batch_size=10000,\n",
    "#                     shuffle=True,\n",
    "#                     validation_data=(valid_data256,\n",
    "#                                      valid_data256),\n",
    "#                     callbacks=[es, history_rsf_2l_lin_lin_rl_40, checkpoints_rsf_2l_lin_lin_rl_40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper copy to check if it's to do with training on the\n",
    "# AWS that is causing the trouble\n",
    "k = 8\n",
    "Nc = 4\n",
    "# sigma_m = 0.33\n",
    "sigma_m = np.sqrt(0.5)\n",
    "sigma_a = db_to_sigma_a(40)\n",
    "autoencoder_rsf_40_aws, autoencoder_symbs_rsf_40_aws, transmitter_rsf_40_aws, \\\n",
    "    channel_sym_with_noise_rsf_40_aws, receiver_rsf_40_aws \\\n",
    "    = get_rsf_autoencoder(k, Nc, sigma_m, sigma_a)\n",
    "history_rsf_40_aws = History()\n",
    "checkpoints_rsf_40_aws = ModelCheckpoint(\"./models/rsf_supervised/autoencoder_rsf_40_aws_vl{val_loss}_e{epoch:02d}.h5\", save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720000 samples, validate on 800000 samples\n",
      "Epoch 1/10\n",
      "720000/720000 [==============================] - 55s 77us/step - loss: 5.5500 - val_loss: 5.5454\n",
      "Epoch 2/10\n",
      "720000/720000 [==============================] - 49s 68us/step - loss: 5.5452 - val_loss: 5.5454\n",
      "Epoch 3/10\n",
      "720000/720000 [==============================] - 49s 68us/step - loss: 5.5452 - val_loss: 5.5454\n",
      "Epoch 4/10\n",
      "720000/720000 [==============================] - 49s 68us/step - loss: 5.5452 - val_loss: 5.5454\n",
      "Epoch 5/10\n",
      "720000/720000 [==============================] - 50s 69us/step - loss: 5.5452 - val_loss: 5.5454\n",
      "Epoch 6/10\n",
      "720000/720000 [==============================] - 50s 70us/step - loss: 5.5452 - val_loss: 5.5454\n",
      "Epoch 7/10\n",
      "720000/720000 [==============================] - 50s 69us/step - loss: 5.5451 - val_loss: 5.5454\n",
      "Epoch 8/10\n",
      "720000/720000 [==============================] - 49s 67us/step - loss: 5.5451 - val_loss: 5.5454\n",
      "Epoch 9/10\n",
      "720000/720000 [==============================] - 50s 70us/step - loss: 5.5451 - val_loss: 5.5454\n",
      "Epoch 10/10\n",
      "720000/720000 [==============================] - 49s 68us/step - loss: 5.5451 - val_loss: 5.5454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f66b9f1f908>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autoencoder_rsf_40_aws.fit(train_data256[:720000,:], train_data256[:720000,:],\n",
    "#                     epochs=10,\n",
    "#                     batch_size=10000,\n",
    "#                     shuffle=True,\n",
    "#                     validation_data=(valid_data256,\n",
    "#                                      valid_data256),\n",
    "#                     callbacks=[es, history_rsf_40_aws, checkpoints_rsf_40_aws])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Try the paper architecture but 10 epochs with no noise, 10 with 80db, 10 with 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720000 samples, validate on 800000 samples\n",
      "Epoch 1/10\n",
      "720000/720000 [==============================] - 51s 71us/step - loss: 5.5729 - val_loss: 5.5562\n",
      "Epoch 2/10\n",
      "720000/720000 [==============================] - 53s 73us/step - loss: 5.5540 - val_loss: 5.5524\n",
      "Epoch 3/10\n",
      "720000/720000 [==============================] - 53s 74us/step - loss: 5.5515 - val_loss: 5.5505\n",
      "Epoch 4/10\n",
      "720000/720000 [==============================] - 53s 74us/step - loss: 5.5498 - val_loss: 5.5494\n",
      "Epoch 5/10\n",
      "720000/720000 [==============================] - 53s 73us/step - loss: 5.5488 - val_loss: 5.5490\n",
      "Epoch 6/10\n",
      "720000/720000 [==============================] - 53s 74us/step - loss: 5.5483 - val_loss: 5.5485\n",
      "Epoch 7/10\n",
      "720000/720000 [==============================] - 53s 74us/step - loss: 5.5480 - val_loss: 5.5480\n",
      "Epoch 8/10\n",
      "720000/720000 [==============================] - 53s 74us/step - loss: 5.5477 - val_loss: 5.5479\n",
      "Epoch 9/10\n",
      "720000/720000 [==============================] - 54s 75us/step - loss: 5.5472 - val_loss: 5.5473\n",
      "Epoch 10/10\n",
      "720000/720000 [==============================] - 53s 74us/step - loss: 5.5468 - val_loss: 5.5471\n",
      "Train on 720000 samples, validate on 800000 samples\n",
      "Epoch 1/10\n",
      "720000/720000 [==============================] - 55s 76us/step - loss: 5.5469 - val_loss: 5.5454\n",
      "Epoch 2/10\n",
      "720000/720000 [==============================] - 54s 75us/step - loss: 5.5453 - val_loss: 5.5454\n",
      "Epoch 3/10\n",
      "720000/720000 [==============================] - 53s 74us/step - loss: 5.5452 - val_loss: 5.5454\n",
      "Epoch 4/10\n",
      "720000/720000 [==============================] - 54s 75us/step - loss: 5.5452 - val_loss: 5.5454\n",
      "Epoch 5/10\n",
      "720000/720000 [==============================] - 53s 74us/step - loss: 5.5452 - val_loss: 5.5454\n",
      "Epoch 6/10\n",
      "720000/720000 [==============================] - 54s 75us/step - loss: 5.5451 - val_loss: 5.5454\n",
      "Epoch 7/10\n",
      "720000/720000 [==============================] - 54s 75us/step - loss: 5.5451 - val_loss: 5.5453\n",
      "Epoch 8/10\n",
      "720000/720000 [==============================] - 54s 75us/step - loss: 5.5450 - val_loss: 5.5452\n",
      "Epoch 9/10\n",
      "720000/720000 [==============================] - 53s 74us/step - loss: 5.5448 - val_loss: 5.5448\n",
      "Epoch 10/10\n",
      "720000/720000 [==============================] - 54s 75us/step - loss: 5.5405 - val_loss: 5.5213\n",
      "Train on 720000 samples, validate on 800000 samples\n",
      "Epoch 1/10\n",
      "720000/720000 [==============================] - 55s 77us/step - loss: 5.5478 - val_loss: 5.5454\n",
      "Epoch 2/10\n",
      "720000/720000 [==============================] - 54s 75us/step - loss: 5.5453 - val_loss: 5.5455\n",
      "Epoch 3/10\n",
      "720000/720000 [==============================] - 53s 74us/step - loss: 5.5453 - val_loss: 5.5454\n",
      "Epoch 4/10\n",
      "720000/720000 [==============================] - 53s 73us/step - loss: 5.5452 - val_loss: 5.5454\n",
      "Epoch 5/10\n",
      "720000/720000 [==============================] - 54s 75us/step - loss: 5.5452 - val_loss: 5.5454\n",
      "Epoch 6/10\n",
      "720000/720000 [==============================] - 53s 74us/step - loss: 5.5451 - val_loss: 5.5454\n",
      "Epoch 7/10\n",
      "720000/720000 [==============================] - 53s 74us/step - loss: 5.5451 - val_loss: 5.5454\n",
      "Epoch 8/10\n",
      "720000/720000 [==============================] - 54s 75us/step - loss: 5.5451 - val_loss: 5.5454\n",
      "Epoch 9/10\n",
      "720000/720000 [==============================] - 53s 73us/step - loss: 5.5451 - val_loss: 5.5454\n",
      "Epoch 10/10\n",
      "720000/720000 [==============================] - 54s 75us/step - loss: 5.5451 - val_loss: 5.5454\n"
     ]
    }
   ],
   "source": [
    "# # Paper copy to check if it's to do with training on the\n",
    "# # AWS that is causing the trouble\n",
    "# k = 8\n",
    "# Nc = 4\n",
    "# # sigma_m = 0.33\n",
    "# sigma_m = np.sqrt(0.5)\n",
    "# sigma_a = db_to_sigma_a(0)\n",
    "# autoencoder_rsf_0_80_40_aws, autoencoder_symbs_rsf_0_80_40_aws, transmitter_rsf_0_80_40_aws, \\\n",
    "#     channel_sym_with_noise_rsf_0_80_40_aws, receiver_rsf_0_80_40_aws \\\n",
    "#     = get_rsf_autoencoder(k, Nc, sigma_m, sigma_a)\n",
    "# history_rsf_0_80_40_aws = History()\n",
    "# checkpoints_rsf_0_80_40_aws = ModelCheckpoint(\"./models/rsf_supervised/autoencoder_rsf_0_80_40_aws_vl{val_loss}_e{epoch:02d}.h5\", save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# autoencoder_rsf_0_80_40_aws.fit(train_data256[:720000,:], train_data256[:720000,:],\n",
    "#                     epochs=10,\n",
    "#                     batch_size=10000,\n",
    "#                     shuffle=True,\n",
    "#                     validation_data=(valid_data256,\n",
    "#                                      valid_data256),\n",
    "#                     callbacks=[es, history_rsf_0_80_40_aws, checkpoints_rsf_0_80_40_aws])\n",
    "# autoencoder_rsf_0.save('./models/autoencoder_rsf_0_80_40_aws.model')\n",
    "# autoencoder_rsf_0.save_weights('./models/autoencoder_rsf_0_80_40_aws.h5')\n",
    "\n",
    "# sigma_a = db_to_sigma_a(80)\n",
    "# autoencoder_rsf_0_80_40_aws, autoencoder_symbs_rsf_0_80_40_aws, transmitter_rsf_0_80_40_aws, \\\n",
    "#     channel_sym_with_noise_rsf_0_80_40_aws, receiver_rsf_0_80_40_aws \\\n",
    "#     = get_rsf_autoencoder(k, Nc, sigma_m, sigma_a)\n",
    "# autoencoder_rsf.load_weights('./models/autoencoder_rsf_0_80_40_aws.h5', by_name=True)\n",
    "\n",
    "\n",
    "# autoencoder_rsf_0_80_40_aws.fit(train_data256[:720000,:], train_data256[:720000,:],\n",
    "#                     epochs=10,\n",
    "#                     batch_size=10000,\n",
    "#                     shuffle=True,\n",
    "#                     validation_data=(valid_data256,\n",
    "#                                      valid_data256),\n",
    "#                     callbacks=[es, history_rsf_0_80_40_aws, checkpoints_rsf_0_80_40_aws])\n",
    "# autoencoder_rsf_0.save('./models/autoencoder_rsf_0_80_40_aws.model')\n",
    "# autoencoder_rsf_0.save_weights('./models/autoencoder_rsf_0_80_40_aws.h5')\n",
    "\n",
    "\n",
    "# sigma_a = db_to_sigma_a(40)\n",
    "# autoencoder_rsf_0_80_40_aws, autoencoder_symbs_rsf_0_80_40_aws, transmitter_rsf_0_80_40_aws, \\\n",
    "#     channel_sym_with_noise_rsf_0_80_40_aws, receiver_rsf_0_80_40_aws \\\n",
    "#     = get_rsf_autoencoder(k, Nc, sigma_m, sigma_a)\n",
    "# autoencoder_rsf.load_weights('./models/autoencoder_rsf_0_80_40_aws.h5', by_name=True)\n",
    "\n",
    "# autoencoder_rsf_0_80_40_aws.fit(train_data256[:720000,:], train_data256[:720000,:],\n",
    "#                     epochs=10,\n",
    "#                     batch_size=10000,\n",
    "#                     shuffle=True,\n",
    "#                     validation_data=(valid_data256,\n",
    "#                                      valid_data256),\n",
    "#                     callbacks=[es, history_rsf_0_80_40_aws, checkpoints_rsf_0_80_40_aws])\n",
    "# autoencoder_rsf_0.save('./models/autoencoder_rsf_0_80_40_aws.model')\n",
    "# autoencoder_rsf_0.save_weights('./models/autoencoder_rsf_0_80_40_aws.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720000 samples, validate on 800000 samples\n",
      "Epoch 1/10\n",
      "720000/720000 [==============================] - 55s 77us/step - loss: 5.5500 - val_loss: 5.5454\n",
      "Epoch 2/10\n",
      "720000/720000 [==============================] - 49s 68us/step - loss: 5.5452 - val_loss: 5.5454\n",
      "Epoch 3/10\n",
      "720000/720000 [==============================] - 49s 68us/step - loss: 5.5452 - val_loss: 5.5454\n",
      "Epoch 4/10\n",
      "720000/720000 [==============================] - 49s 68us/step - loss: 5.5452 - val_loss: 5.5454\n",
      "Epoch 5/10\n",
      "620000/720000 [========================>.....] - ETA: 5s - loss: 5.5452"
     ]
    }
   ],
   "source": [
    "# autoencoder_rsf_0_80_40_aws.fit(train_data256[:720000,:], train_data256[:720000,:],\n",
    "#                     epochs=10,\n",
    "#                     batch_size=10000,\n",
    "#                     shuffle=True,\n",
    "#                     validation_data=(valid_data256,\n",
    "#                                      valid_data256),\n",
    "#                     callbacks=[es, history_rsf_0_80_40_aws, checkpoints_rsf_0_80_40_aws])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving\n",
    "# history_rsf_2l_lr_lr_tanh_40_aws = np.array(history_rsf_2l_lr_lr_tanh_40.history[\"val_loss\"])\n",
    "# np.save(\"./models/rsf_supervised/histories/history_rsf_2l_lr_lr_tanh_40_aws.npy\",history_rsf_2l_lr_lr_tanh_40_aws)\n",
    "\n",
    "# history_rsf_2l_th_th_th_40_aws = np.array(history_rsf_2l_th_th_th_40.history[\"val_loss\"])\n",
    "# np.save(\"./models/rsf_supervised/histories/history_rsf_2l_th_th_th_40_aws.npy\",history_rsf_2l_th_th_th_40_aws)\n",
    "\n",
    "# history_rsf_2l_lin_lin_rl_40_aws = np.array(history_rsf_2l_lin_lin_rl_40.history[\"val_loss\"])\n",
    "# np.save(\"./models/rsf_supervised/histories/history_rsf_2l_lin_lin_rl_40_aws.npy\",history_rsf_2l_lin_lin_rl_40_aws)\n",
    "\n",
    "# Paper copy but on AWS to check if that's the problem\n",
    "# history_rsf_paper_40_aws = np.array(history_rsf_40_aws.history[\"val_loss\"])\n",
    "# np.save(\"./models/rsf_supervised/histories/history_rsf_paper_40_aws.npy\",history_rsf_paper_40_aws)\n",
    "\n",
    "# history_rsf_0_80_40_aws = np.array(history_rsf_0_80_40_aws.history[\"val_loss\"])\n",
    "# np.save(\"./models/rsf_supervised/histories/history_rsf_0_80_40_aws.npy\",history_rsf_0_80_40_aws)\n",
    "\n",
    "\n",
    "### Loading\n",
    "history_rsf_2l_lr_lr_tanh_40_aws = np.load(\"./models/rsf_supervised/histories/history_rsf_2l_lr_lr_tanh_40_aws.npy\")\n",
    "history_rsf_2l_th_th_th_40_aws = np.load(\"./models/rsf_supervised/histories/history_rsf_2l_th_th_th_40_aws.npy\")\n",
    "history_rsf_2l_lin_lin_rl_40_aws =  np.load(\"./models/rsf_supervised/histories/history_rsf_2l_lin_lin_rl_40_aws.npy\")\n",
    "# history_rsf_paper_40_aws = np.load(\"./models/rsf_supervised/histories/history_rsf_paper_40_aws.npy\")\n",
    "# history_rsf_paper_40_local = np.load(\"./models/rsf_supervised/histories/history_rsf_paper_40_local.npy\")\n",
    "# history_rsf_0_80_40_aws =  np.load(\"./models/rsf_supervised/histories/history_rsf_0_80_40_aws.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing the different architectures for rbf_supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1,11)\n",
    "plt.figure()\n",
    "plt.semilogy(x,history_rsf_40_local[:10])\n",
    "plt.semilogy(x,history_rsf_2l_lr_lr_tanh_40_aws)\n",
    "plt.semilogy(x,history_rsf_2l_th_th_th_40_aws)\n",
    "plt.semilogy(x,history_rsf_2l_lin_lin_rl_40_aws)\n",
    "plt.xlabel(\"SNR (db)\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Comparing Different Architectures/Activation Functions\\nRSF 40db\")\n",
    "plt.grid()\n",
    "plt.legend([\"Paper 2l-lin-lin-relu\", \"2l-lr-lr-tanh\", \"2l_th_th_th\", \"2l_lin_lin_rl\"])\n",
    "# plt.savefig(\"./figures/aoudia_paper/rsf_unsuccessful_alt_archs.png\")\n",
    "# plt.savefig(\"./final_report/figures/aoudia_paper/rsf_unsuccessful_alt_archs.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting an  alternating model for AWGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 8\n",
    "Nc = 4\n",
    "sigma = 0.22\n",
    "transmitter, channel_sym_with_noise, \\\n",
    "    receiver, receiver_symbs \\\n",
    "    = get_paper_awgn_rx_and_tx_models(M, Nc, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting an  alternating model for RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 8\n",
    "Nc = 4\n",
    "sigma_m = 0.33\n",
    "sigma_a = 1\n",
    "transmitter, channel_sym_with_noise, \\\n",
    "    receiver, receiver_symbs \\\n",
    "    = get_paper_rbf_rx_and_tx_models(M, Nc, sigma_m, sigma_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Supervised AWGN performance across the SNR range of [-4,0.5,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_db = -4.0, i = 1/35\r\n",
      "0th loop took 6.67572021484375e-06s\n",
      "ratio_db = -3.5, i = 2/35\r\n",
      "1th loop took 2.384185791015625e-06s\n",
      "ratio_db = -3.0, i = 3/35\r\n",
      "2th loop took 1.9073486328125e-06s\n",
      "ratio_db = -2.5, i = 4/35\r\n",
      "3th loop took 1.6689300537109375e-06s\n",
      "ratio_db = -2.0, i = 5/35\r\n",
      "4th loop took 1.430511474609375e-06s\n",
      "ratio_db = -1.5, i = 6/35\r\n",
      "5th loop took 1.9073486328125e-06s\n",
      "ratio_db = -1.0, i = 7/35\r\n",
      "6th loop took 1.9073486328125e-06s\n",
      "ratio_db = -0.5, i = 8/35\r\n",
      "7th loop took 1.9073486328125e-06s\n",
      "ratio_db = 0.0, i = 9/35\r\n",
      "8th loop took 1.430511474609375e-06s\n",
      "ratio_db = 0.5, i = 10/35\r\n",
      "9th loop took 1.430511474609375e-06s\n",
      "ratio_db = 1.0, i = 11/35\r\n",
      "10th loop took 2.1457672119140625e-06s\n",
      "ratio_db = 1.5, i = 12/35\r\n",
      "11th loop took 1.6689300537109375e-06s\n",
      "ratio_db = 2.0, i = 13/35\r\n",
      "12th loop took 1.430511474609375e-06s\n",
      "ratio_db = 2.5, i = 14/35\r\n",
      "13th loop took 1.9073486328125e-06s\n",
      "ratio_db = 3.0, i = 15/35\r\n",
      "14th loop took 1.6689300537109375e-06s\n",
      "ratio_db = 3.5, i = 16/35\r\n",
      "15th loop took 1.6689300537109375e-06s\n",
      "ratio_db = 4.0, i = 17/35\r\n",
      "16th loop took 1.6689300537109375e-06s\n",
      "ratio_db = 4.5, i = 18/35\r\n",
      "17th loop took 1.430511474609375e-06s\n",
      "ratio_db = 5.0, i = 19/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "18th loop took 44.78780651092529s\n",
      "ratio_db = 5.5, i = 20/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "19th loop took 43.936283111572266s\n",
      "ratio_db = 6.0, i = 21/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20th loop took 44.38273882865906s\n",
      "ratio_db = 6.5, i = 22/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "21th loop took 44.90769720077515s\n",
      "ratio_db = 7.0, i = 23/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "22th loop took 45.09571576118469s\n",
      "ratio_db = 7.5, i = 24/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "23th loop took 45.79934883117676s\n",
      "ratio_db = 8.0, i = 25/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "24th loop took 46.489802837371826s\n",
      "ratio_db = 8.5, i = 26/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "25th loop took 46.33193874359131s\n",
      "ratio_db = 9.0, i = 27/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "26th loop took 47.14553141593933s\n",
      "ratio_db = 9.5, i = 28/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "27th loop took 47.4978563785553s\n",
      "ratio_db = 10.0, i = 29/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28th loop took 47.802834272384644s\n",
      "ratio_db = 10.5, i = 30/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "29th loop took 48.616352558135986s\n",
      "ratio_db = 11.0, i = 31/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "30th loop took 48.884952545166016s\n",
      "ratio_db = 11.5, i = 32/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "31th loop took 49.15961956977844s\n",
      "ratio_db = 12.0, i = 33/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "32th loop took 49.388458490371704s\n",
      "ratio_db = 12.5, i = 34/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "33th loop took 49.931763887405396s\n",
      "ratio_db = 13.0, i = 35/35\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "34th loop took 50.42385458946228s\n",
      "\n",
      "Took 800.5894069671631\n"
     ]
    }
   ],
   "source": [
    "# (8,8) tapered, n layers\n",
    "t0 = time()\n",
    "SNRs_db = np.arange(-4,13.5,0.5)\n",
    "# bler = np.empty(SNRs_db.size)\n",
    "bler = np.load('./key_results/paper2/autoencoder_awgn10.npy')\n",
    "\n",
    "hl_act_f = keras.layers.advanced_activations.LeakyReLU()\n",
    "hl_act_f.__name__ = 'leakyrelu'\n",
    "\n",
    "# for i, ratio_db in enumerate(tqdm_notebook(SNRs_db, desc=\"1st loop\")):\n",
    "for i, ratio_db in enumerate(SNRs_db):\n",
    "    print(f\"ratio_db = {ratio_db}, i = {i+1}/{SNRs_db.size}\", end=\"\\r\")\n",
    "    t1 = time()\n",
    "    if(ratio_db >= 5):\n",
    "        bler[i] = get_complex_tapered_noise_bler_SNR(2**8, 2, ratio_db, \\\n",
    "                                                     './models/autoencoder8_8_tap_2l0.0050.h5', \\\n",
    "                                                     test_data256, hl_act_f, \\\n",
    "                                                     \"tanh\", 2)\n",
    "    print(f\"\\n{i}th loop took {time() - t1}s\")\n",
    "# autoencoder_awgn10 = bler\n",
    "# np.save('./key_results/paper2/autoencoder_awgn10.npy', autoencoder_awgn10)\n",
    "\n",
    "print(f\"\\nTook {time() - t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save\n",
    "# autoencoder_awgn = bler\n",
    "# np.save('./key_results/paper2/autoencoder_awgn.npy', autoencoder_awgn)\n",
    "\n",
    "# autoencoder_awgn10 = bler\n",
    "# np.save('./key_results/paper2/autoencoder_awgn10.npy', autoencoder_awgn10)\n",
    "\n",
    "### Load\n",
    "autoencoder_awgn = np.load('./key_results/paper2/autoencoder_awgn.npy')\n",
    "autoencoder_awgn10 = np.load('./key_results/paper2/autoencoder_awgn10.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.58489319, 1.49623566, 1.41253754, 1.33352143, 1.25892541,\n",
       "       1.18850223, 1.12201845, 1.05925373, 1.        , 0.94406088,\n",
       "       0.89125094, 0.84139514, 0.79432823, 0.74989421, 0.70794578,\n",
       "       0.66834392, 0.63095734, 0.59566214, 0.56234133, 0.53088444,\n",
       "       0.50118723, 0.47315126, 0.44668359, 0.4216965 , 0.39810717,\n",
       "       0.3758374 , 0.35481339, 0.33496544, 0.31622777, 0.29853826,\n",
       "       0.28183829, 0.26607251, 0.25118864, 0.23713737, 0.22387211])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SNRs_db = np.arange(-4,13.5,0.5)\n",
    "db_to_sigma_a(SNRs_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Supervised RSF performance across the SNR range of [-4,1,30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# autoencoder with rbf\n",
    "# - trained at an SNR of 40\n",
    "# - exactly the same architecture as the paper\n",
    "# - training val_loss of 4.3824\n",
    "t0 = time()\n",
    "SNRs_db = np.arange(-4,30,1)\n",
    "bler = np.empty(SNRs_db.size)\n",
    "\n",
    "for i, ratio_db in enumerate(tqdm_notebook(SNRs_db, desc=\"1st loop\")):\n",
    "# for i, ratio_db in enumerate(SNRs_db):\n",
    "    print(f\"ratio_db = {ratio_db}, i = {i+1}/{SNRs_db.size}\", end=\"\\r\")\n",
    "    t1 = time()\n",
    "    bler[i] = get_supervised_rsf_bler_SNR(8, 4, np.sqrt(0.5), ratio_db, \\\n",
    "                                          './models/rsf_supervised/autoencoder_rsf_sa40_4.3824.h5', \\\n",
    "                                          test_data256)\n",
    "\n",
    "    \n",
    "#     print(f\"\\n{i}th loop took {time() - t1}s\")\n",
    "print(f\"\\nTook {time() - t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_cores =  4\n",
      "Took 136.46156430244446s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print(\"num_cores = \", num_cores)\n",
    "\n",
    "def tmp(SNR):\n",
    "    return get_supervised_rsf_bler_SNR(8, 4, np.sqrt(0.5), SNR, \\\n",
    "                                          './models/rsf_supervised/autoencoder_rbf_sa40_4.3824.h5', \\\n",
    "                                          test_data256)\n",
    "\n",
    "results = Parallel(n_jobs=num_cores)(delayed(tmp)(SNR) for SNR in SNRs_db)\n",
    "print(f\"Took {time()-t0}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.995525, 0.994975, 0.99555, 0.99547]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.995455, 0.995345, 0.9953  , 0.99551 , 0.994965, 0.994895,\n",
       "       0.99479 , 0.9947  , 0.99411 , 0.99397 , 0.994045, 0.993545,\n",
       "       0.99333 , 0.992605, 0.99238 , 0.991515, 0.990945, 0.990435,\n",
       "       0.989335, 0.98868 , 0.987745, 0.98669 , 0.984195, 0.982355,\n",
       "       0.98044 , 0.97794 , 0.97541 , 0.970705, 0.967605, 0.96307 ,\n",
       "       0.95757 , 0.950065, 0.942965, 0.935495, 0.9258  ])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From second run to check constistency\n",
    "bler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9945, 0.9975, 0.9945, 0.9955, 0.9955, 0.991 , 0.993 , 0.9925,\n",
       "       0.989 , 0.9915, 0.983 , 0.98  , 0.976 , 0.9765, 0.96  , 0.9575,\n",
       "       0.9405, 0.926 , 0.8955, 0.878 , 0.8475, 0.806 , 0.7665, 0.6995,\n",
       "       0.6605, 0.6145, 0.559 , 0.512 , 0.4585, 0.424 , 0.397 , 0.361 ,\n",
       "       0.342 , 0.3165])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from -4 to 30 run\n",
    "bler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.995535, 0.99516 , 0.99502 , 0.994455, 0.99419 , 0.993845,\n",
       "       0.993175, 0.99226 , 0.990925, 0.98967 , 0.98738 , 0.984355,\n",
       "       0.980925, 0.974795, 0.967475, 0.95784 , 0.94379 , 0.92575 ,\n",
       "       0.902345, 0.87441 , 0.838795, 0.798815, 0.75632 , 0.70641 ,\n",
       "       0.656895, 0.606725, 0.55868 , 0.51174 , 0.473755, 0.434715,\n",
       "       0.4013  , 0.37387 , 0.34917 , 0.327785])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from -4 to 30 run\n",
    "bler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.995415, 0.99536 , 0.995215, 0.995095])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From first run\n",
    "bler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder_rbf_sa40_4_3824_bler = bler\n",
    "# np.save('./key_results/paper2/autoencoder_rbf_sa40_4_3824_bler.npy', autoencoder_rbf_sa40_4_3824_bler)\n",
    "\n",
    "# autoencoder_rbf_sa40_4_3824_bler_4_30 = bler\n",
    "# np.save('./key_results/paper2/autoencoder_rbf_sa40_4_3824_bler_4_30.npy', autoencoder_rbf_sa40_4_3824_bler_4_30)\n",
    "\n",
    "\n",
    "autoencoder_rsf_sa40_4_3824_bler = np.load('./key_results/paper2/autoencoder_rbf_sa40_4_3824_bler.npy')\n",
    "autoencoder_rbf_sa40_4_3824_bler_4_30 = np.load('./key_results/paper2/autoencoder_rbf_sa40_4_3824_bler_4_30.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 6a - AWGN supervised vs unsupervised across a range of SNRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data for messing with the way the plot looks\n",
    "x = np.arange(-4,13.5,0.5)\n",
    "y = np.exp(-0.1*x**2)\n",
    "z = y*0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.64925e-01, 9.56290e-01, 9.44590e-01, 9.29410e-01, 9.08040e-01,\n",
       "       8.80110e-01, 8.42395e-01, 7.93590e-01, 7.25765e-01, 6.45315e-01,\n",
       "       5.48310e-01, 4.36680e-01, 3.20405e-01, 2.09300e-01, 1.21800e-01,\n",
       "       5.94700e-02, 2.34100e-02, 7.48500e-03, 1.68500e-03, 2.40000e-04,\n",
       "       3.50000e-05, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "       0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "       0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder_awgn10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.64925e-01, 9.56290e-01, 9.44590e-01, 9.29410e-01, 9.08040e-01,\n",
       "       8.80110e-01, 8.42395e-01, 7.93590e-01, 7.25765e-01, 6.45315e-01,\n",
       "       5.48310e-01, 4.36680e-01, 3.20405e-01, 2.09300e-01, 1.21800e-01,\n",
       "       5.94700e-02, 2.34100e-02, 7.48500e-03, 1.68800e-03, 2.68000e-04,\n",
       "       2.70000e-05, 2.00000e-06, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "       0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "       0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder_awgn10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-4,13.5,0.5)\n",
    "autoencoder_awgn = np.load('./key_results/paper2/autoencoder_awgn.npy')\n",
    "autoencoder_awgn10 = np.load('./key_results/paper2/autoencoder_awgn10.npy')\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogy(x,autoencoder_awgn,'^-', color=\"orange\", markerfacecolor='none', markersize=4, linewidth=0.5)\n",
    "plt.semilogy(x,autoencoder_awgn10,'^-', color=\"blue\", markerfacecolor='none', markersize=4, linewidth=0.5)\n",
    "# plt.semilogy(x,z,'D-', color=\"blue\", markerfacecolor='none', markersize=3, linewidth=0.5)\n",
    "plt.xlabel(\"SNR (db)\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.grid()\n",
    "plt.legend([\"Supervised20\", \"Supervised10\"])\n",
    "# plt.savefig(\"./figures/aoudia_paper/awgn_supervised.png\")\n",
    "# plt.savefig(\"./final_report/figures/aoudia_paper/awgn_supervised.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(x,autoencoder_awgn,'^-', color=\"orange\", markerfacecolor='none', markersize=4, linewidth=0.5)\n",
    "plt.semilogy(x,z,'D-', color=\"blue\", markerfacecolor='none', markersize=3, linewidth=0.5)\n",
    "plt.xlabel(\"SNR (db)\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.grid()\n",
    "plt.legend([\"Supervised\", \"Alternating\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 6b - RBF supervised vs unsupervised across a range of SNRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x30 = np.arange(-4,30,1)\n",
    "\n",
    "plt.figure()\n",
    "# plt.semilogy(x,autoencoder_rsf_sa40_4_3824_bler,'^-', color=\"orange\", markerfacecolor='none', markersize=4, linewidth=0.5)\n",
    "plt.semilogy(x30,autoencoder_rbf_sa40_4_3824_bler_4_30,'^-', color=\"orange\", markerfacecolor='none', markersize=4, linewidth=0.5)\n",
    "# plt.semilogy(x,z,'D-', color=\"blue\", markerfacecolor='none', markersize=3, linewidth=0.5)\n",
    "plt.xlabel(\"SNR (db)\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.grid()\n",
    "plt.legend([\"Supervised RSF\"])\n",
    "# plt.savefig(\"./figures/aoudia_paper/autoencoder_rsf_bler.png\")\n",
    "# plt.savefig(\"./final_report/figures/aoudia_paper/autoencoder_rsf_bler.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rician Fading\n",
    "\n",
    "https://en.wikipedia.org/wiki/Rician_fading\n",
    "<br>\n",
    "\n",
    "https://www.gaussianwaves.com/tag/rician/\n",
    "\n",
    "Also see the document in downloads about it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
